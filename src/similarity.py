import numpy as np

import mmh3
import networkx as nx
import warnings
from tqdm import tqdm
from networkx.algorithms import community
from sklearn.cluster import KMeans
from sklearn.metrics import davies_bouldin_score, rand_score

# find the minimum value between hashes
def minhashes(shingles, seeds):
    hashs =[]
    for seed in range(seeds):
        mini = float('inf')
        for shi in shingles:
            # hashes a list of strings
            hash = 0
            for e in shi:
                hash = hash ^ mmh3.hash(e, seed)
            # find the minimum value
            if mini > hash:
                mini = hash
        hashs.append(mini)
    return list(hashs)

# get every signature in data
def signatures(df, k, seeds):
    hash_dic = {}
    for i in range(len(df)):
        # make a description into k-shingles
        shi = []
        for ch in range(len(df[i])-k+1):
            shi.append(df[i][ch:ch+k])
        
        hash_dic[i] = minhashes(list(shi), seeds)
    return hash_dic

def convert_matrix(N, scores):
    similarity_matrix = np.zeros((N,N))
    for i in range(N):
        for j in range(N):
            if i == j:
                similarity_matrix[i][j] = 1.0
            elif i > j:
                similarity_matrix[i][j] = scores[(j, i)]
            else:
                similarity_matrix[i][j] = scores[(i, j)]
    return similarity_matrix

def find_sim(data, q, seed):
    """
    Finds the similarity between any two job's description for a given dataset using the shingle, minihash 
    and jaccord similarity.

    Args:
      data: The "data" parameter is the dataset that you want to cluster. It should be a 2D array-like
    object, such as a numpy array or a pandas DataFrame, where each row represents a data point and each
    column represents a feature of that data point.
      q: The q parameter represents the number of shingles ( k = 2 or 3 for small documents such as emails)
      seed: The seed parameter represents how mand seeds to use for doing the minihashes

    Returns:
      A dictionary where the keys are pairs of indices, and the values are scores representing the similarity 
      between job descriptions at those indices
    """
    sign = signatures(data, q, seed)

    score_list = {}
    keys =list(sign.keys())
    for k in tqdm(range(len(keys)-1), desc = 'Calculating jaccard similarity', delay=0.1):
        for j in range(k+1,len(keys)):
            # calculate jaccard simiarity and store the score
            score = len(np.intersect1d(sign[keys[k]],sign[keys[j]]))/len(np.union1d(sign[keys[k]],sign[keys[j]]))
            score_list[(keys[k],keys[j])] = score
    return score_list

def louvain_cluster(N, scores, ground_truth):
    """
    Determines the best partition of a graph for a given similarity score value
    using the Louvain Community Detection Algorithm, (Not using Girvan Newman
    because it's too time comsuming) and find its Davies-Bouldin index value.
    
    Args:
      N: length of data
      scores: A dictionary where the keys are pairs of indices, and the values are scores representing the similarity 
      between job descriptions at those indices
      ground_truth: An array of cluster label generated by feature clustering 

    Returns:
      the cluster label for each data points and the corresponding Davies-Bouldin index.
    """
    # Create a graph
    G = nx.Graph()

    # Add nodes (text points)
    G.add_nodes_from(range(N))

    # Add edges based on similarity scores (you can adjust the threshold)
    for idx, idy in scores:
        G.add_edge(idx, idy, weight = scores[(idx,idy)])

    # Use Girvan-Newman algorithm to detect communities
    communities = community.louvain_communities(G)
    
    # Retrieve the cluster assignments
    clusters = {}
    for label, nodes in enumerate(communities):
        for idx in nodes:
            clusters[idx] = label
    
    # Sort the cluster based on id order and calculate the dbi
    sorted_dict = dict(sorted(clusters.items()))
    dbi = davies_bouldin_score(convert_matrix(N,scores), list(sorted_dict.values()))
    rand_index = rand_score(ground_truth, list(sorted_dict.values()))
    return clusters, dbi, rand_index

def kmean_cluster(N, scores, ground_truth, k_max=30):
    """
    Determines the optimal number of
    clusters for a given similarity matrix using the Davies-Bouldin index.
    The minimum score is zero, with lower values indicating better clustering
    
    Args:
      N: length of data
      scores: A dictionary where the keys are pairs of indices, and the values are scores representing the similarity 
      between job descriptions at those indices
      k_max: The parameter `k_max` represents the maximum number of clusters to consider. In the given
      code, it is set to 30, which means the function will iterate over values of `k` from 2 to 30
      (inclusive) to find the best value of `k` based on the. Defaults to 30
      ground_truth: An array of cluster label generated by feature clustering 

    Returns:
      the cluster label for each data points and the corresponding Davies-Bouldin index .
    """
    best_dbi = float("inf")  # Initialize with a high value
    best_labels = None
    warnings.filterwarnings("ignore")
    similarity_matrix = convert_matrix(N,scores)
    for k in tqdm(range(10, k_max), desc = 'Finding optimal number of cluster in Kmeans', delay=0.1):
        # Kmeans clustering
        kmeans = KMeans(n_clusters=k, random_state=0)
        labels = kmeans.fit_predict(1-similarity_matrix)  # Convert similarity to distance

        dbi = davies_bouldin_score(1-similarity_matrix, labels)
        
        if dbi < best_dbi:
            best_labels = labels
            best_dbi = dbi
    rand_index = rand_score(ground_truth, labels)
    # Retrieve the cluster assignments
    clusters = {}
    for label, idx in zip(best_labels, range(N)):
        clusters[idx] = label

    return clusters, dbi, rand_index
