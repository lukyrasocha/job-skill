{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02807 Computational Tools for Data Science\n",
    "\n",
    "## Project: Identifying Key Skills in Job Markets through Clustering Analysis üíº\n",
    "\n",
    "authors: Ting-Hui Cheng (s232855), Tomasz Truszkowski (s223219), Lukas Rasocha (s233498), Henrietta Domokos (s233107) \n",
    "\n",
    "### üîÅ  Reproducability\n",
    "\n",
    "This jupyter notebook is self-contained and includes the code for the entirety of the project. However, for the completeness sake the github repository can be accessed [here](https://github.com/lukyrasocha/02807-comp-tools). In there we have created an intuitive command line interface, which shows the entire project pipeline as well.\n",
    "You can go ahead and follow these steps to reproduce the results.\n",
    "```\n",
    "# Clone the repo\n",
    "git clone https://github.com/lukyrasocha/02807-comp-tools.git\n",
    "\n",
    "# Change directory\n",
    "cd 02807-comp-tools\n",
    "\n",
    "# Create an environment (Python version 3.10.4) - newer versions not tested\n",
    "conda create -n \"comp-ds\" python=\"3.10.4\"\n",
    "\n",
    "# Install dependencies\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Make sure your PYTHONPATH is set correctly and includes our project\n",
    "export PYTHONPATH=\"$PWD\"\n",
    "\n",
    "# Run our main which guides you through the entire project\n",
    "python src/main.py\n",
    "```\n",
    "\n",
    "Else, you can just follow this notebook from the top to bottom.\n",
    "\n",
    "### üìù Introduction and outline\n",
    "\n",
    "In this project we aim to identity key skills in job markets through clustering analysis. The main motivation is to help job seekers understand the ever evolving job market by allowing them to view and compare the most prominent skills for each cluster of job descriptions. This approach in a long run would enable people to input a job post and get back a set of relevant skills based on other jobs in the same cluster. \n",
    "\n",
    "The structure of how we approached this problem is as follow:\n",
    "\n",
    "1. üîé **Data Scraping**\n",
    "- To collect data we have created a Linkedin Scraper which is able to scrape data for a particular location, a particular keyword or simply scrapes all the newest job posts. Together with the job description we can also obtain any other Linkedin specific job metadata, such as:*Job Title, Industry or Function, number of applicants, date posted, company* etc. \n",
    "\n",
    "2. üßπ **Data Preprocessing**\n",
    "- Since the data was not always clean, we performed various preprocessing techniques, especially for the job posts' text, that included, lemmatization, tokenization, removing stop words etc.\n",
    "\n",
    "3. üìê **Clustering analysis**\n",
    "- We have performed numerous different clustering techniques on the job descriptions themselves. The obvious problem was, how do we translate the long textual descriptions into a format (vectors) which the methods required. This was achieved differently depending on the method (for a more elaborative description please refer to the report).\n",
    "  - TFIDF based on the whole job description\n",
    "  - TFIDF based on the nouns from the job description\n",
    "  - TFIDF based on the verbs from the job description\n",
    "  - TFIDF based on the adjectives from the job descriptions\n",
    "  - Word2Vec model to obtain embeddings for the words in the job description, where the job description is then represented as an average of the words that appear in it\n",
    "  - Doc2Vec model that obtains an embedding for longer text (such as documents or paragraphs). This way we were able to represent each job description as its on vector (embedding)\n",
    "  - Similarity, each job description was represented as a vector that represents the similarities of itself to the other job descriptions in the dataset\n",
    "\n",
    "- Given these different methods to translate a job description into a numerical format, we have then tried different clustering methods, such as K-Means, DBSCAN or Gaussian Mixture Model\n",
    "\n",
    "4. üìã **Ground Truth**\n",
    "- To be able to compare the different methods, we first needed to find our ground truth. We have once again tried different ways to infer the ground truth (for more elaborative description please refer to the report)\n",
    "  - One Hot encoding\n",
    "  - Keyword inference\n",
    "  - OpenAI's GPT3.5-turbo (General purpose large language model) prompted to categorize job descriptions\n",
    "\n",
    "5. üìà **Evaluation**\n",
    "- The best clustering method was then chosen to be the one, which is closest to the ground truth (measured by a normalized mutual information (NMI) score)\n",
    "\n",
    "5. üî¨ **Skill extraction**\n",
    "- After finding the best performing method, we delved into the problem of skill extraction: finding the 5 most promiment skills for each cluster. To extract the skills from the job descriptions we have tried different methods:\n",
    "  - Hugging face open source model, trained to extract hard and soft skills from text\n",
    "  - OpenAI's GPT3.5-turbo (General purpose large language model) prompted to extract job skills\n",
    "\n",
    "6. üìä **Skill Analysis**\n",
    "- We then visualized the most important skills for each cluster using *word clouds* and *bar charts* by looking at the most frequent skills for each cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2212a55e",
   "metadata": {},
   "source": [
    "## üì¶ Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53e6c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import string\n",
    "import ast\n",
    "import mmh3\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import yaml\n",
    "import threading\n",
    "import networkx as nx\n",
    "import warnings\n",
    "import nltk\n",
    "import textwrap\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from transformers import pipeline\n",
    "from openai import OpenAI\n",
    "from rich.console import Console\n",
    "from langdetect import detect\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from networkx.algorithms import community\n",
    "from sklearn.metrics import davies_bouldin_score, rand_score, normalized_mutual_info_score\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from openai import OpenAI\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK DOWNLOADS (Natural Language Toolkit)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f266d602",
   "metadata": {},
   "source": [
    "##  üîß Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258346b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(kind=\"processed\"):\n",
    "  \"\"\"\n",
    "  Load the data from the data folder.\n",
    "  args:\n",
    "    kind: [\"raw\", \"processed\", \"ground_truth_gpt\", \"skills_gpt\"]\n",
    "  \"\"\"\n",
    "  if kind == \"raw\":\n",
    "    df = pd.read_csv('data/raw/jobs.csv', sep=';')\n",
    "  elif kind == \"processed\":\n",
    "    df = pd.read_csv('data/processed/cleaned_jobs.csv', sep=';')\n",
    "  elif kind == \"ground_truth_gpt\":\n",
    "    df = pd.read_csv('clusters/ground_truth_gpt.csv')\n",
    "  elif kind == \"skills_gpt\":\n",
    "    df = pd.read_csv('extracted_skills/skills_extracted_gpt3.csv')\n",
    "  return df\n",
    "\n",
    "\n",
    "def is_english(text):\n",
    "  try:\n",
    "    return detect(text) == 'en'\n",
    "  except:\n",
    "    return False\n",
    "\n",
    "\n",
    "def apply_kmeans(tfidf_matrix, k=5):\n",
    "  kmeans = KMeans(n_clusters=k, random_state=0, n_init=10)\n",
    "  return kmeans.fit_predict(tfidf_matrix.toarray())\n",
    "\n",
    "\n",
    "def words2sentence(word_list):\n",
    "  return \" \".join(word_list)\n",
    "\n",
    "\n",
    "def apply_tftidf(data):\n",
    "  vectorizer = TfidfVectorizer()\n",
    "  return vectorizer.fit_transform(data)\n",
    "\n",
    "\n",
    "def visualize_cluster(data,\n",
    "                      cluster,\n",
    "                      reduce_dim=True,\n",
    "                      savefig=False,\n",
    "                      filename=\"cluster.png\",\n",
    "                      name=\"Cluster method\"):\n",
    "  \"\"\"\n",
    "  Visualize the clusters\n",
    "  Data: 2d numpy array of the individual data points that were used for clustering\n",
    "  cluster: 1d numpy array of the cluster labels\n",
    "  reduced_dim: Boolean, if True, perform pca to 2 dimensions\n",
    "  \"\"\"\n",
    "\n",
    "  if reduce_dim:\n",
    "    pca = PCA(n_components=2)\n",
    "    data = pca.fit_transform(data)\n",
    "\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  plt.scatter(data[:, 0], data[:, 1], c=cluster,\n",
    "              cmap='tab20', edgecolor='black', alpha=0.7, s=100)\n",
    "  plt.title(name, fontsize=16, fontweight='bold')\n",
    "  plt.xlabel(\"PCA 1\", fontsize=14)\n",
    "  plt.ylabel(\"PCA 2\", fontsize=14)\n",
    "  plt.colorbar()\n",
    "  plt.grid(True, linestyle='--', alpha=0.5)\n",
    "  plt.tight_layout()\n",
    "  if savefig:\n",
    "    plt.savefig(f\"figures/{filename}\")\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def visualize_ground_truth(gt, savefig=False, filename=\"ground_truth.png\"):\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  plt.bar(gt[\"category\"].value_counts().index,\n",
    "          gt[\"category\"].value_counts().values, color='dodgerblue')\n",
    "\n",
    "  plt.xticks(rotation=75)\n",
    "  plt.title(\"Ground truth distribution\", fontsize=16, fontweight='bold')\n",
    "  plt.xlabel(\"Category\", fontsize=14)\n",
    "  plt.ylabel(\"Count\", fontsize=14)\n",
    "  plt.grid(True, linestyle='--', alpha=0.5)\n",
    "  plt.tight_layout()\n",
    "  if savefig:\n",
    "    plt.savefig(f\"figures/{filename}\")\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791922bc",
   "metadata": {},
   "source": [
    "## Logger\n",
    "To have nice unified logging messages in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e3c36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def working_on(message):\n",
    "  console = Console()\n",
    "  console.print(\":wrench: [bold green]WORKING ON[/bold green]: \" + message)\n",
    "\n",
    "\n",
    "def warning(message):\n",
    "  console = Console()\n",
    "  console.print(\":tomato: [bold red]WARNING[/bold red]: \" + message)\n",
    "\n",
    "\n",
    "def error(message):\n",
    "  console = Console()\n",
    "  console.print(\":tomato: [bold red]ERROR[/bold red]: \" + message)\n",
    "\n",
    "\n",
    "def info(message):\n",
    "  console = Console()\n",
    "  console.print(\n",
    "      \":information_source: [bold yellow]INFO[/bold yellow]: \" + message)\n",
    "\n",
    "\n",
    "def success(message):\n",
    "  console = Console()\n",
    "  console.print(\n",
    "      \":white_check_mark: [bold green]SUCCESS[/bold green]: \" + message)\n",
    "\n",
    "\n",
    "def winner(message):\n",
    "  console = Console()\n",
    "  console.print(\n",
    "      \":trophy: [bold yellow]WINNER[/bold yellow]: \" + message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2011947d",
   "metadata": {},
   "source": [
    "## üíº 1. Linkedin Scraper \n",
    "\n",
    "Our custom made scraper, that scrapes data from the publicly posted jobs on *linkedin.com*. Can scrape data for a specific location, or with a specific keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d135a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkedinScraper:\n",
    "  def __init__(self, location, keywords=None, amount=50):\n",
    "    self.location = location\n",
    "    self.keywords = keywords\n",
    "    self.amount = amount\n",
    "    self.job_ids = []\n",
    "    self.jobs = []\n",
    "\n",
    "    if amount > 1000:\n",
    "      print(\n",
    "          \"‚ö†Ô∏è WARNING: LinkedIn only allows you to scrape 1000 jobs per search. ‚ö†Ô∏è\"\n",
    "      )\n",
    "      print(\"‚ö†Ô∏è WARNING: The amount will be set to 1000. ‚ö†Ô∏è\")\n",
    "      self.amount = 1000\n",
    "    if keywords == None:\n",
    "      self.all_jobs_url = f\"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?location={self.location}\"\n",
    "      self.all_jobs_url += \"&start={}\"\n",
    "    else:\n",
    "      self.all_jobs_url = f\"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords={self.keywords}&location={self.location}\"\n",
    "      self.all_jobs_url += \"&start={}\"\n",
    "\n",
    "    self.job_url = \"https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/{}\"\n",
    "\n",
    "  def save_to_csv(self, filename=\"data/raw/jobs.csv\"):\n",
    "    print(\"üìù Saving jobs to CSV file...\")\n",
    "    if os.path.isfile(filename):\n",
    "      existing_ids = set(pd.read_csv(filename, sep=\";\")[\"id\"])\n",
    "    else:\n",
    "      existing_ids = set()\n",
    "\n",
    "    # Filter out jobs that are already saved in the CSV\n",
    "\n",
    "    unique_jobs = [job for job in self.jobs if int(\n",
    "        job[\"id\"]) not in existing_ids]\n",
    "\n",
    "    if unique_jobs:\n",
    "      df = pd.DataFrame(unique_jobs)\n",
    "      df.to_csv(\n",
    "          filename,\n",
    "          mode=\"a\",\n",
    "          sep=\";\",\n",
    "          header=not os.path.isfile(filename),\n",
    "          index=False,\n",
    "      )\n",
    "\n",
    "  def _get_job_ids(self):\n",
    "    for i in tqdm(\n",
    "        range(0, self.amount, 25),\n",
    "        desc=\"üíº Scraping job IDs üîç\",\n",
    "        ascii=True,\n",
    "        colour=\"#0077B5\",\n",
    "    ):\n",
    "      res = requests.get(self.all_jobs_url.format(i))\n",
    "      soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "      alljobs_on_this_page = soup.find_all(\"li\")\n",
    "      for x in range(0, len(alljobs_on_this_page)):\n",
    "        try:\n",
    "          jobid = (\n",
    "              alljobs_on_this_page[x]\n",
    "              .find(\"div\", {\"class\": \"base-card\"})\n",
    "              .get(\"data-entity-urn\")\n",
    "              .split(\":\")[3]\n",
    "          )\n",
    "          self.job_ids.append(jobid)\n",
    "        except:\n",
    "          print(\"‚ùå One Job ID could not be retrieved ‚ùå\")\n",
    "          pass\n",
    "\n",
    "  def scrape(self):\n",
    "    # First scrape the job ids\n",
    "    self._get_job_ids()\n",
    "\n",
    "    # Then scrape the job details\n",
    "    for j in tqdm(\n",
    "        range(0, len(self.job_ids)),\n",
    "        desc=\"üíº Scraping job details üîç\",\n",
    "        ascii=True,\n",
    "        colour=\"#0077B5\",\n",
    "    ):\n",
    "      job = {}  # Create a new job dictionary\n",
    "      resp = requests.get(self.job_url.format(self.job_ids[j]))\n",
    "      soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "      job[\"id\"] = self.job_ids[j]\n",
    "      job[\"date_scraped\"] = pd.Timestamp.now()\n",
    "      job[\"keyword_scraped\"] = self.keywords\n",
    "      job[\"location_scraped\"] = self.location\n",
    "      job[\"linkedin_num\"] = j\n",
    "\n",
    "      try:\n",
    "        job[\"company\"] = (\n",
    "            soup.find(\"div\", {\"class\": \"top-card-layout__card\"})\n",
    "            .find(\"a\")\n",
    "            .find(\"img\")\n",
    "            .get(\"alt\")\n",
    "        )\n",
    "      except:\n",
    "        job[\"company\"] = None\n",
    "\n",
    "      try:\n",
    "        job[\"title\"] = (\n",
    "            soup.find(\"div\", {\"class\": \"top-card-layout__entity-info\"})\n",
    "            .find(\"a\")\n",
    "            .text.strip()\n",
    "        )\n",
    "      except:\n",
    "        job[\"title\"] = None\n",
    "\n",
    "      try:\n",
    "        job[\"num_applicants\"] = (\n",
    "            soup.find(\"div\", {\"class\": \"top-card-layout__entity-info\"})\n",
    "            .find(\"h4\")\n",
    "            .find(\"span\", {\"class\": \"num-applicants__caption\"})\n",
    "            .text.strip()\n",
    "        )\n",
    "      except:\n",
    "        job[\"num_applicants\"] = None\n",
    "\n",
    "      try:\n",
    "        job[\"date_posted\"] = (\n",
    "            soup.find(\"div\", {\"class\": \"top-card-layout__entity-info\"})\n",
    "            .find(\"h4\")\n",
    "            .find(\"span\", {\"class\": \"posted-time-ago__text\"})\n",
    "            .text.strip()\n",
    "        )\n",
    "      except:\n",
    "        job[\"date_posted\"] = None\n",
    "\n",
    "      try:\n",
    "        ul_element = soup.find(\n",
    "            \"ul\", {\"class\": \"description__job-criteria-list\"}\n",
    "        )\n",
    "\n",
    "        for li_element in ul_element.find_all(\"li\"):\n",
    "          subheader = li_element.find(\n",
    "              \"h3\", {\"class\": \"description__job-criteria-subheader\"}\n",
    "          ).text.strip()\n",
    "          criteria = li_element.find(\n",
    "              \"span\",\n",
    "              {\n",
    "                  \"class\": \"description__job-criteria-text description__job-criteria-text--criteria\"\n",
    "              },\n",
    "          ).text.strip()\n",
    "\n",
    "          if \"Seniority level\" in subheader:\n",
    "            job[\"level\"] = criteria\n",
    "          elif \"Employment type\" in subheader:\n",
    "            job[\"employment_type\"] = criteria\n",
    "          elif \"Job function\" in subheader:\n",
    "            job[\"function\"] = criteria\n",
    "          elif \"Industries\" in subheader:\n",
    "            job[\"industries\"] = criteria\n",
    "      except:\n",
    "        job[\"level\"] = None\n",
    "        job[\"employment_type\"] = None\n",
    "        job[\"function\"] = None\n",
    "        job[\"industries\"] = None\n",
    "\n",
    "      try:\n",
    "        job[\"description\"] = soup.find(\n",
    "            \"div\", {\"class\": \"description__text description__text--rich\"}\n",
    "        ).text.strip()\n",
    "      except:\n",
    "        job[\"description\"] = None\n",
    "\n",
    "      self.jobs.append(job)\n",
    "\n",
    "      # Checkpoint to save the jobs to the CSV file every 500 jobs\n",
    "      if (j + 1) % 500 == 0:\n",
    "        self.save_to_csv()\n",
    "        self.jobs = []\n",
    "\n",
    "    if self.jobs:\n",
    "      self.save_to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235f080e",
   "metadata": {},
   "source": [
    "## üßπ 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebf4802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words_with_numbers(word_list):\n",
    "  \"\"\"\n",
    "  Takes a string representation of a list of words as input,\n",
    "  removes any special characters from the words, and then removes any words that contain numbers.\n",
    "\n",
    "  Args:\n",
    "    word_list_str: A string representation of a list of words.\n",
    "\n",
    "  Returns:\n",
    "    The function `remove_words_with_numbers` returns a list of words without any special characters or\n",
    "  numbers.\n",
    "  \"\"\"\n",
    "  word_list_without_special = [\n",
    "      re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", word) for word in word_list\n",
    "  ]\n",
    "  word_list_without_numbers = [\n",
    "      word for word in word_list_without_special if not re.search(r\"\\d\", word)\n",
    "  ]\n",
    "  return word_list_without_numbers\n",
    "\n",
    "\n",
    "def convert_date_posted(date_str, date_scraped):\n",
    "  try:\n",
    "    days_ago = int(date_str.split(' ')[0])\n",
    "    actual_date = pd.to_datetime(date_scraped) - pd.Timedelta(days=days_ago)\n",
    "    return actual_date\n",
    "  except:\n",
    "    return date_scraped  # If the format is not \"x days ago\", use the scraped date\n",
    "\n",
    "\n",
    "def split_combined_words(text):\n",
    "  \"\"\"\n",
    "  Since during the scraping, some words are combined, e.g. \"requirementsYou're\" or \"offerings.If\" we need to split them\n",
    "  Splits words at:\n",
    "  1. Punctuation marks followed by capital letters.\n",
    "  2. Lowercase letters followed by uppercase letters.\n",
    "  \"\"\"\n",
    "  # 1. split\n",
    "  text = re.sub(r'([!?,.;:])([A-Z])', r'\\1 \\2', text)\n",
    "\n",
    "  # 2. split\n",
    "  text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
    "\n",
    "  return text\n",
    "\n",
    "\n",
    "def text_preprocessing(text):\n",
    "  \"\"\"\n",
    "  Preprocesses text by:\n",
    "    - Splitting combined words\n",
    "    - Tokenizing\n",
    "    - Removing stopwords\n",
    "    - Remove punctuation\n",
    "    - Lemmatizing\n",
    "  \"\"\"\n",
    "\n",
    "  text = split_combined_words(text)\n",
    "  text = text.lower()\n",
    "\n",
    "  # Remove punctuation\n",
    "  text = re.sub(f'[{string.punctuation}]', '', text)\n",
    "  # Remove numbers\n",
    "  text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "  tokens = word_tokenize(text)\n",
    "\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "\n",
    "  punctuation = {'!', ',', '.', ';', ':', '?',\n",
    "                 '(', ')', '[', ']', '-', '+', '\"', '*', '‚Äî', '‚Ä¢', '‚Äô', '‚Äò', '‚Äú', '‚Äù', '``'}\n",
    "  tokens = [w for w in tokens if w not in punctuation]\n",
    "\n",
    "  # Remove last 3 words since they are always the same (scraped buttons from the website)\n",
    "  tokens = tokens[:-3]\n",
    "\n",
    "  return tokens\n",
    "\n",
    "\n",
    "def preprocess():\n",
    "  \"\"\"\n",
    "  Main function of the preprocessing module.\n",
    "  Loads the raw data and does the following:\n",
    "  - Checks for english language\n",
    "  - Removes rows with missing descriptions\n",
    "  - Inferes the date posted\n",
    "  - Preprocesses the description (tokenization, removing stopwords, removing punctuation, lemmatization, splits combined words (e.g \"requirementsYou're\"))\n",
    "  - Removes rows with empty descriptions or descriptions containing less than 3 words\n",
    "  - Removes special characters and numbers from the tokenized list\n",
    "  - Preprocesses the title, function and industries\n",
    "  - Saves the preprocessed data to data/processed/cleaned_jobs.csv\n",
    "  \"\"\"\n",
    "\n",
    "  working_on(\"Loading data\")\n",
    "  df = load_data(kind=\"raw\")\n",
    "\n",
    "  # Remove duplicates\n",
    "  df.drop_duplicates(subset=['id'], inplace=True)\n",
    "  df.drop_duplicates(subset=['description'], inplace=True)\n",
    "  # Filter out jobs with missing descriptions\n",
    "  df = df[df['description'].notna()]\n",
    "\n",
    "  working_on(\"Filtering out non-english descriptions ...\")\n",
    "  for index, row in df.iterrows():\n",
    "    if not is_english(row['description'][:100]):\n",
    "      df.drop(index, inplace=True)\n",
    "\n",
    "  working_on(\"Infering dates ...\")\n",
    "  df['date_posted'] = df.apply(lambda x: convert_date_posted(\n",
    "      x['date_posted'], x['date_scraped']), axis=1)\n",
    "\n",
    "  # Lower case all text\n",
    "  df['title'] = df['title'].str.lower()\n",
    "  df['function'] = df['function'].str.lower()\n",
    "  df['industries'] = df['industries'].str.lower()\n",
    "  df['industries'] = df['industries'].str.replace('\\n', ' ')\n",
    "\n",
    "  # Removing outliers (where industries is whole description of offer)\n",
    "  df[\"industries_length\"] = df[\"industries\"].str.split()\n",
    "  df[\"industries_length\"] = df[\"industries_length\"].str.len()\n",
    "  df = df[df[\"industries_length\"] < 15]\n",
    "  df.drop(columns=[\"industries_length\"], inplace=True)\n",
    "\n",
    "  df[\"industries\"] = df[\"industries\"].str.replace(\" and \", \",\")\n",
    "  df[\"function\"] = df[\"function\"].str.replace(\" and \", \",\")\n",
    "  df[\"industries\"] = df[\"industries\"].str.replace(\"/\", \",\")\n",
    "  df[\"function\"] = df[\"function\"].str.replace(\"/\", \",\")\n",
    "\n",
    "  df[\"industries\"] = df[\"industries\"].str.replace(r\",,|, ,\", \",\")\n",
    "  df[\"function\"] = df[\"function\"].str.replace(r\",,|, ,\", \",\")\n",
    "\n",
    "  tqdm.pandas(desc=\"üêº Preprocessing description\", ascii=True, colour=\"#0077B5\")\n",
    "\n",
    "  df['description'] = df['description'].progress_apply(text_preprocessing)\n",
    "\n",
    "  # Remove rows with empty descriptions or descriptions containing less than 3 words\n",
    "  df = df[df['description'].map(len) > 3]\n",
    "\n",
    "  # Remove special characters and numbers from the tokenized list\n",
    "  df['description'] = df['description'].apply(\n",
    "      lambda x: remove_words_with_numbers(x)\n",
    "  )\n",
    "\n",
    "  df = df.reset_index(drop=True)\n",
    "\n",
    "  working_on(\"Saving preprocessed data ...\")\n",
    "  df.to_csv('data/processed/cleaned_jobs.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3935e1d",
   "metadata": {},
   "source": [
    "## üìê 3. Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. TFIDF Clustering\n",
    "- Based on whole text\n",
    "- Based on nouns\n",
    "- Based on verbs\n",
    "- Based on adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4307d70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def TFIDF_cluster(data, save_clusters=True, n_clusters=20):\n",
    "  \"\"\"\n",
    "  data: pandas dataframe (cleaned jobs)\n",
    "  save_clusters: Boolean, if True, save the clusters to a csv file in a format \"id, cluster\"\n",
    "  n_clusters: Number of clusters \n",
    "  \"\"\"\n",
    "\n",
    "  data[\"description\"] = data[\"description\"].apply(words2sentence)\n",
    "  tfidf_matrix = apply_tftidf(data[\"description\"])\n",
    "\n",
    "  data[\"cluster\"] = apply_kmeans(tfidf_matrix, k=n_clusters)\n",
    "\n",
    "  if save_clusters:\n",
    "    data[[\"id\", \"cluster\"]].to_csv(\n",
    "        \"clusters/tfidf_clusters_job_desc.csv\", index=False)\n",
    "    success(\"Clusters saved to clusters/tfidf_clusters_job_desc.csv\")\n",
    "\n",
    "  dbs = round(davies_bouldin_score(tfidf_matrix.toarray(), data[\"cluster\"]), 3)\n",
    "\n",
    "  success(\"David Bouldin score: \" + str(dbs))\n",
    "\n",
    "  return data[[\"id\", \"cluster\"]], tfidf_matrix.toarray()\n",
    "\n",
    "# Define the pos_tagger function\n",
    "def pos_tagger(nltk_tag):\n",
    "  if nltk_tag.startswith('J'):\n",
    "    return wordnet.ADJ\n",
    "  elif nltk_tag.startswith('V'):\n",
    "    return wordnet.VERB\n",
    "  elif nltk_tag.startswith('N'):\n",
    "    return wordnet.NOUN\n",
    "  elif nltk_tag.startswith('R'):\n",
    "    return wordnet.ADV\n",
    "  else:\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_tfidf_vectors(description_type):\n",
    "  \"\"\"\n",
    "  Get TF-IDF vectors and keywords for each description.\n",
    "\n",
    "  Parameters:\n",
    "  - description_type: List of text descriptions.\n",
    "\n",
    "  Returns:\n",
    "  - vectors: TF-IDF vectors.\n",
    "  - all_keywords: List of keywords for each description.\n",
    "  \"\"\"\n",
    "  # create a TF-IDF vectorizer\n",
    "  vectorizer = TfidfVectorizer()\n",
    "\n",
    "  # fit and transform the text descriptions using the TF-IDF vectorizer\n",
    "  vectors = vectorizer.fit_transform(description_type)\n",
    "\n",
    "  # get the feature names (words) corresponding to the TF-IDF vectors\n",
    "  feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "  # convert the sparse TF-IDF matrix to a dense representation\n",
    "  dense = vectors.todense()\n",
    "  denselist = dense.tolist()\n",
    "\n",
    "  # initialize a list to store keywords for each description\n",
    "  all_keywords = []\n",
    "\n",
    "  # iterate through each description in the dense representation\n",
    "  for description in denselist:\n",
    "    # extract keywords (feature names) where the TF-IDF value is greater than 0\n",
    "    keywords = [feature_names[i]\n",
    "                for i, word in enumerate(description) if word > 0]\n",
    "\n",
    "    # append the keywords for the current description to the list\n",
    "    all_keywords.append(keywords)\n",
    "\n",
    "  return vectors, all_keywords, vectorizer\n",
    "\n",
    "\n",
    "def TFIDF_verbs_cluster(data, save_clusters=True, n_clusters=20):\n",
    "  \"\"\"\n",
    "  data: pandas dataframe (cleaned jobs)\n",
    "  save_clusters: Boolean, if True, save the clusters to a csv file in a format \"id, cluster\"\n",
    "  n_clusters: Number of clusters \n",
    "  \"\"\"\n",
    "\n",
    "  data = data[['id', 'description']]\n",
    "\n",
    "  data = data.copy()\n",
    "  working_on(\"Extracting verbs from descriptions ...\")\n",
    "  data.loc[:, 'description_verb'] = data['description'].apply(lambda x: [word for word, pos_tag in [(word, pos_tagger(tag[1][0].upper(\n",
    "  ))) for word, tag in zip(eval(x), nltk.pos_tag(eval(x))) if pos_tagger(tag[1][0].upper()) == wordnet.VERB] if pos_tag is not None])\n",
    "\n",
    "  description_verb = data['description_verb']\n",
    "\n",
    "  description_verb_strings = [' '.join(description)\n",
    "                              for description in description_verb]\n",
    "\n",
    "  working_on(\"Clustering verbs ...\")\n",
    "  model = KMeans(n_clusters=n_clusters, init=\"k-means++\", n_init=10)\n",
    "\n",
    "  vectors, all_keywords, vectorizer = get_tfidf_vectors(\n",
    "      description_verb_strings)\n",
    "\n",
    "  # Fit the model to the TF-IDF vectors\n",
    "  model.fit(vectors)\n",
    "\n",
    "  # Get the indices that sort the cluster centers in descending order\n",
    "  order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "  # Get the feature names (words) from the TF-IDF vectorizer\n",
    "  terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "  # Write the top terms for each cluster to a text file\n",
    "  with open(\"results/kmeans_verb_results.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(n_clusters):\n",
    "      f.write(f\"Cluster {i+1}\")\n",
    "      f.write(\"\\n\")\n",
    "      for ind in order_centroids[i, :10]:\n",
    "        f.write(' %s' % terms[ind],)\n",
    "        f.write(\"\\n\")\n",
    "      f.write(\"\\n\")\n",
    "      f.write(\"\\n\")\n",
    "\n",
    "  # Predict cluster assignments for each document in the TF-IDF vectors\n",
    "  kmean_indicates = model.fit_predict(vectors)\n",
    "\n",
    "  # Add a 'cluster' column to the 'data' DataFrame\n",
    "  # data['cluster_verb'] = kmean_indicates\n",
    "  data.loc[:, 'cluster_verb'] = kmean_indicates\n",
    "\n",
    "  data = data.rename(columns={'cluster_verb': 'cluster'})\n",
    "\n",
    "  # Save the results to a CSV file with 'id' and 'cluster' columns\n",
    "  if save_clusters:\n",
    "    result_df = data[['id', 'cluster']]\n",
    "    result_df.to_csv('clusters/tfidf_verb_clusters.csv', index=False)\n",
    "    success(\"Clusters saved to clusters/tfidf_verb_clusters.csv\")\n",
    "\n",
    "  dbs = round(davies_bouldin_score(vectors.toarray(), kmean_indicates), 3)\n",
    "\n",
    "  success(\"David Bouldin score: \" + str(dbs))\n",
    "\n",
    "  return data[[\"id\", \"cluster\"]], vectors.toarray()\n",
    "\n",
    "\n",
    "def TFIDF_nouns_cluster(data,  save_clusters=True, n_clusters=20):\n",
    "  \"\"\"\n",
    "  data: pandas dataframe (cleaned jobs)\n",
    "  save_clusters: Boolean, if True, save the clusters to a csv file in a format \"id, cluster\"\n",
    "  n_clusters: Number of clusters \n",
    "  \"\"\"\n",
    "\n",
    "  data = data[['id', 'description']]\n",
    "\n",
    "  data = data.copy()\n",
    "  working_on(\"Extracting nouns from descriptions ...\")\n",
    "  data.loc[:, 'description_noun'] = data['description'].apply(\n",
    "      # use a lambda function to extract nouns using POS tagging\n",
    "      lambda x: [\n",
    "          word  # extract the word\n",
    "          for word, pos_tag in [\n",
    "              (word, pos_tagger(tag[1][0].upper()))  # POS tag each word\n",
    "              # pair each word with its POS tag\n",
    "              for word, tag in zip(eval(x), nltk.pos_tag(eval(x)))\n",
    "              # filter for nouns\n",
    "              if pos_tagger(tag[1][0].upper()) == wordnet.NOUN\n",
    "          ]\n",
    "          if pos_tag is not None  # exclude words with undefined POS tags\n",
    "      ]\n",
    "  )\n",
    "  description_noun = data['description_noun']\n",
    "\n",
    "  description_noun_strings = [' '.join(description)\n",
    "                              for description in description_noun]\n",
    "\n",
    "  working_on(\"Clustering nouns ...\")\n",
    "  model = KMeans(n_clusters=n_clusters, init=\"k-means++\", n_init=10)\n",
    "\n",
    "  vectors, all_keywords, vectorizer = get_tfidf_vectors(\n",
    "      description_noun_strings)\n",
    "\n",
    "  # Fit the model to the TF-IDF vectors\n",
    "  model.fit(vectors)\n",
    "\n",
    "  # Get the indices that sort the cluster centers in descending order\n",
    "  order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "  # Get the feature names (words) from the TF-IDF vectorizer\n",
    "  terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "  # Write the top terms for each cluster to a text file\n",
    "  with open(\"results/kmeans_noun_results.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(n_clusters):\n",
    "      f.write(f\"Cluster {i+1}\")\n",
    "      f.write(\"\\n\")\n",
    "      for ind in order_centroids[i, :10]:\n",
    "        f.write(' %s' % terms[ind],)\n",
    "        f.write(\"\\n\")\n",
    "      f.write(\"\\n\")\n",
    "      f.write(\"\\n\")\n",
    "\n",
    "  # Predict cluster assignments for each document in the TF-IDF vectors\n",
    "  kmean_indicates = model.fit_predict(vectors)\n",
    "\n",
    "  data.loc[:, 'cluster_noun'] = kmean_indicates\n",
    "\n",
    "  data = data.rename(columns={'cluster_noun': 'cluster'})\n",
    "\n",
    "  if save_clusters:\n",
    "    result_df = data[['id', 'cluster']]\n",
    "    result_df.to_csv('clusters/tfidf_noun_clusters.csv', index=False)\n",
    "    success(\"Clusters saved to clusters/tfidf_noun_clusters.csv\")\n",
    "\n",
    "  dbs = round(davies_bouldin_score(vectors.toarray(), kmean_indicates), 3)\n",
    "\n",
    "  success(\"David Bouldin score: \" + str(dbs))\n",
    "\n",
    "  return data[[\"id\", \"cluster\"]], vectors.toarray()\n",
    "\n",
    "\n",
    "def TFIDF_adjectives_cluster(data, save_clusters=True, n_clusters=20):\n",
    "  \"\"\"\n",
    "  data: pandas dataframe (cleaned jobs)\n",
    "  save_clusters: Boolean, if True, save the clusters to a csv file in a format \"id, cluster\"\n",
    "  n_clusters: Number of clusters \n",
    "  \"\"\"\n",
    "\n",
    "  data = data[['id', 'description']]\n",
    "\n",
    "  data = data.copy()\n",
    "  working_on(\"Extracting adjectives from descriptions ...\")\n",
    "  data.loc[:, 'description_adj'] = data['description'].apply(\n",
    "      lambda x: [word for word, pos_tag in [(word, pos_tagger(tag[1][0].upper())) for word, tag in zip(\n",
    "          eval(x), nltk.pos_tag(eval(x))) if pos_tagger(tag[1][0].upper()) == wordnet.ADJ] if pos_tag is not None]\n",
    "  )\n",
    "  description_adj = data['description_adj']\n",
    "\n",
    "  description_adj_strings = [' '.join(description)\n",
    "                             for description in description_adj]\n",
    "\n",
    "  working_on(\"Clustering adjectives ...\")\n",
    "  model = KMeans(n_clusters=n_clusters, init=\"k-means++\", n_init=10)\n",
    "\n",
    "  vectors, all_keywords, vectorizer = get_tfidf_vectors(\n",
    "      description_adj_strings)\n",
    "\n",
    "  # Fit the model to the TF-IDF vectors\n",
    "  model.fit(vectors)\n",
    "\n",
    "  # Get the indices that sort the cluster centers in descending order\n",
    "  order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "  # Get the feature names (words) from the TF-IDF vectorizer\n",
    "  terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "  # Write the top terms for each cluster to a text file\n",
    "  with open(\"results/kmeans_adj_results.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(n_clusters):\n",
    "      f.write(f\"Cluster {i+1}\")\n",
    "      f.write(\"\\n\")\n",
    "      for ind in order_centroids[i, :10]:\n",
    "        f.write(' %s' % terms[ind],)\n",
    "        f.write(\"\\n\")\n",
    "      f.write(\"\\n\")\n",
    "      f.write(\"\\n\")\n",
    "\n",
    "  # Predict cluster assignments for each document in the TF-IDF vectors\n",
    "  kmean_indicates = model.fit_predict(vectors)\n",
    "\n",
    "  # Add a 'cluster' column to the 'data' DataFrame\n",
    "\n",
    "  data.loc[:, 'cluster_adj'] = kmean_indicates\n",
    "\n",
    "  data = data.rename(columns={'cluster_adj': 'cluster'})\n",
    "\n",
    "  # Save the results to a CSV file with 'id' and 'cluster' columns\n",
    "\n",
    "  if save_clusters:\n",
    "    result_df = data[['id', 'cluster']]\n",
    "    result_df.to_csv('clusters/tfidf_adj_clusters.csv', index=False)\n",
    "    success(\"Clusters saved to clusters/tfidf_adj_clusters.csv\")\n",
    "\n",
    "  dbs = round(davies_bouldin_score(vectors.toarray(), kmean_indicates), 3)\n",
    "\n",
    "  success(\"David Bouldin score: \" + str(dbs))\n",
    "\n",
    "  return data[[\"id\", \"cluster\"]], vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b22880",
   "metadata": {},
   "source": [
    "### 3.2 Word2Vec clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031faa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def description_to_vector(model, description):\n",
    "  # Filter out words not in the model's vocabulary\n",
    "  valid_words = [word for word in description if word in model.wv.key_to_index]\n",
    "  if valid_words:\n",
    "    # Average the vectors of the words in the description\n",
    "    return np.mean(model.wv[valid_words], axis=0)\n",
    "  else:\n",
    "    # If no valid words, return a zero vector\n",
    "    return np.zeros(model.vector_size)\n",
    "\n",
    "\n",
    "def word2vec_cluster(data,\n",
    "                     save_clusters=True,\n",
    "                     vector_size=100,\n",
    "                     window=5,\n",
    "                     min_count=1,\n",
    "                     workers=4,\n",
    "                     n_clusters=20):\n",
    "  \"\"\"\n",
    "  data: pandas dataframe (cleaned jobs)\n",
    "  save_clusters: Boolean, if True, save the clusters to a csv file in a format \"id, cluster\"\n",
    "  vector_size: Dimensionality of the feature vectors\n",
    "  window: Maximum distance between the current and predicted word within a sentence\n",
    "  min_count: Ignores all words with total frequency lower than this\n",
    "  workers: Use these many worker threads to train the model\n",
    "  n_clusters: Number of clusters to cluster the embeddings into\n",
    "  \"\"\"\n",
    "\n",
    "  ######### Word2Vec #########\n",
    "  model = Word2Vec(sentences=data['description'],\n",
    "                   vector_size=vector_size, window=window, min_count=min_count, workers=workers)\n",
    "\n",
    "  working_on(\"Creating vectors ...\")\n",
    "  data['vector'] = data['description'].apply(\n",
    "      lambda x: description_to_vector(model, x))\n",
    "\n",
    "  vectors = np.array(data['vector'].tolist())\n",
    "\n",
    "  kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=0)\n",
    "  data['cluster'] = kmeans.fit_predict(vectors)\n",
    "\n",
    "  df_id_and_cluster = data[[\"id\", \"cluster\"]].sort_values(\n",
    "      by=\"cluster\", ascending=True\n",
    "  )\n",
    "\n",
    "  if save_clusters:\n",
    "    df_id_and_cluster.to_csv(\"clusters/word2vec_clusters.csv\", index=False)\n",
    "\n",
    "  dbs = round(davies_bouldin_score(vectors, data[\"cluster\"]), 3)\n",
    "\n",
    "  success(\"David Bouldin score: \" + str(dbs))\n",
    "\n",
    "  return data[[\"id\", \"cluster\"]], vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9286fd73",
   "metadata": {},
   "source": [
    "### Doc2Vec Wrapper\n",
    "- We used the `gensim` package for Doc2Vec model but built a custom wrapper around it to define some useful methods that will be used before the clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f85b76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2VecWrapper:\n",
    "    def __init__(self):\n",
    "        self.tagged_documents = None\n",
    "        self.model = None\n",
    "        self.epochs = None\n",
    "        self.original_data_mapping = None\n",
    "\n",
    "    def init(self, vector_size, alpha, min_alpha, min_count, epochs):\n",
    "        \"\"\"\n",
    "        Initializes the doc2vec model.\n",
    "\n",
    "        vector_size: Dimensionality of the feature vectors.\n",
    "        alpha: The initial learning rate.\n",
    "        min_alpha: Learning rate will linearly drop to min_alpha as training progresses.\n",
    "        min_count: Ignores all words with total frequency lower than this.\n",
    "        epochs: Number of iterations (epochs) over the corpus.\n",
    "        \"\"\"\n",
    "        self.model = Doc2Vec(\n",
    "            vector_size=vector_size,\n",
    "            alpha=alpha,\n",
    "            min_alpha=min_alpha,\n",
    "            min_count=min_count,\n",
    "        )\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def fit(self, tokenized_texts: list[list[str]]):\n",
    "        \"\"\"\n",
    "        Fits the doc2vec model on the data.\n",
    "\n",
    "        tokenized_texts: List of lists of tokens.\n",
    "        \"\"\"\n",
    "        self._tag_data(tokenized_texts)\n",
    "        self.model.build_vocab(self.tagged_documents)\n",
    "\n",
    "        self.original_data_mapping = {\n",
    "            f\"DOC_{str(i)}\": text for i, text in enumerate(tokenized_texts)\n",
    "        }\n",
    "\n",
    "    def _tag_data(self, tokenized_texts: list[list[str]]):\n",
    "        \"\"\"\n",
    "        Tags the data for the doc2vec model.\n",
    "\n",
    "        tokenized_texts: List of lists of tokens.\n",
    "        \"\"\"\n",
    "        self.tagged_documents = [\n",
    "            TaggedDocument(words=_d, tags=[f\"DOC_{str(i)}\"])\n",
    "            for i, _d in enumerate(tokenized_texts)\n",
    "        ]\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains a doc2vec model on the data.\n",
    "        \"\"\"\n",
    "\n",
    "        for epoch in tqdm(\n",
    "            range(self.epochs), desc=\"Training doc2vec\", ascii=True, colour=\"#0077B5\"\n",
    "        ):\n",
    "            self.model.train(\n",
    "                self.tagged_documents, total_examples=self.model.corpus_count, epochs=1\n",
    "            )\n",
    "            # decrease the learning rate\n",
    "            self.model.alpha -= 0.002\n",
    "            # fix the learning rate, no decay\n",
    "            self.model.min_alpha = self.model.alpha\n",
    "\n",
    "    def infer(self, tokenized_text: list[str]):\n",
    "        \"\"\"\n",
    "        Infers a vector for a given tokenized text.\n",
    "\n",
    "        tokenized_text: List of tokens.\n",
    "\n",
    "        returns: Vector representation of the text.\n",
    "        \"\"\"\n",
    "        return self.model.infer_vector(tokenized_text)\n",
    "\n",
    "    def most_similar(self, doc_tag, topn=10):\n",
    "        \"\"\"\n",
    "        Finds the most similar documents to a given document.\n",
    "\n",
    "        doc_tag: Tag of the document.\n",
    "        topn: Number of similar documents to return.\n",
    "\n",
    "        returns: List of tuples (tag, similarity).\n",
    "        \"\"\"\n",
    "        return self.model.dv.most_similar(doc_tag, topn=topn)\n",
    "\n",
    "    def most_similar_original_format(self, doc_tag, topn=10):\n",
    "        \"\"\"\n",
    "        Finds the most similar documents to a given document.\n",
    "\n",
    "        doc_tag: Tag of the document.\n",
    "        topn: Number of similar documents to return.\n",
    "\n",
    "        returns: List of tuples (tag, similarity).\n",
    "        \"\"\"\n",
    "        return [\n",
    "            (self.original_data_mapping[doc_tag], similarity)\n",
    "            for doc_tag, similarity in self.most_similar(doc_tag, topn)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a79ddd",
   "metadata": {},
   "source": [
    "### 3.3 Doc2Vec clustering\n",
    "- Trains a Doc2Vec model on the job descriptions and then clusters them based on the embeddings (vectors) obtained from the model\n",
    "- Either uses KMeans or Gaussian Mixture Model depending on `method` chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaa6cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec_cluster(data,\n",
    "                    save_clusters=True,\n",
    "                    method=\"kmeans\",\n",
    "                    vector_size=100,\n",
    "                    alpha=0.025,\n",
    "                    min_alpha=0.00025,\n",
    "                    min_count=10,\n",
    "                    epochs=500,\n",
    "                    n_clusters=20):\n",
    "  \"\"\"\n",
    "  data: pandas dataframe (cleaned jobs)\n",
    "  save_clusters: Boolean, if True, save the clusters to a csv file in a format \"id, cluster\"\n",
    "  vector_size: Dimensionality of the feature vectors\n",
    "  alpha: The initial learning rate\n",
    "  min_alpha: Learning rate will linearly drop to min_alpha as training progresses\n",
    "  min_count: Ignores all words with total frequency lower than this\n",
    "  epochs: Number of iterations (epochs) over the corpus\n",
    "  n_clusters: Number of clusters to cluster the embeddings into\n",
    "  \"\"\"\n",
    "\n",
    "  jobs_descriptions = data['description'].tolist()\n",
    "\n",
    "  working_on(\"Training doc2vec ...\")\n",
    "  doc2vec = Doc2VecWrapper()\n",
    "  doc2vec.init(vector_size=vector_size, alpha=alpha,\n",
    "               min_alpha=min_alpha, min_count=min_count, epochs=epochs)\n",
    "  doc2vec.fit(jobs_descriptions)\n",
    "  doc2vec.train()\n",
    "\n",
    "  # Cluster similar documents\n",
    "  vectors = [doc2vec.model.dv[i] for i in range(len(doc2vec.model.dv))]\n",
    "  vectors = np.array(vectors)\n",
    "\n",
    "  # Put the embeddings into the original dataframe\n",
    "  data['embeddings'] = vectors.tolist()\n",
    "\n",
    "  # Cluster the embeddings\n",
    "\n",
    "  if method == \"kmeans\":\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42,\n",
    "                    n_init=10).fit(data['embeddings'].tolist())\n",
    "    data['cluster'] = kmeans.labels_.tolist()\n",
    "  elif method == \"gmm\":\n",
    "    gmm = GaussianMixture(n_components=n_clusters, random_state=42).fit(\n",
    "        data['embeddings'].tolist())\n",
    "    data['cluster'] = gmm.predict(data['embeddings'].tolist())\n",
    "\n",
    "  if save_clusters:\n",
    "    data[[\"id\", \"cluster\"]].to_csv(\n",
    "        f\"clusters/doc2vec_{method}_clusters.csv\", index=False)\n",
    "    success(\n",
    "        f\"Clusters saved to clusters/doc2vec_{method}_clusters.csv\")\n",
    "\n",
    "  return data[[\"id\", \"cluster\"]], np.array(data['embeddings'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942f20b7",
   "metadata": {},
   "source": [
    "### Similarity utilities \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d1204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minhashes(shingles, seeds):\n",
    "  hashs = []\n",
    "  for seed in range(seeds):\n",
    "    mini = float('inf')\n",
    "    for shi in shingles:\n",
    "      # hashes a list of strings\n",
    "      hash = 0\n",
    "      for e in shi:\n",
    "        hash = hash ^ mmh3.hash(e, seed)\n",
    "      # find the minimum value\n",
    "      if mini > hash:\n",
    "        mini = hash\n",
    "    hashs.append(mini)\n",
    "  return list(hashs)\n",
    "\n",
    "# get every signature in data\n",
    "\n",
    "\n",
    "def signatures(df, k, seeds):\n",
    "  hash_dic = {}\n",
    "  # If the data in the dataframe is not a list, convert it to a list\n",
    "  if type(df[0]) != list:\n",
    "    df = df.apply(\n",
    "        lambda x: ast.literal_eval(x)\n",
    "    )\n",
    "  for i in range(len(df)):\n",
    "    # make a description into k-shingles\n",
    "    shi = []\n",
    "    for ch in range(len(df[i])-k+1):\n",
    "      shi.append(df[i][ch:ch+k])\n",
    "\n",
    "    hash_dic[i] = minhashes(list(shi), seeds)\n",
    "  return hash_dic\n",
    "\n",
    "\n",
    "def convert_matrix(N, scores):\n",
    "  similarity_matrix = np.zeros((N, N))\n",
    "  for i in range(N):\n",
    "    for j in range(N):\n",
    "      if i == j:\n",
    "        similarity_matrix[i][j] = 1.0\n",
    "      elif i > j:\n",
    "        similarity_matrix[i][j] = scores[(j, i)]\n",
    "      else:\n",
    "        similarity_matrix[i][j] = scores[(i, j)]\n",
    "  return similarity_matrix\n",
    "\n",
    "\n",
    "def find_sim(data, q, seed):\n",
    "  \"\"\"\n",
    "  Finds the similarity between any two job's description for a given dataset using the shingle, minihash \n",
    "  and jaccord similarity.\n",
    "\n",
    "  Args:\n",
    "    data: The \"data\" parameter is the dataset that you want to cluster. It should be a 2D array-like\n",
    "  object, such as a numpy array or a pandas DataFrame, where each row represents a data point and each\n",
    "  column represents a feature of that data point.\n",
    "    q: The q parameter represents the number of shingles ( k = 2 or 3 for small documents such as emails)\n",
    "    seed: The seed parameter represents how mand seeds to use for doing the minihashes\n",
    "\n",
    "  Returns:\n",
    "    A dictionary where the keys are pairs of indices, and the values are scores representing the similarity \n",
    "    between job descriptions at those indices\n",
    "  \"\"\"\n",
    "  sign = signatures(data, q, seed)\n",
    "\n",
    "  score_list = {}\n",
    "  keys = list(sign.keys())\n",
    "  for k in tqdm(range(len(keys)-1), desc='Calculating jaccard similarity', ascii=True, colour=\"#0077B5\"):\n",
    "    for j in range(k+1, len(keys)):\n",
    "      # calculate jaccard simiarity and store the score\n",
    "      score = len(np.intersect1d(\n",
    "          sign[keys[k]], sign[keys[j]]))/len(np.union1d(sign[keys[k]], sign[keys[j]]))\n",
    "      score_list[(keys[k], keys[j])] = score\n",
    "  return score_list\n",
    "\n",
    "\n",
    "def louvain_cluster(N, scores):\n",
    "  \"\"\"\n",
    "  Determines the best partition of a graph for a given similarity score value\n",
    "  using the Louvain Community Detection Algorithm, (Not using Girvan Newman\n",
    "  because it's too time comsuming) and find its Davies-Bouldin index value.\n",
    "\n",
    "  Args:\n",
    "    N: length of data\n",
    "    scores: A dictionary where the keys are pairs of indices, and the values are scores representing the similarity \n",
    "    between job descriptions at those indices\n",
    "\n",
    "  Returns:\n",
    "    the cluster label for each data points and the corresponding Davies-Bouldin index.\n",
    "  \"\"\"\n",
    "  # Create a graph\n",
    "  G = nx.Graph()\n",
    "\n",
    "  # Add nodes (text points)\n",
    "  G.add_nodes_from(range(N))\n",
    "\n",
    "  # Add edges based on similarity scores (you can adjust the threshold)\n",
    "  for idx, idy in scores:\n",
    "    G.add_edge(idx, idy, weight=scores[(idx, idy)]*100)\n",
    "\n",
    "  # Use Louvain community detection algorithm to detect communities\n",
    "  communities = community.louvain_communities(G)\n",
    "\n",
    "  # Retrieve the cluster assignments\n",
    "  clusters = {}\n",
    "  for label, nodes in enumerate(communities):\n",
    "    for idx in nodes:\n",
    "      clusters[idx] = label\n",
    "\n",
    "  # Sort the cluster based on id order and calculate the dbi\n",
    "  sorted_dict = dict(sorted(clusters.items()))\n",
    "  dbi = davies_bouldin_score(convert_matrix(\n",
    "      N, scores), list(sorted_dict.values()))\n",
    "  return sorted_dict, dbi\n",
    "\n",
    "\n",
    "def kmean_cluster(N, scores, n_clusters=20):\n",
    "  \"\"\"\n",
    "  Determines the clusters for a given similarity matrix.\n",
    "\n",
    "  Args:\n",
    "    N: length of data\n",
    "    scores: A dictionary where the keys are pairs of indices, and the values are scores representing the similarity \n",
    "    between job descriptions at those indices\n",
    "    k_max: The parameter `k_max` represents the maximum number of clusters to consider. In the given\n",
    "    code, it is set to 30, which means the function will iterate over values of `k` from 2 to 30\n",
    "    (inclusive) to find the best value of `k` based on the. Defaults to 30\n",
    "    ground_truth: An array of cluster label generated by feature clustering \n",
    "\n",
    "  Returns:\n",
    "    the cluster label for each data points and the corresponding Davies-Bouldin index .\n",
    "  \"\"\"\n",
    "\n",
    "  warnings.filterwarnings(\"ignore\")\n",
    "  similarity_matrix = convert_matrix(N, scores)\n",
    "\n",
    "  # Kmeans clustering\n",
    "  kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "  # Convert similarity to distance\n",
    "  labels = kmeans.fit_predict(similarity_matrix)\n",
    "\n",
    "  dbi = davies_bouldin_score(similarity_matrix, labels)\n",
    "\n",
    "  # Retrieve the cluster assignments\n",
    "  clusters = {}\n",
    "  for label, idx in zip(labels, range(N)):\n",
    "    clusters[idx] = label\n",
    "\n",
    "  return clusters, dbi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfd874e",
   "metadata": {},
   "source": [
    "### 3.4 Similarity clustering \n",
    "- Clusters the job descriptions based on the similarity of the job descriptions to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c083787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_cluster(data, save_clusters=True, q=2, seeds=100, n_clusters=20):\n",
    "  \"\"\"\n",
    "  data : pandas dataframe (cleaned jobs)\n",
    "  save_clusters : Boolean, if True, save the clusters to a csv file in a format \"id, cluster\"\n",
    "  q : number of singles ( k = 2 or 3 for small documents such as emails)\n",
    "  seeds : number of seeds to generate\n",
    "  n_clusters : number of clusters to generate\n",
    "  \"\"\"\n",
    "\n",
    "  df = data\n",
    "\n",
    "  # Number of jobs\n",
    "  N = len(df)\n",
    "  # Give q & seeds for hash to find similarity for each job's descriptions\n",
    "  # q = number of singles ( k = 2 or 3 for small documents such as emails)\n",
    "  q = 2\n",
    "  seeds = 100\n",
    "  working_on(\"Finding similarity ...\")\n",
    "  scores = find_sim(df['description'], q, seeds)\n",
    "\n",
    "  # Plot the network based on similarity and find community based on graph\n",
    "  # To evaluate the functionaly of cluster, calculate the dbi(The minimum\n",
    "  # score is zero, with lower values indicating better clustering)\n",
    "  # (similarity score between 0.0 and 1.0, inclusive, 1.0 stands for perfect match)\n",
    "\n",
    "  working_on(\"Clustering based on community discovery ...\")\n",
    "  cluster_graph, dbi_graph = louvain_cluster(N, scores)\n",
    "  df['cluster_graph'] = cluster_graph\n",
    "\n",
    "  working_on(\"Clustering based on kmean ...\")\n",
    "  cluster_kmean, dbi_kmean = kmean_cluster(N, scores, n_clusters=n_clusters)\n",
    "  df['cluster_kmean'] = cluster_kmean\n",
    "\n",
    "  graph_clusters = df[[\"id\", \"cluster_graph\"]]\n",
    "  graph_clusters = graph_clusters.rename(columns={\"cluster_graph\": \"cluster\"})\n",
    "\n",
    "  kmean_clusters = df[[\"id\", \"cluster_kmean\"]]\n",
    "  kmean_clusters = kmean_clusters.rename(columns={\"cluster_kmean\": \"cluster\"})\n",
    "\n",
    "  if save_clusters:\n",
    "    working_on(\"Saving clusters ...\")\n",
    "    graph_clusters.to_csv(\n",
    "        \"clusters/sim_community_discovery_clusters.csv\", index=False)\n",
    "    kmean_clusters.to_csv(\"clusters/sim_kmeans_clusters.csv\", index=False)\n",
    "    success(\"Clusters saved to clusters/sim_community_discovery_clusters.csv\")\n",
    "    success(\"Clusters saved to clusters/sim_kmeans_clusters.csv\")\n",
    "\n",
    "  sim_matrix = convert_matrix(N, scores)\n",
    "\n",
    "  return graph_clusters, kmean_clusters, sim_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011e05cb",
   "metadata": {},
   "source": [
    "## üìã 4. Ground Truth "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3064ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_general_category(industry, mapping):\n",
    "  \"\"\"\n",
    "  Takes an industry and a mapping of categories to industries,\n",
    "  and returns the general categories that the industry belongs to.\n",
    "\n",
    "  Args:\n",
    "    industry: The industry parameter is the specific industry that you want to map to a general\n",
    "  category. It could be a string representing the industry name.\n",
    "    mapping: The `mapping` parameter is a dictionary where the keys are general categories and the\n",
    "  values are lists of industries that fall under each category.\n",
    "\n",
    "  Returns:\n",
    "    a string that represents the general category of the given industry. If the industry is found in\n",
    "  the mapping, the function returns a comma-separated string of the categories that the industry\n",
    "  belongs to. If the industry is not found in the mapping, the function returns the string 'Other'.\n",
    "  \"\"\"\n",
    "  categories = set()  # Use a set to keep unique categories\n",
    "  for category, industries in mapping.items():\n",
    "    if industry in industries:\n",
    "      categories.add(category)\n",
    "  return \", \".join(categories) if categories else \"Other\"\n",
    "\n",
    "\n",
    "def ground_truth_onehot(data, save_clusters=True, n_clusters=20):\n",
    "  \"\"\"\n",
    "  data: pandas dataframe (cleaned jobs)\n",
    "  save_clusters: Boolean, if True, save the clusters to a csv file in a format \"id, cluster\"\n",
    "  n_clusters: Number of clusters\n",
    "  \"\"\"\n",
    "\n",
    "  data = data[[\"id\", \"title\", \"function\", \"industries\"]].fillna(\"\")\n",
    "\n",
    "  with open(\"data/industries.yaml\", \"r\") as yaml_file:\n",
    "    industry_mapping = yaml.safe_load(yaml_file)\n",
    "  with open(\"data/functions.yaml\", \"r\") as yaml_file:\n",
    "    function_mapping = yaml.safe_load(yaml_file)\n",
    "\n",
    "  industry_categories = list(industry_mapping.keys())\n",
    "  function_categories = list(function_mapping.keys())\n",
    "\n",
    "  data[\"industries\"] = data[\"industries\"].apply(\n",
    "      lambda x: \",\".join([s.strip() for s in x.split(\",\")])\n",
    "  )\n",
    "  data[\"industry_group\"] = data[\"industries\"].apply(\n",
    "      lambda x: \", \".join(\n",
    "          map_to_general_category(ind, industry_mapping) for ind in x.split(\",\")\n",
    "      )\n",
    "  )\n",
    "  data[\"industry_group\"] = data[\"industry_group\"].apply(\n",
    "      lambda x: \", \".join(sorted(set(x.split(\", \"))))\n",
    "  )\n",
    "\n",
    "  data[\"function\"] = data[\"function\"].apply(\n",
    "      lambda x: \",\".join([s.strip() for s in x.split(\",\")])\n",
    "  )\n",
    "  data[\"function_group\"] = data[\"function\"].apply(\n",
    "      lambda x: \", \".join(\n",
    "          map_to_general_category(ind, function_mapping) for ind in x.split(\",\")\n",
    "      )\n",
    "  )\n",
    "  data[\"function_group\"] = data[\"function_group\"].apply(\n",
    "      lambda x: \", \".join(sorted(set(x.split(\", \"))))\n",
    "  )\n",
    "\n",
    "  # Create a new column for each general category and initialize with 0\n",
    "  for category in industry_categories:\n",
    "    data[category] = 0\n",
    "\n",
    "  for category in function_categories:\n",
    "    data[category] = 0\n",
    "\n",
    "  # Iterate through the 'industry_group' column and set corresponding columns to 1\n",
    "  for idx, row in data.iterrows():\n",
    "    industry_groups = row[\"industry_group\"].split(\", \")\n",
    "    function_groups = row[\"function_group\"].split(\", \")\n",
    "\n",
    "    for category in industry_groups:\n",
    "      if category in industry_categories:\n",
    "        data.at[idx, category] = 1\n",
    "\n",
    "    for category in function_groups:\n",
    "      if category in function_categories:\n",
    "        data.at[idx, category] = 1\n",
    "\n",
    "  industries_and_functions = [\n",
    "      \"Technology and Information\",\n",
    "      \"Manufacturing\",\n",
    "      \"Financial and Business Services\",\n",
    "      \"Transportation and Logistics\",\n",
    "      \"Healthcare and Pharmaceuticals\",\n",
    "      \"Retail and Consumer Goods\",\n",
    "      \"Education and Non-profit\",\n",
    "      \"Real Estate and Construction\",\n",
    "      \"Energy and Environment\",\n",
    "      \"Aerospace and Defense\",\n",
    "      \"Food and Beverage\",\n",
    "      \"Services and Miscellaneous\",\n",
    "      \"Management and Leadership\",\n",
    "      \"Manufacturing and Engineering\",\n",
    "      \"Information Technology\",\n",
    "      \"Sales and Marketing\",\n",
    "      \"Administrative and Support\",\n",
    "      \"Writing, Editing, and Creative\",\n",
    "      \"Customer Service\",\n",
    "      \"Legal and Finance\",\n",
    "      \"Research and Analysis\",\n",
    "      \"Human Resources and People Management\",\n",
    "      \"Purchasing and Supply Chain\",\n",
    "      \"Healthcare and Science\",\n",
    "      \"Education and Training\",\n",
    "  ]\n",
    "\n",
    "  kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=10)\n",
    "  data[\"cluster\"] = kmeans.fit_predict(data[industries_and_functions])\n",
    "\n",
    "  # Saving below df is for later comparison of text and feature clustering.\n",
    "  df_id_and_cluster = data[[\"id\", \"cluster\"]].sort_values(\n",
    "      by=\"cluster\", ascending=True\n",
    "  )\n",
    "\n",
    "  if save_clusters:\n",
    "    df_id_and_cluster.to_csv(\n",
    "        \"clusters/ground_truth_onehot.csv\", index=False)\n",
    "    success(\"Clusters saved to clusters/ground_truth_onehot.csv\")\n",
    "\n",
    "  dbs = round(davies_bouldin_score(\n",
    "      data[industries_and_functions], data[\"cluster\"]), 3)\n",
    "\n",
    "  success(\"David Bouldin score: \" + str(dbs))\n",
    "\n",
    "  return data[[\"id\", \"cluster\"]], data[industries_and_functions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740fc2a4",
   "metadata": {},
   "source": [
    "### 4.2 Keyword Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5c902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = {\n",
    "    'Software & IT': ['software', 'it support', 'network administration', 'cybersecurity', 'system analysis'],\n",
    "    'Healthcare & Medicine': ['medical', 'healthcare', 'nursing', 'doctor', 'clinical'],\n",
    "    'Education & Training': ['teaching', 'academic', 'education administration', 'training', 'curriculum development'],\n",
    "    'Engineering & Manufacturing': ['mechanical engineering', 'civil engineering', 'electrical engineering', 'manufacturing', 'quality control'],\n",
    "    'Finance & Accounting': ['accounting', 'financial', 'auditing', 'banking', 'investment'],\n",
    "    'Sales & Marketing': ['sales', 'social media', 'digital marketing', 'public relations', 'brand strategy'],\n",
    "    'Creative Arts & Design': ['graphic design', 'fashion', 'photography', 'graphics', 'creative'],\n",
    "    'Hospitality & Tourism': ['hotel', 'travel consulting', 'event', 'culinary arts', 'tourism'],\n",
    "    'Construction & Real Estate': ['construction', 'architecture', 'urban planning', 'real estate', 'building design'],\n",
    "    'Legal & Compliance': ['legal', 'compliance', 'law', 'regulatory affairs', 'legal advisory'],\n",
    "    'Science & Research': ['research', 'laboratory', 'scientific', 'study', 'experimental'],\n",
    "    'Human Resources & Recruitment': ['employee management', 'recruitment', 'training development', 'organizational', 'workforce planning'],\n",
    "    'Transportation & Logistics': ['transportation', 'supply chain', 'logistics', 'fleet', 'shipping coordination'],\n",
    "    'Agriculture & Environmental': ['farming', 'environmental', 'resource management', 'agricultural', 'sustainable'],\n",
    "    'Retail & Consumer Goods': ['retail', 'goods', 'consumer', 'product marketing', 'merchandising'],\n",
    "    'Media & Communications': ['journalism', 'broadcasting', 'content', 'communication', 'media production'],\n",
    "    'Government & Public Sector': ['public administration', 'policy', 'service', 'government', 'civil'],\n",
    "    'Non-Profit & Social Services': ['non-profit', 'social', 'community', 'charity', 'volunteer'],\n",
    "    'Energy & Utilities': ['energy', 'renewable', 'utility management', 'energy conservation', 'sustainability'],\n",
    "    'Arts & Entertainment': ['acting', 'music performance', 'event coordination', 'entertainment', 'art']\n",
    "}\n",
    "\n",
    "\n",
    "def transform_string(s):\n",
    "  return s[1:-1].replace(\"'\", \"\").replace(\", \", \" \")\n",
    "\n",
    "\n",
    "def refined_keyword_based_categorize_job(row):\n",
    "  # Combine the function, industries, and description into a single string for analysis\n",
    "  combined_info = f\"{row['function']} {row['industries']} {row['description']}\".lower(\n",
    "  )\n",
    "\n",
    "  # Count matching keywords for each category\n",
    "  keyword_counts = {category: sum(keyword in combined_info for keyword in keywords)\n",
    "                    for category, keywords in keywords.items()}\n",
    "\n",
    "  # Determine the category with the most matching keywords\n",
    "  best_category = max(keyword_counts, key=keyword_counts.get)\n",
    "\n",
    "  # If the best category has 0 matches and there's an 'Other' category, assign 'Other'\n",
    "  if keyword_counts[best_category] == 0 and 'Other' in keywords:\n",
    "    return 'Other'\n",
    "  return best_category\n",
    "\n",
    "\n",
    "def ground_truth_keywords(data, save_clusters=False):\n",
    "\n",
    "  df_jobs = data\n",
    "  df_jobs['description'] = df_jobs['description'].apply(transform_string)\n",
    "\n",
    "  working_on(\"Categorizing jobs ...\")\n",
    "  # Apply the refined keyword-based categorization function to each row\n",
    "  df_jobs['category'] = df_jobs.apply(\n",
    "      refined_keyword_based_categorize_job, axis=1)\n",
    "\n",
    "  # Display the updated dataframe with the new 'refined_keyword_based_general_cluster' column\n",
    "  df_jobs[['title', 'function', 'industries', 'description', 'category']].head()\n",
    "\n",
    "  print(\"Number of jobs in each category based on keywords ground truth:\")\n",
    "  for key in keywords.keys():\n",
    "    print(key, sum(df_jobs.category == key))\n",
    "\n",
    "  # Turn to categories\n",
    "  df_jobs['category'] = pd.Categorical(df_jobs['category'])\n",
    "  df_jobs['cluster'] = df_jobs['category'].cat.codes\n",
    "  df_id_and_cluster = df_jobs[[\"id\", \"category\", \"cluster\"]].sort_values(\n",
    "      by=\"cluster\", ascending=True\n",
    "  )\n",
    "\n",
    "  if save_clusters:\n",
    "    df_id_and_cluster.to_csv(\"clusters/ground_truth_keywords.csv\", index=False)\n",
    "    success(\"Clusters saved to clusters/ground_truth_keywords.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c5cde",
   "metadata": {},
   "source": [
    "### 4.3 OpenAI GPT3.5-turbo \n",
    "\n",
    "- Please note, that you need to have an OpenAI API key to be able to use this method. You can get one [here](https://openai.com/blog/openai-api). It cost us around 10$ to run this method. So please be aware of that. Also note, that running this cell takes significantly longer than the other methods.\n",
    "- The infered ground truth data is provided in the handin. (`clusters/ground_truth_gpt.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129bd279",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FILL IN YOUR OPENAI API KEY HERE\n",
    "OPENAI_API_KEY = \"\"\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "\n",
    "  def transform_string(s):\n",
    "    return s[1:-1].replace(\"'\", \"\").replace(\", \", \" \")\n",
    "\n",
    "\n",
    "  def api_call_thread(offer, result_container):\n",
    "    # Load OpenAI API from your environment\n",
    "    API_KEY = OPENAI_API_KEY\n",
    "    client = OpenAI(api_key=API_KEY)\n",
    "    try:\n",
    "      response = client.chat.completions.create(\n",
    "          model=\"gpt-3.5-turbo\",\n",
    "          messages=[\n",
    "              {\"role\": \"system\", \"content\": f\"You are a professional job recruiter. Your task is to categorize a job description with keywords into one and only one of the specified 20 categories: {industries}. You are not allowed to use any other categories.\"},\n",
    "              {\"role\": \"user\", \"content\": \"Classify into one of the given indsutries. Job description: '''would like part ryanair group amazing cabin crew family k crew customer oriented love delivering great service want fast track career opportunity would delighted hear experience required bag enthusiasm team spirit europe largest airline carrying k guest daily flight looking next generation cabin crew join u brand new copenhagen base flying board ryanair group aircraft amazing perk including discounted staff travel destination across ryanair network fixed roster pattern free training industry leading pay journey becoming qualified cabin crew member start week training course learn fundamental skill require part day day role delivering top class safety customer service experience guest course required study exam taking place regular interval training culminates supernumerary flight followed cabin crew wing member ryanair group cabin crew family immersed culture day one career opportunity endless including becoming number base supervisor european base manager regional manager aspire becoming director inflight life cabin crew fun rewarding however demanding position safety number priority required operate early late shift report duty early morning early roster return home midnight afternoon roster morning person think twice applying requirement bag enthusiasm customer serviceoriented background ie previous experience working bar restaurant shop etc applicant must demonstrate legal entitlement work unrestricted basis across euyou must cm cm height must able swim meter unaided help hardworking flexible outgoing friendly personality adaptable happy work shift roster enjoy dealing public ability provide excellent customer service attitude comfortable speaking writing english ease passion travelling meeting new people benefit free cabin crew training course adventure experience lifetime within cabin crew network explore new culture city colleague day flexible day day staff roster unlimited highly discounted staff travel rate sale bonus free uniform year security working financially stable airline daily per diem provided whilst training direct employment contract highly competitive salary package click link start new exciting career sky'''. Keywords: '''management,manufacturing, technology, information,internet'''\"},\n",
    "              {\"role\": \"assistant\", \"content\": \"Hospitality & Tourism\"},\n",
    "              {\"role\": \"user\",\n",
    "              \"content\": f\"Classify into one of the given indsutries. Job description: '''{offer['description']}'''. Keywords: '''{offer['keywords']}'''\"},\n",
    "          ]\n",
    "      )\n",
    "      result_container[\"response\"] = response\n",
    "    except Exception as e:\n",
    "      result_container[\"error\"] = str(e)\n",
    "\n",
    "\n",
    "  def restart_script():\n",
    "    print(\"Restarting script...\")\n",
    "    os.execv(sys.executable, ['python'] + sys.argv)\n",
    "\n",
    "\n",
    "  df = load_data(kind=\"processed\")\n",
    "\n",
    "  df['description'] = df['description'].apply(transform_string)\n",
    "  df['keywords'] = df['function'] + ', ' + df['industries']\n",
    "  job_descriptions = df[['id', 'keywords', 'description']]\n",
    "\n",
    "  industries = \"Software & IT, Healthcare & Medicine, Education & Training, Engineering & Manufacturing, Finance & Accounting, Sales & Marketing, Creative Arts & Design, Hospitality & Tourism, Construction & Real Estate, Legal & Compliance, Science & Research, Human Resources & Recruitment, Transportation & Logistics, Agriculture & Environmental, Retail & Consumer Goods, Media & Communications, Government & Public Sector, Non-Profit & Social Services, Energy & Utilities, Arts & Entertainment\"\n",
    "\n",
    "  ground_truth = {}\n",
    "  yaml_file = 'ground_truth.yaml'\n",
    "  if os.path.exists(yaml_file):\n",
    "    with open(yaml_file, 'r') as file:\n",
    "      ground_truth = yaml.safe_load(file) or {}\n",
    "\n",
    "  for index, offer in job_descriptions.iterrows():\n",
    "    if offer['id'] in ground_truth:\n",
    "      continue\n",
    "\n",
    "    result_container = {}\n",
    "    thread = threading.Thread(target=api_call_thread,\n",
    "                              args=(offer, result_container))\n",
    "    thread.start()\n",
    "    thread.join(timeout=10)\n",
    "\n",
    "    if thread.is_alive() or \"error\" in result_container:\n",
    "      restart_script()\n",
    "\n",
    "    response = result_container.get(\"response\")\n",
    "    if response:\n",
    "      skills = response.choices[0].message.content\n",
    "      ground_truth[offer['id']] = skills\n",
    "      with open(yaml_file, 'w') as file:\n",
    "        yaml.dump(ground_truth, file, default_flow_style=False)\n",
    "      print(f\"Saved ground truth for offer ID: {offer['id']}\")\n",
    "\n",
    "  ground_truth_df = pd.DataFrame.from_dict(\n",
    "      yaml_file, orient='index', columns=['category'])\n",
    "  ground_truth_df.index.name = 'id'\n",
    "  ground_truth_df.reset_index(inplace=True)\n",
    "\n",
    "  mapping_rules = {\n",
    "      'Software & IT': 'Software & IT',\n",
    "      'Creative Arts & Design': 'Creative Arts & Design',\n",
    "      'Engineering & Manufacturing': 'Engineering & Manufacturing',\n",
    "      'Manufacturing': 'Engineering & Manufacturing',\n",
    "      'Human Resources & Recruitment': 'Human Resources & Recruitment',\n",
    "      'Energy & Utilities': 'Energy & Utilities',\n",
    "      'Sales & Marketing': 'Sales & Marketing',\n",
    "      'Consumer Goods': 'Retail & Consumer Goods',\n",
    "      'Transportation & Logistics': 'Transportation & Logistics',\n",
    "      'Finance & Accounting': 'Finance & Accounting',\n",
    "      'Information Technology & Services': 'Software & IT',\n",
    "      'IT & Software': 'Software & IT',\n",
    "      'Non-Profit & Social Services': 'Non-Profit & Social Services',\n",
    "      'Media & Communications': 'Media & Communications',\n",
    "      'Technology': 'Software & IT',\n",
    "      'Hospitality & Tourism': 'Hospitality & Tourism',\n",
    "      'Retail & Consumer Goods': 'Retail & Consumer Goods',\n",
    "      'Technology & Information': 'Software & IT',\n",
    "      'Legal & Compliance': 'Legal & Compliance',\n",
    "      'Healthcare & Medicine': 'Healthcare & Medicine',\n",
    "      'Science & Research': 'Science & Research',\n",
    "      'Information Technology': 'Software & IT',\n",
    "      'Education & Training': 'Education & Training',\n",
    "      'Business & Entrepreneurship': 'Finance & Accounting',\n",
    "      'Logistics & Supply Chain': 'Transportation & Logistics',\n",
    "      'Construction & Real Estate': 'Construction & Real Estate',\n",
    "      'Arts & Entertainment': 'Arts & Entertainment',\n",
    "      'Agriculture & Environmental': 'Agriculture & Environmental',\n",
    "      'Staffing & Recruiting': 'Human Resources & Recruitment',\n",
    "      'Maritime & Transportation': 'Transportation & Logistics',\n",
    "      'Technology & IT': 'Software & IT',\n",
    "      'Public Relations & Communications': 'Media & Communications',\n",
    "      'Customer Service': 'Human Resources & Recruitment',\n",
    "      'Information Technology (IT)': 'Software & IT',\n",
    "      'Manufacturing & Engineering': 'Engineering & Manufacturing',\n",
    "      'Renewable energy': 'Energy & Utilities',\n",
    "      'Government & Public Sector': 'Government & Public Sector',\n",
    "      'Customer Success': 'Sales & Marketing',\n",
    "      'Insurance & Risk Management': 'Finance & Accounting',\n",
    "      'Human Resources': 'Human Resources & Recruitment',\n",
    "      'Marketing & Advertising': 'Sales & Marketing',\n",
    "      'Pharmaceutical & Healthcare': 'Healthcare & Medicine',\n",
    "      'Retail': 'Retail & Consumer Goods',\n",
    "      'Environmental & Sustainability': 'Agriculture & Environmental',\n",
    "      'Real Estate & Construction': 'Construction & Real Estate',\n",
    "      'Aerospace & Defense': 'Engineering & Manufacturing',\n",
    "      'Public Relations': 'Media & Communications',\n",
    "      'Event Planning & Management': 'Hospitality & Tourism',\n",
    "      'Sports & Recreation': 'Arts & Entertainment',\n",
    "      'Medical equipment manufacturing': 'Healthcare & Medicine',\n",
    "      'Renewable Energy': 'Energy & Utilities',\n",
    "      'Technology & Internet': 'Software & IT',\n",
    "      'Technology & Information Technology': 'Software & IT',\n",
    "      'Administration & Office Support': 'Human Resources & Recruitment',\n",
    "      'Information & Technology': 'Software & IT',\n",
    "      'Administration': 'Human Resources & Recruitment',\n",
    "      'Technology & Telecommunications': 'Software & IT',\n",
    "      'Insurance': 'Finance & Accounting',\n",
    "      'Insurance & Financial Services': 'Finance & Accounting',\n",
    "      'Logistics & Supply Chain Management': 'Transportation & Logistics',\n",
    "      'Market Research': 'Sales & Marketing'\n",
    "  }\n",
    "\n",
    "  ground_truth_df['category'] = ground_truth_df['category'].map(mapping_rules)\n",
    "\n",
    "  ground_truth_df['category'] = pd.Categorical(ground_truth_df['category'])\n",
    "  ground_truth_df['cluster'] = ground_truth_df['category'].cat.codes\n",
    "  df_id_and_cluster = ground_truth_df[[\"id\", \"category\", \"cluster\"]].sort_values(\n",
    "      by=\"cluster\", ascending=True\n",
    "  )\n",
    "\n",
    "  df_id_and_cluster.to_csv(\"clusters/ground_truth_gpt.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01bc7dd",
   "metadata": {},
   "source": [
    "## 5. üìà Evaluation \n",
    "\n",
    "- To compare which clustering method is the best, we use the normalized mutual information (NMI) score and Rand Score. The higher the scores, the more similar the clustering is to the ground truth and thus the better the method is.\n",
    "\n",
    "**Rand Score**\n",
    "Rand Score:\n",
    "- a measure of the similarity between two data clusterings. It is calculated based on the agreement between the cluster assignments of the data points.\n",
    "\n",
    "**Normalized Mutual Information (NMI) Score**:\n",
    "- based on mutual information but normalized to account for the size of the clusters. It measures how much information is shared between the clustering assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53471654",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_clustering_methods(paths):\n",
    "  \"\"\"Loads multiple clusterings from specified file paths.\"\"\"\n",
    "  clustering_method = {}\n",
    "  for name, path in paths.items():\n",
    "    clustering_method[name] = pd.read_csv(path)\n",
    "  return clustering_method\n",
    "\n",
    "\n",
    "def compare_clusters_nmi(clusters):\n",
    "  \"\"\"Compares multiple sets of clusters using Normalized Mutual Information (NMI).\"\"\"\n",
    "  nmi_matrix = pd.DataFrame(index=clusters.keys(), columns=clusters.keys())\n",
    "  for name1, data1 in clusters.items():\n",
    "    for name2, data2 in clusters.items():\n",
    "      if name1 != name2:\n",
    "        merged_data = pd.merge(data1, data2, on='id', suffixes=('_1', '_2'))\n",
    "        nmi_score = normalized_mutual_info_score(\n",
    "            merged_data['cluster_1'], merged_data['cluster_2'])\n",
    "        nmi_matrix.loc[name1, name2] = nmi_score\n",
    "      else:\n",
    "        nmi_matrix.loc[name1, name2] = 1.0  # Same clustering method comparison\n",
    "  return nmi_matrix\n",
    "\n",
    "\n",
    "def compare_clusters_rand_index(clusters):\n",
    "  \"\"\"Compares multiple sets of clusters using Rand Index.\"\"\"\n",
    "  rand_matrix = pd.DataFrame(index=clusters.keys(), columns=clusters.keys())\n",
    "  for name1, data1 in clusters.items():\n",
    "    for name2, data2 in clusters.items():\n",
    "      if name1 != name2:\n",
    "        merged_data = pd.merge(data1, data2, on='id', suffixes=('_1', '_2'))\n",
    "        rand = rand_score(\n",
    "            merged_data[\"cluster_1\"], merged_data[\"cluster_2\"])\n",
    "        rand_matrix.loc[name1, name2] = rand\n",
    "      else:\n",
    "        # Same clustering method comparison\n",
    "        rand_matrix.loc[name1, name2] = 1.0\n",
    "  return rand_matrix\n",
    "\n",
    "\n",
    "def evaluation():\n",
    "  paths = {\n",
    "      # 'ground_truth': 'clusters/ground_truth_onehot.csv',\n",
    "      # 'ground_truth': 'clusters/ground_truth_keywords.csv',\n",
    "      'ground_truth_gpt': 'clusters/ground_truth_gpt.csv',\n",
    "      'word2vec': 'clusters/word2vec_clusters.csv',\n",
    "      'tfidf_text': 'clusters/tfidf_clusters_job_desc.csv',\n",
    "      'tfidf_nouns': 'clusters/tfidf_noun_clusters.csv',\n",
    "      'tfidf_adj': 'clusters/tfidf_adj_clusters.csv',\n",
    "      'tfidf_verbs': 'clusters/tfidf_verb_clusters.csv',\n",
    "      'similarity_community_disc': 'clusters/sim_community_discovery_clusters.csv',\n",
    "      'similarity_kmeans': 'clusters/sim_kmeans_clusters.csv',\n",
    "      # 'doc2vec_gmm': 'clusters/doc2vec_gmm_clusters.csv',\n",
    "      'doc2vec_kmeans': 'clusters/doc2vec_kmeans_clusters.csv',\n",
    "  }\n",
    "\n",
    "  # If path does not exist throw error\n",
    "  for name, path in paths.items():\n",
    "    try:\n",
    "      with open(path, 'r'):\n",
    "        pass\n",
    "    except FileNotFoundError:\n",
    "      error(f\"File {path} not found!\")\n",
    "      FileNotFoundError(f\"File {path} not found!\")\n",
    "\n",
    "  # Load the datasets\n",
    "  working_on(\"Comparing clusters ...\")\n",
    "  cluster_methods = load_clustering_methods(paths)\n",
    "\n",
    "  # Compare the clusters and get NMI matrix\n",
    "  nmi_matrix = compare_clusters_nmi(cluster_methods)\n",
    "  rand_index_matrix = compare_clusters_rand_index(cluster_methods)\n",
    "\n",
    "  success(\"Normalized Mutual Information matrix:\")\n",
    "  # Dataframe to string\n",
    "  print(nmi_matrix.to_string(index=False))\n",
    "\n",
    "  success(\"Rand Index matrix:\")\n",
    "  print(rand_index_matrix.to_string(index=False))\n",
    "\n",
    "  ground_truth_nmi = nmi_matrix['ground_truth_gpt'].drop('ground_truth_gpt')\n",
    "  ground_truth_rand = rand_index_matrix['ground_truth_gpt'].drop(\n",
    "      'ground_truth_gpt')\n",
    "\n",
    "  # Select the best clustering method based on NMI and Rand Index\n",
    "  best_nmi = ground_truth_nmi.idxmax()\n",
    "  best_rand = ground_truth_rand.idxmax()\n",
    "\n",
    "  winner(f\"Best clustering method based on NMI: {best_nmi}\")\n",
    "  print(f\"NMI SCORE: {round(ground_truth_nmi[best_nmi],3)}\")\n",
    "\n",
    "  winner(f\"Best clustering method based on Rand Index: {best_rand}\")\n",
    "  print(f\"RAND SCORE: {round(ground_truth_rand[best_rand],3)}\")\n",
    "\n",
    "  # Plot the NMI and Rand Index\n",
    "\n",
    "  # Plotting NMI for each clustering method\n",
    "  plt.figure(figsize=(10, 5))\n",
    "  plt.bar(ground_truth_nmi.index, ground_truth_nmi.values, color='dodgerblue')\n",
    "  plt.xticks(rotation=25)\n",
    "  plt.title(\"NMI for each method compared to ground truth\",\n",
    "            fontsize=14, fontweight='bold')\n",
    "  plt.xlabel(\"Clustering method\", fontsize=12)\n",
    "  plt.ylabel(\"NMI\", fontsize=12)\n",
    "  plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "  plt.tight_layout()\n",
    "\n",
    "  # Save the first plot\n",
    "  plt.savefig('figures/nmi_plot.png')\n",
    "  plt.close()\n",
    "\n",
    "  # Plotting Rand Index for each clustering method\n",
    "  plt.figure(figsize=(10, 5))\n",
    "  plt.bar(ground_truth_rand.index,\n",
    "          ground_truth_rand.values, color='mediumslateblue')\n",
    "  plt.xticks(rotation=25)\n",
    "  plt.title(\"Rand Index for each method compared to ground truth\",\n",
    "            fontsize=14, fontweight='bold')\n",
    "  plt.xlabel(\"Clustering method\", fontsize=12)\n",
    "  plt.ylabel(\"Rand Index\", fontsize=12)\n",
    "  plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "  plt.tight_layout()\n",
    "\n",
    "  # Save the second plot\n",
    "  plt.savefig('figures/rand_plot.png')\n",
    "  plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288814bb",
   "metadata": {},
   "source": [
    "## 6. üî¨ Skill Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Hugging Face Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def words_to_sentence(word_list):\n",
    "  return \" \".join(ast.literal_eval(word_list))\n",
    "\n",
    "\n",
    "def extract_skills_hugging_face(merged):\n",
    "  \"\"\"\n",
    "  Take a dictionary including the id and their cluster label. Extract skills and knowledge from the \n",
    "  raw job description using pretrained model: https://huggingface.co/spaces/jjzha/skill_extraction_demo\n",
    "\n",
    "  Args:\n",
    "    cluster_dict: A dictionary with a key of id and a value of its corresponding cluster label \n",
    "\n",
    "  Returns:\n",
    "    returns a dictionary with a key of cluster label and a value of the extracted skills.\n",
    "  \"\"\"\n",
    "  # Load the data\n",
    "\n",
    "  # Use a pipeline as a high-level helper\n",
    "\n",
    "  token_skill_classifier = pipeline(\n",
    "      \"token-classification\", model=\"jjzha/jobbert_skill_extraction\")\n",
    "  token_knowledge_classifier = pipeline(\n",
    "      \"token-classification\", model=\"jjzha/jobbert_knowledge_extraction\")\n",
    "\n",
    "  # Or use download the model and use it directly\n",
    "  # token_skill_classifier = pipeline(\n",
    "  #    \"token-classification\", model=\"src/model/jobbert_skill_extraction\", aggregation_strategy=\"first\")\n",
    "  # token_knowledge_classifier = pipeline(\n",
    "  #    \"token-classification\", model=\"src/model/jobbert_knowledge_extraction\", aggregation_strategy=\"first\")\n",
    "\n",
    "  N = len(merged)\n",
    "  count = 0\n",
    "  extracted_skills = {\"id\": [], \"skills\": [], \"description_raw\": []}\n",
    "\n",
    "  for _, row in merged.iterrows():\n",
    "    skills = []\n",
    "    job_description = row['description_raw']\n",
    "    job_description = job_description.replace(\"\\n\", \" \")\n",
    "    pattern = r'(?<=[a-z])(?=[A-Z])'\n",
    "    job_description = re.sub(pattern, ' ', job_description)\n",
    "    # Remove the last 56 trash characters\n",
    "    job_description = job_description[:-56]\n",
    "    # print(job_description)\n",
    "\n",
    "    lines = textwrap.wrap(job_description, 500, break_long_words=False)\n",
    "\n",
    "    [skills.extend(skill) for skill in token_skill_classifier(lines)]\n",
    "    [skills.extend(knowledge)\n",
    "     for knowledge in token_knowledge_classifier(lines)]\n",
    "    skills = [entry['word'] for entry in skills]\n",
    "    _id = row['id']\n",
    "\n",
    "    extracted_skills[\"id\"].append(_id)\n",
    "    extracted_skills[\"skills\"].append(skills)\n",
    "    extracted_skills[\"description_raw\"].append(job_description)\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    # Print progress in place\n",
    "    print(f\"\\rüí¨ Skills for {_id} extracted! Progress: {count}/{N}\", end=\"\")\n",
    "\n",
    "  return extracted_skills\n",
    "\n",
    "\n",
    "def skill_extraction_hf(save_skills=False):\n",
    "\n",
    "  df_clean = load_data(\"processed\")\n",
    "  df_raw = load_data(\"raw\")\n",
    "\n",
    "  df_clean = df_clean[['id', 'description']]\n",
    "  df_raw = df_raw[['id', 'description']]\n",
    "\n",
    "  # Obtain the original unprocessed job descriptions from the jobs that appear in the clean dataset\n",
    "  merged = pd.merge(df_clean, df_raw, on='id', how=\"left\",\n",
    "                    suffixes=('_clean', '_raw'))\n",
    "\n",
    "  # Drop duplicates based on id\n",
    "  merged = merged.drop_duplicates(subset=['id'])\n",
    "\n",
    "  skills = extract_skills_hugging_face(merged)\n",
    "\n",
    "  extracted_skills_df = pd.DataFrame(skills)\n",
    "  success(\"Skills extracted\")\n",
    "  # Save results\n",
    "  if save_skills:\n",
    "    name = \"huggleface_skills.csv\"\n",
    "    extracted_skills_df.to_csv(\n",
    "        f\"extracted_skills/{name}\", index=False)\n",
    "    success(f\"Skills saved to extracted_skills/{name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 OpenAI GPT3.5-turbo\n",
    "- Please note, that you need to have an OpenAI API key to be able to use this method. You can get one [here](https://openai.com/blog/openai-api). It cost us around 10$ to run this method. So please be aware of that. Also note, that running this cell takes significantly longer than the other methods.\n",
    "- The extracted skills data is provided in the handin. (`extracted_skills/skills_extracted_gpt3.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc23e789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skills_gpt3(job_description):\n",
    "  # SET YOUR OPENAI API KEY HERE\n",
    "  OPENAI_API_KEY = \"\" \n",
    "  assert OPENAI_API_KEY, \"Please set your OpenAI API key in the section 6.2 of this noteboo\"\n",
    "  client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "  system = f\"You are an expert in job analysis. Your task is to extract at most 10 skills required for a job based on its description. Do not infer or add skills not mentioned in the description. You are required to present me the skills in a raw list format: [skill1, skill2, ... skill10].\"\n",
    "\n",
    "  prompt = f\"Identify at most 10 skills required for this job based on the description. Present them to me in a raw list format [skill1, skill2, ..., skill10]. Description: '{job_description}'\"\n",
    "\n",
    "  response = client.chat.completions.create(model=\"gpt-3.5-turbo\",\n",
    "                                            messages=[\n",
    "                                                {\"role\": \"system\",\n",
    "                                                  \"content\": system},\n",
    "                                                {\"role\": \"user\", \"content\": \"Identify at most 10 skills required for this job based on the description. Present them to me in a raw list format [skill1, skill2, ..., skill10]. Description: 'If you are customer oriented, love delivering a great service & want fast track career opportunities, we would be delighted to hear from you! No experience required, just bags of enthusiasm & team spirit! As Europe‚Äôs largest airline carrying over 550k guests on over 3,000 daily flights, we are looking for the next generation of cabin crew to join us at our brand new Copenhagen base. Flying on board Ryanair Group aircraft there are some amazing perks, including; discounted staff travel to over 230+ destinations across the Ryanair network, a fixed 5/3 roster pattern, free training & industry leading pay.Your journey to becoming a qualified cabin crew member will start on a 6 Week training course where you will learn all of the fundamental skills that you will require as part of your day to day role delivering a top class safety & customer service experience to our guests. '\"},\n",
    "                                                {\"role\": \"assistant\",\n",
    "                                                    \"content\": \"[Customer Service Orientation, Teamwork\"},\n",
    "                                                {\"role\": \"user\", \"content\": prompt},\n",
    "                                            ])\n",
    "  skills_response = response.choices[0].message.content\n",
    "  return skills_response\n",
    "\n",
    "\n",
    "def skill_extraction_gpt(save_skills=False):\n",
    "\n",
    "  df_clean = load_data(\"processed\")\n",
    "  df_raw = load_data(\"raw\")\n",
    "\n",
    "  df_clean = df_clean[['id', 'description']]\n",
    "  df_raw = df_raw[['id', 'description']]\n",
    "\n",
    "  # Obtain the original unprocessed job descriptions from the jobs that appear in the clean dataset\n",
    "  merged = pd.merge(df_clean, df_raw, on='id', how=\"left\",\n",
    "                    suffixes=('_clean', '_raw'))\n",
    "\n",
    "  # Drop duplicates based on id\n",
    "  merged = merged.drop_duplicates(subset=['id'])\n",
    "\n",
    "  extracted_skills = {\"id\": [], \"skills\": [], \"description_raw\": []}\n",
    "\n",
    "  N = len(merged)\n",
    "  count = 0\n",
    "\n",
    "  for _, row in merged.iterrows():\n",
    "    job_description = row['description_raw']\n",
    "    job_description = job_description.replace(\"\\n\", \" \")\n",
    "    pattern = r'(?<=[a-z])(?=[A-Z])'\n",
    "    job_description = re.sub(pattern, ' ', job_description)\n",
    "    # Remove the last 56 trash characters\n",
    "    job_description = job_description[:-56]\n",
    "\n",
    "    skills = extract_skills_gpt3(job_description)\n",
    "    _id = row['id']\n",
    "\n",
    "    extracted_skills[\"id\"].append(_id)\n",
    "    extracted_skills[\"skills\"].append(skills)\n",
    "    extracted_skills[\"description_raw\"].append(job_description)\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    # Print progress in place\n",
    "    print(f\"\\rüí¨ Skills for {_id} extracted! Progress: {count}/{N}\", end=\"\")\n",
    "\n",
    "  extracted_skills_df = pd.DataFrame(extracted_skills)\n",
    "  success(\"Skills extracted\")\n",
    "  if save_skills:\n",
    "    name = \"skills_extracted_gpt3_v2.csv\"\n",
    "    extracted_skills_df.to_csv(\n",
    "        f\"extracted_skills/{name}\", index=False)\n",
    "    success(f\"Skills saved to extracted_skills/{name}\")\n",
    "  return extracted_skills_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f3721f",
   "metadata": {},
   "source": [
    "## 7. üìä Skill Analysis\n",
    "- This section is used to visualize the most important skills for each cluster using *word clouds* and *bar charts* by looking at the most frequent skills for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac56b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(words):\n",
    "\n",
    "  # Use WordNet lemmatizer to reduce words\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  # Apply lemmatization to the words\n",
    "  word = [lemmatizer.lemmatize(word.lower()) for word in words]\n",
    "\n",
    "  word_counts = Counter(word)\n",
    "  return dict(word_counts.most_common())\n",
    "\n",
    "\n",
    "def group_skill_cluster(skills, file):\n",
    "  cluster_skill = {}\n",
    "  for label in set(file[\"cluster\"]):\n",
    "    skill_group = []\n",
    "    for id in file.loc[file[\"cluster\"] == label][\"id\"]:\n",
    "      word_list = [word.strip() for word in skills.loc[skills['id']\n",
    "                                                       == id]['skills'].values[0][1:-1].split(',')]\n",
    "      skill_group.extend(word_list)\n",
    "    cluster_skill[label] = count_words(entry for entry in skill_group)\n",
    "  return cluster_skill\n",
    "\n",
    "\n",
    "def plot_top(words, key, ax, top_n):\n",
    "  sorted_word_frequency = sorted(\n",
    "      words.items(), key=lambda x: x[1], reverse=True)\n",
    "  top_words = dict(sorted_word_frequency[:top_n])\n",
    "\n",
    "  words = list(top_words.keys())\n",
    "  frequencies = list(top_words.values())\n",
    "\n",
    "  ax.bar(words, frequencies, color='dodgerblue')\n",
    "  ax.set_title(f'Top {top_n} Word Frequency')\n",
    "  ax.set_title(f'Cluster{key}')\n",
    "  return ax\n",
    "\n",
    "\n",
    "def plot_wordcloud(words, key, ax):\n",
    "  wordcloud = WordCloud(width=800, height=400,\n",
    "                        background_color='white').generate_from_frequencies(words)\n",
    "  ax.imshow(wordcloud, interpolation='bilinear')\n",
    "  ax.axis('off')\n",
    "  ax.set_title(f' Cluster {key}')\n",
    "  return ax\n",
    "\n",
    "\n",
    "def plot_frequency_histogram(skills, filename, top_n=5):\n",
    "\n",
    "  N = len(skills)\n",
    "  num_cols = 2  # Number of columns in the subplot grid\n",
    "  num_rows = N // num_cols  # Ceiling division to determine the number of rows\n",
    "\n",
    "  # Create subplots\n",
    "  fig, axs = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(N, 2*N))\n",
    "  fig.suptitle('Word Frequency in each cluster', fontsize=20)\n",
    "\n",
    "  # Flatten the axs array for easier indexing\n",
    "  axs = axs.flatten()\n",
    "\n",
    "  # Plot each dictionary on a subplot\n",
    "  for i, (key, word_frequency) in enumerate(skills.items()):\n",
    "    axs[i] = plot_top(word_frequency, key, axs[i], top_n)\n",
    "\n",
    "  # Adjust layout\n",
    "  plt.tight_layout(rect=[0.04, 0, 1, 0.96])\n",
    "  plt.figtext(0.025, 0.5, 'frequency', ha=\"center\",\n",
    "              va=\"top\", fontsize=20, rotation='vertical')\n",
    "  # Show the plot\n",
    "  fig.savefig('figures/'+filename+'_histogram.jpg')\n",
    "\n",
    "  # Create subplots\n",
    "  fig, axs = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(N, 2*N))\n",
    "  fig.suptitle('Word Cloud Subplots', fontsize=16)\n",
    "\n",
    "  # Flatten the axs array for easier indexing\n",
    "  axs = axs.flatten()\n",
    "  # Generate word clouds for each inner dictionary and plot on a subplot\n",
    "  for i, (key, word_frequency) in enumerate(skills.items()):\n",
    "    axs[i] = plot_wordcloud(word_frequency, key, axs[i])\n",
    "\n",
    "  # Remove empty subplots if needed\n",
    "  for j in range(i + 1, num_rows * num_cols):\n",
    "    fig.delaxes(axs[j])\n",
    "\n",
    "  # Adjust layout\n",
    "  plt.tight_layout(rect=[0.04, 0, 1, 0.96])\n",
    "\n",
    "  # Show the plot\n",
    "  fig.savefig('figures/'+filename+'_wordcloud.jpg')\n",
    "\n",
    "\n",
    "def plot_compared(gt, compare, top_n=5):\n",
    "  N = len(gt)\n",
    "  num_cols = 2  # Number of columns in the subplot grid\n",
    "  num_rows = N  # Ceiling division to determine the number of rows\n",
    "\n",
    "  # Create subplots\n",
    "  fig, axs = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(24, 5*N))\n",
    "  fig.suptitle('Word Frequency in each cluster', fontsize=25)\n",
    "\n",
    "  # Flatten the axs array for easier indexing\n",
    "  axs = axs.flatten()\n",
    "  # Plot each dictionary on a subplot\n",
    "  i = 0\n",
    "  for key, word_frequency in gt.items():\n",
    "    axs[i] = plot_top(word_frequency, key, axs[i], top_n)\n",
    "    axs[i+1] = plot_top(compare[key], key, axs[i+1], top_n)\n",
    "    i += 2\n",
    "\n",
    "  # Adjust layout\n",
    "  plt.figtext(0.3, 0.97, \"Ground truth\", ha=\"center\",\n",
    "              va=\"top\", fontsize=20, color=\"black\")\n",
    "  plt.figtext(0.8, 0.97, \"The comparing result\", ha=\"center\",\n",
    "              va=\"top\", fontsize=20, color=\"black\")\n",
    "  plt.figtext(0.025, 0.5, 'frequency', ha=\"center\",\n",
    "              va=\"top\", fontsize=20, rotation='vertical')\n",
    "  plt.tight_layout(rect=[0.04, 0, 1, 0.96])\n",
    "\n",
    "  # Show the plot\n",
    "  fig.savefig('figures/compared_histogram.jpg')\n",
    "\n",
    "  # Create subplots\n",
    "  fig, axs = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(24, 5*N))\n",
    "  fig.suptitle('Word Cloud Subplots', fontsize=25)\n",
    "\n",
    "  # Flatten the axs array for easier indexing\n",
    "  axs = axs.flatten()\n",
    "  # Generate word clouds for each inner dictionary and plot on a subplot\n",
    "  i = 0\n",
    "  for key, word_frequency in gt.items():\n",
    "    axs[i] = plot_wordcloud(word_frequency, key, axs[i])\n",
    "    axs[i+1] = plot_wordcloud(compare[key], key, axs[i+1])\n",
    "    i += 2\n",
    "\n",
    "  # Remove empty subplots if needed\n",
    "  for j in range(i + 1, num_rows * num_cols):\n",
    "    fig.delaxes(axs[j])\n",
    "\n",
    "  # Adjust layout\n",
    "  plt.figtext(0.3, 0.97, \"Ground truth\", ha=\"center\",\n",
    "              va=\"top\", fontsize=20, color=\"black\")\n",
    "  plt.figtext(0.8, 0.97, \"The comparing result\", ha=\"center\",\n",
    "              va=\"top\", fontsize=20, color=\"black\")\n",
    "  plt.tight_layout(rect=[0.04, 0, 1, 0.96])\n",
    "  # Show the plot\n",
    "  fig.savefig('figures/compared_wordcloud.jpg')\n",
    "\n",
    "\n",
    "def skill_analysis(compare, gt=pd.DataFrame()):\n",
    "\n",
    "  # Load the datasets\n",
    "  working_on(\"Skill analysis ...\")\n",
    "\n",
    "  skills = load_data(\"skills_gpt\")\n",
    "\n",
    "  if not gt.empty:\n",
    "    gt_skill = group_skill_cluster(skills, gt)\n",
    "  if not compare.empty:\n",
    "    cluster_skill = group_skill_cluster(skills, compare)\n",
    "    working_on(\"Plot wordcloud and histogram ...\")\n",
    "    plot_frequency_histogram(cluster_skill, 'cluster', 5)\n",
    "\n",
    "  if not gt.empty and not compare.empty:\n",
    "    plot_compared(gt_skill, cluster_skill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aed958",
   "metadata": {},
   "source": [
    "## üöÄ Main Entrypoint\n",
    "\n",
    "Please define here what parts of the project you want to run by setting the corresponding flags to `True`. The main entrypoint will then run the project pipeline for you. \n",
    "\n",
    "Be aware that for the GPT3.5-turbo methods you need to have an OpenAI API key. And to run those part you need to set the keys in the corresponding cells. *Skill extraction* based on Hugging face model and OpenAI's closed source model also take a long time to run. We have provided the data from our run in the handin. So you are not required to run them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have scraped more than 6000 jobs from linkedin from countries of our origin (Denmark, Poland, Hungary, Czechia, Taiwan)\n",
    "# We have provided the raw scraped data in the handin, but if you want to scrape more data yourself, set this to True\n",
    "SCRAPING = False \n",
    "SCRAPING_LOCATION = \"Denmark\"\n",
    "SCRAPING_KEYWORDS = \"all\"\n",
    "SCRAPING_AMOUNT = 50 # max 1000 at a time\n",
    "\n",
    "# We have provided the cleaned data in the handin, however if you want to re-clean the data, set this to True -> it results in about 1900 jobs (removing non english etc...)\n",
    "PREPROCESSING = False \n",
    "\n",
    "# We have provided all the data in the handin, however if you want to re-save the data, set this to True\n",
    "SAVE_CLUSTERING = True \n",
    "\n",
    "# If you want to skip the clustering, set this to False\n",
    "CLUSTERING_TFIDF_TEXT = True \n",
    "CLUSTERING_TFIDF_NOUNS = True \n",
    "CLUSTERING_TFIDF_ADJ = True \n",
    "CLUSTERING_TFIDF_VERBS = True \n",
    "CLUSTERING_WORD2VEC = True \n",
    "CLUSTERING_DOC2VEC_KMEANS = True \n",
    "CLUSTERING_DOC2VEC_GMM = True \n",
    "CLUSTERING_SIMILARITY = True\n",
    "\n",
    "# If you want to skip the ground truth inference, set this to False (we have provided the ground truth from our run in the handin)\n",
    "CLUSTERING_GROUND_TRUTH_ONEHOT = True \n",
    "CLUSTERING_GROUND_TRUTH_KEYWORDS = True \n",
    "\n",
    "# This method requires OpenAI API key and takes a long time to run (we have provided the ground truth from our run in the handin)\n",
    "# clusters/ground_truth_gpt.csv\n",
    "CLUSTERING_GROUND_TRUTH_GPT = False\n",
    "\n",
    "EVALUATION = True\n",
    "\n",
    "# These methods take a long time to run (we have provided the skills from our run in the handin)\n",
    "# extracted_skills/huggleface_skills.csv\n",
    "# extracted_skills/skills_extracted_gpt3.csv\n",
    "SKILL_EXTRACTION_HF = False \n",
    "SKILL_EXTRACTION_GPT = False\n",
    "\n",
    "SKILL_ANALYSIS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"SCRAPING\"\"\"\n",
    "if SCRAPING:\n",
    "  if keywords == \"all\":\n",
    "    keywords = None\n",
    "  scraper = LinkedinScraper(location=SCRAPING_LOCATION,\n",
    "                      keywords=SCRAPING_KEYWORDS,\n",
    "                      amount=SCRAPING_AMOUNT)\n",
    "  scraper.scrape()\n",
    "  success(\"Job posts saved to 'data/raw/jobs.csv'\")\n",
    "\n",
    "\"\"\"PREPROCESSING\"\"\"\n",
    "if PREPROCESSING:\n",
    "  working_on(\"Preprocessing\")\n",
    "  preprocess()\n",
    "  success(\"Data preprocessed and saved to 'data/processed/cleaned_jobs.csv'\")\n",
    "\n",
    "\"\"\"LOAD CLEAN DATA\"\"\"\n",
    "working_on(\"Loading data\")\n",
    "data = load_data(kind=\"processed\")\n",
    "data[\"description\"] = data[\"description\"].apply(ast.literal_eval)\n",
    "success(\"Data loaded\")\n",
    "\n",
    "\"\"\"CLUSTERING METHODS\"\"\"\n",
    "if SAVE_CLUSTERING:\n",
    "  working_on(\"Saving clusters\")\n",
    "  if not os.path.exists(\"clusters\"):\n",
    "    os.mkdir(\"clusters\")\n",
    "  info(\"The clusters will be saved to 'clusters/'\")\n",
    "\n",
    "\"\"\"TF IDF CLUSTERING BASED ON JOB DESCRIPTIONS\"\"\"\n",
    "if CLUSTERING_TFIDF_TEXT:\n",
    "  working_on(\"TFIDF Clustering (job descriptions)\")\n",
    "  tfidf_clusters, tfidf_matrix = TFIDF_cluster(data,\n",
    "                                        save_clusters=SAVE_CLUSTERING,\n",
    "                                        n_clusters=20)\n",
    "  visualize_cluster(tfidf_matrix,\n",
    "              tfidf_clusters[\"cluster\"].to_numpy(),\n",
    "              savefig=True,\n",
    "              filename=\"tfidf_clusters.png\",\n",
    "              name=\"TFIDF Clustering\")\n",
    "  tfidf_matrix = 0\n",
    "\n",
    "\"\"\"TF IDF CLUSTERING BASED ON VERBS FROM THE JOB DESCRIPTION\"\"\"\n",
    "if CLUSTERING_TFIDF_VERBS:\n",
    "  working_on(\"TFIDF Clustering (verbs)\")\n",
    "  data_string_desc = load_data(kind=\"processed\")\n",
    "  tfidf_clusters, tfidf_matrix = TFIDF_verbs_cluster(data_string_desc,\n",
    "                                              save_clusters=SAVE_CLUSTERING,\n",
    "                                              n_clusters=20)\n",
    "  visualize_cluster(tfidf_matrix,\n",
    "              tfidf_clusters[\"cluster\"].to_numpy(),\n",
    "              savefig=True,\n",
    "              filename=\"tfidf_verbs_clusters.png\",\n",
    "              name=\"TFIDF Clustering (verbs)\")\n",
    "  tfidf_matrix = 0\n",
    "\n",
    "\"\"\"TF IDF CLUSTERING BASED ON NOUNS FROM THE JOB DESCRIPTION\"\"\"\n",
    "if CLUSTERING_TFIDF_NOUNS:\n",
    "  working_on(\"TFIDF Clustering (nouns)\")\n",
    "  data_string_desc = load_data(kind=\"processed\")\n",
    "  tfidf_clusters, tfidf_matrix = TFIDF_nouns_cluster(data_string_desc,\n",
    "                                              save_clusters=SAVE_CLUSTERING,\n",
    "                                              n_clusters=20)\n",
    "  visualize_cluster(tfidf_matrix,\n",
    "              tfidf_clusters[\"cluster\"].to_numpy(),\n",
    "              savefig=True,\n",
    "              filename=\"tfidf_nouns_clusters.png\",\n",
    "              name=\"TFIDF Clustering (nouns)\")\n",
    "  tfidf_matrix = 0\n",
    "\"\"\"TF IDF CLUSTERING BASED ON ADJECTIVES FROM THE JOB DESCRIPTION\"\"\"\n",
    "if CLUSTERING_TFIDF_ADJ:\n",
    "  working_on(\"TFIDF Clustering (adjectives)\")\n",
    "  data_string_desc = load_data(kind=\"processed\")\n",
    "  tfidf_clusters, tfidf_matrix = TFIDF_adjectives_cluster(data_string_desc,\n",
    "                                                  save_clusters=SAVE_CLUSTERING,\n",
    "                                                  n_clusters=20)\n",
    "  visualize_cluster(tfidf_matrix,\n",
    "              tfidf_clusters[\"cluster\"].to_numpy(),\n",
    "              savefig=True,\n",
    "              filename=\"tfidf_adjectives_clusters.png\",\n",
    "              name=\"TFIDF Clustering (adjectives)\")\n",
    "  tfidf_matrix = 0\n",
    "\n",
    "\"\"\"WORD2VEC CLUSTERING\"\"\"\n",
    "if CLUSTERING_WORD2VEC:\n",
    "  working_on(\"Word2Vec Clustering\")\n",
    "  word2vec_clusters, word2vec_vectors = word2vec_cluster(data,\n",
    "                                                  save_clusters=SAVE_CLUSTERING,\n",
    "                                                  vector_size=100,\n",
    "                                                  window=5,\n",
    "                                                  min_count=1,\n",
    "                                                  workers=4,\n",
    "                                                  n_clusters=20)\n",
    "  visualize_cluster(word2vec_vectors,\n",
    "              word2vec_clusters[\"cluster\"].to_numpy(),\n",
    "              savefig=True,\n",
    "              filename=\"word2vec_clusters.png\",\n",
    "              name=\"Word2Vec Clustering\")\n",
    "\n",
    "\"\"\"DOC2VEC CLUSTERING GMM\"\"\"\n",
    "if CLUSTERING_DOC2VEC_GMM:\n",
    "  working_on(\"Doc2Vec Clustering GMM\")\n",
    "  doc2vec_clusters, doc2vec_vectors = doc2vec_cluster(data,\n",
    "                                                save_clusters=SAVE_CLUSTERING,\n",
    "                                                method=\"gmm\",\n",
    "                                                vector_size=100,\n",
    "                                                alpha=0.025,\n",
    "                                                min_alpha=0.00025,\n",
    "                                                min_count=10,\n",
    "                                                epochs=300,\n",
    "                                                n_clusters=20)\n",
    "  visualize_cluster(doc2vec_vectors,\n",
    "              doc2vec_clusters[\"cluster\"].to_numpy(),\n",
    "              savefig=True,\n",
    "              filename=\"doc2vec_gmm_clusters.png\",\n",
    "              name=\"Doc2Vec Clustering\")\n",
    "\"\"\"DOC2VEC CLUSTERING KMEANS\"\"\"\n",
    "if CLUSTERING_DOC2VEC_KMEANS:\n",
    "  working_on(\"Doc2Vec Clustering KMeans\")\n",
    "  doc2vec_clusters, doc2vec_vectors = doc2vec_cluster(data,\n",
    "                                                save_clusters=SAVE_CLUSTERING,\n",
    "                                                method=\"kmeans\",\n",
    "                                                vector_size=100,\n",
    "                                                alpha=0.025,\n",
    "                                                min_alpha=0.00025,\n",
    "                                                min_count=10,\n",
    "                                                epochs=300,\n",
    "                                                n_clusters=20)\n",
    "  visualize_cluster(doc2vec_vectors,\n",
    "              doc2vec_clusters[\"cluster\"].to_numpy(),\n",
    "              savefig=True,\n",
    "              filename=\"doc2vec_kmeans_clusters.png\",\n",
    "              name=\"Doc2Vec Clustering\")\n",
    "\n",
    "\"\"\"SIMILARITY CLUSTERING\"\"\"\n",
    "if CLUSTERING_SIMILARITY:\n",
    "  working_on(\"Similarity Clustering\")\n",
    "  data_string_desc = load_data(kind=\"processed\")\n",
    "  sim_community_discovery_clusters, sim_kmeans_clusters, sim_matrix = similarity_cluster(data_string_desc,\n",
    "                                                                                  save_clusters=SAVE_CLUSTERING,\n",
    "                                                                                  q=2,\n",
    "                                                                                  seeds=100,\n",
    "                                                                                  n_clusters=20)\n",
    "  visualize_cluster(sim_matrix,\n",
    "              sim_community_discovery_clusters[\"cluster\"].to_numpy(),\n",
    "              savefig=True,\n",
    "              filename=\"sim_community_discovery_clusters.png\",\n",
    "              name=\"Similarity Clustering (Community Discovery)\")\n",
    "\n",
    "  visualize_cluster(sim_matrix,\n",
    "              sim_kmeans_clusters[\"cluster\"].to_numpy(),\n",
    "              savefig=True,\n",
    "              filename=\"sim_kmeans_clusters.png\",\n",
    "              name=\"Similarity Clustering (KMeans)\")\n",
    "\n",
    "success(\"Clustering completed\")\n",
    "\n",
    "\"\"\"GROUND TRUTH INFERENCE\"\"\"\n",
    "if CLUSTERING_GROUND_TRUTH_ONEHOT:\n",
    "  working_on(\"One hot clustering (industries and functions)\")\n",
    "  onehot_clusters, onehot_features = ground_truth_onehot(data,\n",
    "                                                save_clusters=SAVE_CLUSTERING,\n",
    "                                                n_clusters=20)\n",
    "  visualize_cluster(onehot_features,\n",
    "            onehot_clusters[\"cluster\"].to_numpy(),\n",
    "            savefig=True,\n",
    "            filename=\"onehot_clusters.png\",\n",
    "            name=\"One hot clustering\")\n",
    "  \n",
    "if CLUSTERING_GROUND_TRUTH_KEYWORDS:\n",
    "  working_on(\"Keyword based clustering\")\n",
    "  data_string_desc = load_data(kind=\"processed\")\n",
    "  keywords_clusters = ground_truth_keywords(data_string_desc,\n",
    "                                      save_clusters=SAVE_CLUSTERING)\n",
    "\n",
    "if not os.path.exists(\"clusters/ground_truth_gpt.csv\"):\n",
    "  error(\"GPT Ground truth not found: missing clusters/ground_truth_gpt.csv\")\n",
    "  print(\"First you need to infer the GPT ground truth. You can reproduce the ground truth by running the cell in section 4.3 of the notebook.\") \n",
    "  assert False\n",
    "\n",
    "working_on(\"Loading GPT-3.5 ground truth\")\n",
    "gt = load_data(kind=\"ground_truth_gpt\")\n",
    "visualize_ground_truth(gt, savefig=True, filename=\"ground_truth.png\")\n",
    "success(\"Ground truth loaded\")\n",
    "\n",
    "\"\"\"EVALUATION OF CLUSTERS COMPARED TO GROUND TRUTH\"\"\"\n",
    "if EVALUATION:\n",
    "  evaluation()\n",
    "\n",
    "\"\"\"SKILL EXTRACTION\"\"\"\n",
    "if SKILL_EXTRACTION_GPT:\n",
    "  working_on(\"Skill extraction using GPT-3.5-turbo\")\n",
    "  skill_extraction_gpt(save_skills=True)\n",
    "  success(\n",
    "  \"Skills saved to 'extracted_skills/skills_extracted_gpt3.csv'\")\n",
    "\n",
    "if SKILL_EXTRACTION_HF:\n",
    "  working_on(\"Skill extraction using Hugging Face\")\n",
    "  skill_extraction_hf(save_skills=True)\n",
    "  success(\n",
    "  \"Skills saved to 'extracted_skills/huggleface_skills.csv'\")\n",
    "\n",
    "\"\"\"SKILL ANALYSIS\"\"\"\n",
    "if SKILL_ANALYSIS:\n",
    "  working_on(\"Skill analysis\")\n",
    "  gt = pd.read_csv('clusters/ground_truth_gpt.csv')\n",
    "  compare = pd.read_csv('clusters/tfidf_clusters_job_desc.csv')\n",
    "  skill_analysis(compare, gt)\n",
    "  success(\"Saved to 'figures/.'\")\n",
    "\n",
    "success(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
