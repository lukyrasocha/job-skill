{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2212a55e",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b53e6c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import string\n",
    "import ast\n",
    "import mmh3\n",
    "import matplotlib.pyplot as plt\n",
    "import openai\n",
    "import sys\n",
    "import yaml\n",
    "import threading\n",
    "import networkx as nx\n",
    "import warnings\n",
    "\n",
    "from langdetect import detect\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import gensim.downloader as api\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from networkx.algorithms import community\n",
    "from sklearn.metrics import davies_bouldin_score, rand_score, normalized_mutual_info_score\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f266d602",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "258346b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(kind=\"processed\"):\n",
    "  \"\"\"\n",
    "  Load the data from the data folder.\n",
    "  args:\n",
    "    kind: \"raw\" or \"processed\"\n",
    "  \"\"\"\n",
    "  if kind == \"raw\":\n",
    "    df = pd.read_csv('data/raw/jobs.csv', sep=';')\n",
    "  elif kind == \"processed\":\n",
    "    df = pd.read_csv('data/processed/cleaned_jobs.csv', sep=';')\n",
    "  elif kind == \"ground_truth\":\n",
    "    df = pd.read_csv('clusters/ground_truth_gpt.csv')\n",
    "  elif kind == \"skills\":\n",
    "    df = pd.read_csv('extracted_skills/skills_extracted_gpt3_v2.csv')\n",
    "  return df\n",
    "\n",
    "\n",
    "def is_english(text):\n",
    "  try:\n",
    "    return detect(text) == 'en'\n",
    "  except:\n",
    "    return False\n",
    "\n",
    "\n",
    "def apply_kmeans(tfidf_matrix, k=5):\n",
    "  kmeans = KMeans(n_clusters=k, random_state=0, n_init=10)\n",
    "  return kmeans.fit_predict(tfidf_matrix.toarray())\n",
    "\n",
    "\n",
    "def words2sentence(word_list):\n",
    "  return \" \".join(word_list)\n",
    "\n",
    "\n",
    "def apply_tftidf(data):\n",
    "  vectorizer = TfidfVectorizer()\n",
    "  return vectorizer.fit_transform(data)\n",
    "\n",
    "\n",
    "def visualize_cluster(data,\n",
    "                      cluster,\n",
    "                      reduce_dim=True,\n",
    "                      savefig=False,\n",
    "                      filename=\"cluster.png\",\n",
    "                      name=\"Cluster method\"):\n",
    "  \"\"\"\n",
    "  Visualize the clusters\n",
    "  Data: 2d numpy array of the individual data points that were used for clustering\n",
    "  cluster: 1d numpy array of the cluster labels\n",
    "  reduced_dim: Boolean, if True, perform pca to 2 dimensions\n",
    "  \"\"\"\n",
    "\n",
    "  if reduce_dim:\n",
    "    pca = PCA(n_components=2)\n",
    "    data = pca.fit_transform(data)\n",
    "\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  plt.scatter(data[:, 0], data[:, 1], c=cluster,\n",
    "              cmap='tab20', edgecolor='black', alpha=0.7, s=100)\n",
    "  plt.title(name, fontsize=16, fontweight='bold')\n",
    "  plt.xlabel(\"PCA 1\", fontsize=14)\n",
    "  plt.ylabel(\"PCA 2\", fontsize=14)\n",
    "  plt.grid(True, linestyle='--', alpha=0.5)\n",
    "  plt.tight_layout()\n",
    "  if savefig:\n",
    "    plt.savefig(f\"figures/{filename}\")\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def visualize_ground_truth(gt, savefig=False, filename=\"ground_truth.png\"):\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  plt.bar(gt[\"category\"].value_counts().index,\n",
    "          gt[\"category\"].value_counts().values, color='dodgerblue')\n",
    "\n",
    "  plt.xticks(rotation=75)\n",
    "  plt.title(\"Ground truth distribution\", fontsize=16, fontweight='bold')\n",
    "  plt.xlabel(\"Category\", fontsize=14)\n",
    "  plt.ylabel(\"Count\", fontsize=14)\n",
    "  plt.grid(True, linestyle='--', alpha=0.5)\n",
    "  plt.tight_layout()\n",
    "  if savefig:\n",
    "    plt.savefig(f\"figures/{filename}\")\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def skill_cleanup(data):\n",
    "\n",
    "  # skills is a list of strings, connect them into one string\n",
    "\n",
    "  data[\"skills_string\"] = data[\"skills\"].apply(lambda x: ' '.join(x))\n",
    "\n",
    "  print(data.head())\n",
    "  return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791922bc",
   "metadata": {},
   "source": [
    "## Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54e3c36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def working_on(message):\n",
    "  print(\":wrench: [bold green]WORKING ON[/bold green]: \" + message)\n",
    "\n",
    "\n",
    "def warning(message):\n",
    "  print(\":tomato: [bold red]WARNING[/bold red]: \" + message)\n",
    "\n",
    "\n",
    "def info(message):\n",
    "  print(\":information_source: [bold yellow]INFO[/bold yellow]: \" + message)\n",
    "\n",
    "\n",
    "def success(message):\n",
    "  print(\":white_check_mark: [bold green]SUCCESS[/bold green]: \" + message)\n",
    "\n",
    "\n",
    "def winner(message):\n",
    "    print(\":trophy: [bold yellow]WINNER[/bold yellow]: \" + message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2011947d",
   "metadata": {},
   "source": [
    "## SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34d135a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üíº Scraping job IDs üîç:  12%|\u001b[38;2;0;119;181m##5                 \u001b[0m| 5/40 [00:05<00:37,  1.07s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå One Job ID could not be retrieved ‚ùå\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üíº Scraping job IDs üîç:  22%|\u001b[38;2;0;119;181m####5               \u001b[0m| 9/40 [00:08<00:29,  1.05it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå One Job ID could not be retrieved ‚ùå\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üíº Scraping job IDs üîç:  38%|\u001b[38;2;0;119;181m#######1           \u001b[0m| 15/40 [00:13<00:21,  1.14it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå One Job ID could not be retrieved ‚ùå\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üíº Scraping job IDs üîç:  45%|\u001b[38;2;0;119;181m########5          \u001b[0m| 18/40 [00:16<00:21,  1.04it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå One Job ID could not be retrieved ‚ùå\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üíº Scraping job IDs üîç:  50%|\u001b[38;2;0;119;181m#########5         \u001b[0m| 20/40 [00:18<00:18,  1.10it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå One Job ID could not be retrieved ‚ùå\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üíº Scraping job IDs üîç:  55%|\u001b[38;2;0;119;181m##########4        \u001b[0m| 22/40 [00:20<00:17,  1.03it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå One Job ID could not be retrieved ‚ùå\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "üíº Scraping job IDs üîç:  57%|\u001b[38;2;0;119;181m##########9        \u001b[0m| 23/40 [00:21<00:16,  1.05it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå One Job ID could not be retrieved ‚ùå\n",
      "‚ùå One Job ID could not be retrieved ‚ùå\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "üíº Scraping job IDs üîç:  60%|\u001b[38;2;0;119;181m###########4       \u001b[0m| 24/40 [00:22<00:16,  1.01s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå One Job ID could not be retrieved ‚ùå\n",
      "‚ùå One Job ID could not be retrieved ‚ùå\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üíº Scraping job IDs üîç:  80%|\u001b[38;2;0;119;181m###############2   \u001b[0m| 32/40 [00:31<00:07,  1.05it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå One Job ID could not be retrieved ‚ùå\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üíº Scraping job IDs üîç:  92%|\u001b[38;2;0;119;181m#################5 \u001b[0m| 37/40 [00:35<00:02,  1.09it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå One Job ID could not be retrieved ‚ùå\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üíº Scraping job IDs üîç: 100%|\u001b[38;2;0;119;181m###################\u001b[0m| 40/40 [00:38<00:00,  1.04it/s]\u001b[0m\n",
      "üíº Scraping job details üîç:  52%|\u001b[38;2;0;119;181m######7      \u001b[0m| 500/961 [04:37<04:49,  1.59it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Saving jobs to CSV file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üíº Scraping job details üîç: 100%|\u001b[38;2;0;119;181m#############\u001b[0m| 961/961 [08:46<00:00,  1.82it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Saving jobs to CSV file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class LinkedinScraper:\n",
    "    def __init__(self, location, keywords=None, amount=50):\n",
    "        self.location = location\n",
    "        self.keywords = keywords\n",
    "        self.amount = amount\n",
    "        self.job_ids = []\n",
    "        self.jobs = []\n",
    "\n",
    "        if amount > 1000:\n",
    "            print(\n",
    "                \"‚ö†Ô∏è WARNING: LinkedIn only allows you to scrape 1000 jobs per search. ‚ö†Ô∏è\"\n",
    "            )\n",
    "            print(\"‚ö†Ô∏è WARNING: The amount will be set to 1000. ‚ö†Ô∏è\")\n",
    "            self.amount = 1000\n",
    "        if keywords == None:\n",
    "            self.all_jobs_url = f\"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?location={self.location}\"\n",
    "            self.all_jobs_url += \"&start={}\"\n",
    "        else:\n",
    "            self.all_jobs_url = f\"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords={self.keywords}&location={self.location}\"\n",
    "            self.all_jobs_url += \"&start={}\"\n",
    "\n",
    "        self.job_url = \"https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/{}\"\n",
    "\n",
    "    def save_to_csv(self, filename=\"jobs.csv\"):\n",
    "        print(\"üìù Saving jobs to CSV file...\")\n",
    "        if os.path.isfile(filename):\n",
    "            existing_ids = set(pd.read_csv(filename, sep=\";\")[\"id\"])\n",
    "        else:\n",
    "            existing_ids = set()\n",
    "\n",
    "        # Filter out jobs that are already saved in the CSV\n",
    "\n",
    "        unique_jobs = [job for job in self.jobs if int(job[\"id\"]) not in existing_ids]\n",
    "\n",
    "        if unique_jobs:\n",
    "            df = pd.DataFrame(unique_jobs)\n",
    "            df.to_csv(\n",
    "                filename,\n",
    "                mode=\"a\",\n",
    "                sep=\";\",\n",
    "                header=not os.path.isfile(filename),\n",
    "                index=False,\n",
    "            )\n",
    "\n",
    "    def _get_job_ids(self):\n",
    "        for i in tqdm(\n",
    "            range(0, self.amount, 25),\n",
    "            desc=\"üíº Scraping job IDs üîç\",\n",
    "            ascii=True,\n",
    "            colour=\"#0077B5\",\n",
    "        ):\n",
    "            res = requests.get(self.all_jobs_url.format(i))\n",
    "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "            alljobs_on_this_page = soup.find_all(\"li\")\n",
    "            for x in range(0, len(alljobs_on_this_page)):\n",
    "                try:\n",
    "                    jobid = (\n",
    "                        alljobs_on_this_page[x]\n",
    "                        .find(\"div\", {\"class\": \"base-card\"})\n",
    "                        .get(\"data-entity-urn\")\n",
    "                        .split(\":\")[3]\n",
    "                    )\n",
    "                    self.job_ids.append(jobid)\n",
    "                except:\n",
    "                    print(\"‚ùå One Job ID could not be retrieved ‚ùå\")\n",
    "                    pass\n",
    "\n",
    "    def scrape(self):\n",
    "        # First scrape the job ids\n",
    "        self._get_job_ids()\n",
    "\n",
    "        # Then scrape the job details\n",
    "        for j in tqdm(\n",
    "            range(0, len(self.job_ids)),\n",
    "            desc=\"üíº Scraping job details üîç\",\n",
    "            ascii=True,\n",
    "            colour=\"#0077B5\",\n",
    "        ):\n",
    "            job = {}  # Create a new job dictionary\n",
    "            resp = requests.get(self.job_url.format(self.job_ids[j]))\n",
    "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "            job[\"id\"] = self.job_ids[j]\n",
    "            job[\"date_scraped\"] = pd.Timestamp.now()\n",
    "            job[\"keyword_scraped\"] = self.keywords\n",
    "            job[\"location_scraped\"] = self.location\n",
    "            job[\"linkedin_num\"] = j\n",
    "\n",
    "            try:\n",
    "                job[\"company\"] = (\n",
    "                    soup.find(\"div\", {\"class\": \"top-card-layout__card\"})\n",
    "                    .find(\"a\")\n",
    "                    .find(\"img\")\n",
    "                    .get(\"alt\")\n",
    "                )\n",
    "            except:\n",
    "                job[\"company\"] = None\n",
    "\n",
    "            try:\n",
    "                job[\"title\"] = (\n",
    "                    soup.find(\"div\", {\"class\": \"top-card-layout__entity-info\"})\n",
    "                    .find(\"a\")\n",
    "                    .text.strip()\n",
    "                )\n",
    "            except:\n",
    "                job[\"title\"] = None\n",
    "\n",
    "            try:\n",
    "                job[\"num_applicants\"] = (\n",
    "                    soup.find(\"div\", {\"class\": \"top-card-layout__entity-info\"})\n",
    "                    .find(\"h4\")\n",
    "                    .find(\"span\", {\"class\": \"num-applicants__caption\"})\n",
    "                    .text.strip()\n",
    "                )\n",
    "            except:\n",
    "                job[\"num_applicants\"] = None\n",
    "\n",
    "            try:\n",
    "                job[\"date_posted\"] = (\n",
    "                    soup.find(\"div\", {\"class\": \"top-card-layout__entity-info\"})\n",
    "                    .find(\"h4\")\n",
    "                    .find(\"span\", {\"class\": \"posted-time-ago__text\"})\n",
    "                    .text.strip()\n",
    "                )\n",
    "            except:\n",
    "                job[\"date_posted\"] = None\n",
    "\n",
    "            try:\n",
    "                ul_element = soup.find(\n",
    "                    \"ul\", {\"class\": \"description__job-criteria-list\"}\n",
    "                )\n",
    "\n",
    "                for li_element in ul_element.find_all(\"li\"):\n",
    "                    subheader = li_element.find(\n",
    "                        \"h3\", {\"class\": \"description__job-criteria-subheader\"}\n",
    "                    ).text.strip()\n",
    "                    criteria = li_element.find(\n",
    "                        \"span\",\n",
    "                        {\n",
    "                            \"class\": \"description__job-criteria-text description__job-criteria-text--criteria\"\n",
    "                        },\n",
    "                    ).text.strip()\n",
    "\n",
    "                    if \"Seniority level\" in subheader:\n",
    "                        job[\"level\"] = criteria\n",
    "                    elif \"Employment type\" in subheader:\n",
    "                        job[\"employment_type\"] = criteria\n",
    "                    elif \"Job function\" in subheader:\n",
    "                        job[\"function\"] = criteria\n",
    "                    elif \"Industries\" in subheader:\n",
    "                        job[\"industries\"] = criteria\n",
    "            except:\n",
    "                job[\"level\"] = None\n",
    "                job[\"employment_type\"] = None\n",
    "                job[\"function\"] = None\n",
    "                job[\"industries\"] = None\n",
    "\n",
    "            try:\n",
    "                job[\"description\"] = soup.find(\n",
    "                    \"div\", {\"class\": \"description__text description__text--rich\"}\n",
    "                ).text.strip()\n",
    "            except:\n",
    "                job[\"description\"] = None\n",
    "\n",
    "            self.jobs.append(job)\n",
    "\n",
    "            # Checkpoint to save the jobs to the CSV file every 500 jobs\n",
    "            if (j + 1) % 500 == 0:\n",
    "                self.save_to_csv()\n",
    "                self.jobs = []\n",
    "\n",
    "        if self.jobs:\n",
    "            self.save_to_csv()\n",
    "            \n",
    "scraper = LinkedinScraper(location=\"Denmark\", amount=1000)\n",
    "scraper.scrape()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235f080e",
   "metadata": {},
   "source": [
    "## PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eebf4802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words_with_numbers(word_list):\n",
    "  \"\"\"\n",
    "  Takes a string representation of a list of words as input,\n",
    "  removes any special characters from the words, and then removes any words that contain numbers.\n",
    "\n",
    "  Args:\n",
    "    word_list_str: A string representation of a list of words.\n",
    "\n",
    "  Returns:\n",
    "    The function `remove_words_with_numbers` returns a list of words without any special characters or\n",
    "  numbers.\n",
    "  \"\"\"\n",
    "  word_list_without_special = [\n",
    "      re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", word) for word in word_list\n",
    "  ]\n",
    "  word_list_without_numbers = [\n",
    "      word for word in word_list_without_special if not re.search(r\"\\d\", word)\n",
    "  ]\n",
    "  return word_list_without_numbers\n",
    "\n",
    "\n",
    "def convert_date_posted(date_str, date_scraped):\n",
    "  try:\n",
    "    days_ago = int(date_str.split(' ')[0])\n",
    "    actual_date = pd.to_datetime(date_scraped) - pd.Timedelta(days=days_ago)\n",
    "    return actual_date\n",
    "  except:\n",
    "    return date_scraped  # If the format is not \"x days ago\", use the scraped date\n",
    "\n",
    "\n",
    "def split_combined_words(text):\n",
    "  \"\"\"\n",
    "  Since during the scraping, some words are combined, e.g. \"requirementsYou're\" or \"offerings.If\" we need to split them\n",
    "  Splits words at:\n",
    "  1. Punctuation marks followed by capital letters.\n",
    "  2. Lowercase letters followed by uppercase letters.\n",
    "  \"\"\"\n",
    "  # 1. split\n",
    "  text = re.sub(r'([!?,.;:])([A-Z])', r'\\1 \\2', text)\n",
    "\n",
    "  # 2. split\n",
    "  text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
    "\n",
    "  return text\n",
    "\n",
    "\n",
    "def text_preprocessing(text):\n",
    "  \"\"\"\n",
    "  Preprocesses text by:\n",
    "    - Splitting combined words\n",
    "    - Tokenizing\n",
    "    - Removing stopwords\n",
    "    - Remove punctuation\n",
    "    - Lemmatizing\n",
    "  \"\"\"\n",
    "\n",
    "  text = split_combined_words(text)\n",
    "  text = text.lower()\n",
    "\n",
    "  # Remove punctuation\n",
    "  text = re.sub(f'[{string.punctuation}]', '', text)\n",
    "  # Remove numbers\n",
    "  text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "  tokens = word_tokenize(text)\n",
    "\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "\n",
    "  punctuation = {'!', ',', '.', ';', ':', '?',\n",
    "                 '(', ')', '[', ']', '-', '+', '\"', '*', '‚Äî', '‚Ä¢', '‚Äô', '‚Äò', '‚Äú', '‚Äù', '``'}\n",
    "  tokens = [w for w in tokens if w not in punctuation]\n",
    "\n",
    "  # Remove last 3 words since they are always the same (scraped buttons from the website)\n",
    "  tokens = tokens[:-3]\n",
    "\n",
    "  return tokens\n",
    "\n",
    "\n",
    "def main():\n",
    "  \"\"\"\n",
    "  Main function of the preprocessing module.\n",
    "  Loads the raw data and does the following:\n",
    "  - Checks for english language\n",
    "  - Removes rows with missing descriptions\n",
    "  - Inferes the date posted\n",
    "  - Preprocesses the description\n",
    "  - Saves the preprocessed data to data/processed/cleaned_jobs.csv\n",
    "  \"\"\"\n",
    "\n",
    "  working_on(\"Loading data\")\n",
    "  df = load_data(kind=\"raw\")\n",
    "\n",
    "  # Remove duplicates\n",
    "  df.drop_duplicates(subset=['id'], inplace=True)\n",
    "  df.drop_duplicates(subset=['description'], inplace=True)\n",
    "  # Filter out jobs with missing descriptions\n",
    "  df = df[df['description'].notna()]\n",
    "\n",
    "  working_on(\"Filtering out non-english descriptions ...\")\n",
    "  for index, row in df.iterrows():\n",
    "    if not is_english(row['description'][:100]):\n",
    "      df.drop(index, inplace=True)\n",
    "\n",
    "  working_on(\"Infering dates ...\")\n",
    "  df['date_posted'] = df.apply(lambda x: convert_date_posted(\n",
    "      x['date_posted'], x['date_scraped']), axis=1)\n",
    "\n",
    "  # Lower case all text\n",
    "  df['title'] = df['title'].str.lower()\n",
    "  df['function'] = df['function'].str.lower()\n",
    "  df['industries'] = df['industries'].str.lower()\n",
    "  df['industries'] = df['industries'].str.replace('\\n', ' ')\n",
    "\n",
    "  # Removing outliers (where industries is whole description of offer)\n",
    "  df[\"industries_length\"] = df[\"industries\"].str.split()\n",
    "  df[\"industries_length\"] = df[\"industries_length\"].str.len()\n",
    "  df = df[df[\"industries_length\"] < 15]\n",
    "  df.drop(columns=[\"industries_length\"], inplace=True)\n",
    "\n",
    "  df[\"industries\"] = df[\"industries\"].str.replace(\" and \", \",\")\n",
    "  df[\"function\"] = df[\"function\"].str.replace(\" and \", \",\")\n",
    "  df[\"industries\"] = df[\"industries\"].str.replace(\"/\", \",\")\n",
    "  df[\"function\"] = df[\"function\"].str.replace(\"/\", \",\")\n",
    "\n",
    "  df[\"industries\"] = df[\"industries\"].str.replace(r\",,|, ,\", \",\")\n",
    "  df[\"function\"] = df[\"function\"].str.replace(r\",,|, ,\", \",\")\n",
    "\n",
    "  tqdm.pandas(desc=\"üêº Preprocessing description\", ascii=True, colour=\"#0077B5\")\n",
    "\n",
    "  df['description'] = df['description'].progress_apply(text_preprocessing)\n",
    "\n",
    "  # Remove rows with empty descriptions or descriptions containing less than 3 words\n",
    "  df = df[df['description'].map(len) > 3]\n",
    "\n",
    "  # Remove special characters and numbers from the tokenized list\n",
    "  df['description'] = df['description'].apply(\n",
    "      lambda x: remove_words_with_numbers(x)\n",
    "  )\n",
    "\n",
    "  df = df.reset_index(drop=True)\n",
    "\n",
    "  working_on(\"Saving preprocessed data ...\")\n",
    "  df.to_csv('data/processed/cleaned_jobs.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e680a9f5",
   "metadata": {},
   "source": [
    "## LOAD CLEAN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea745bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":wrench: [bold green]WORKING ON[/bold green]: Loading data\n",
      ":white_check_mark: [bold green]SUCCESS[/bold green]: Data loaded\n"
     ]
    }
   ],
   "source": [
    "df = load_data(kind=\"processed\")\n",
    "working_on(\"Loading data\")\n",
    "data = load_data(kind=\"processed\")\n",
    "data[\"description\"] = data[\"description\"].apply(ast.literal_eval)\n",
    "success(\"Data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3935e1d",
   "metadata": {},
   "source": [
    "## TF IDF CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4307d70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIDF_cluster(data, save_clusters=True):\n",
    "    \"\"\"\n",
    "    data: pandas dataframe (cleaned jobs)\n",
    "    save_clusters: Boolean, if True, save the clusters to a csv file in a format \"id, cluster\"\n",
    "    \"\"\"\n",
    "\n",
    "    data[\"description\"] = data[\"description\"].apply(words2sentence)\n",
    "    tfidf_matrix = apply_tftidf(data[\"description\"])\n",
    "\n",
    "    data[\"cluster\"] = apply_kmeans(tfidf_matrix, k=20)\n",
    "\n",
    "    if save_clusters:\n",
    "        data[[\"id\", \"cluster\"]].to_csv(\"clusters/tfidf_clusters.csv\", index=False)\n",
    "\n",
    "    dbs = round(davies_bouldin_score(tfidf_matrix.toarray(), data[\"cluster\"]), 3)\n",
    "\n",
    "    success(\"David Bouldin score: \" + str(dbs))\n",
    "\n",
    "    return data[[\"id\", \"cluster\"]], tfidf_matrix.toarray()\n",
    "\n",
    "\n",
    "def TFIDF_industries_and_functions_cluster(data, save_clusters=False):\n",
    "    \"\"\"\n",
    "    data: pandas dataframe (cleaned jobs)\n",
    "    save_clusters: Boolean, if True, save the clusters to a csv file in a format \"id, cluster\"\n",
    "    \"\"\"\n",
    "\n",
    "    data[\"industries\"] = data['function'] + ', ' + data['industries']\n",
    "    data['industries'] = data['industries'].str.replace(',,', ',', regex=False)\n",
    "\n",
    "    tfidf_matrix = apply_tftidf(data[\"industries\"])\n",
    "\n",
    "    data[\"cluster\"] = apply_kmeans(tfidf_matrix, k=20)\n",
    "\n",
    "    if save_clusters:\n",
    "        data[[\"id\", \"cluster\"]].to_csv(\"clusters/tfidf_industries_and_functions_clusters.csv\", index=False)\n",
    "\n",
    "    dbs = round(davies_bouldin_score(tfidf_matrix.toarray(), data[\"cluster\"]), 3)\n",
    "\n",
    "    success(\"David Bouldin score: \" + str(dbs))\n",
    "\n",
    "    return data[[\"id\", \"cluster\"]], tfidf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9385bbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":white_check_mark: [bold green]SUCCESS[/bold green]: David Bouldin score: 5.13\n",
      ":white_check_mark: [bold green]SUCCESS[/bold green]: David Bouldin score: 2.24\n",
      "(              id  cluster\n",
      "0     3701420292        6\n",
      "1     3712625082        6\n",
      "2     3664534049        6\n",
      "3     3576920179        6\n",
      "4     3590303623        6\n",
      "...          ...      ...\n",
      "1860  3666672537        7\n",
      "1861  3686843498        7\n",
      "1862  3748928225        6\n",
      "1863  3585733023        6\n",
      "1864  3748579693        8\n",
      "\n",
      "[1865 rows x 2 columns], array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]])) (              id  cluster\n",
      "0     3701420292        8\n",
      "1     3712625082        6\n",
      "2     3664534049        6\n",
      "3     3576920179        6\n",
      "4     3590303623        8\n",
      "...          ...      ...\n",
      "1860  3666672537       17\n",
      "1861  3686843498       17\n",
      "1862  3748928225        2\n",
      "1863  3585733023        9\n",
      "1864  3748579693        8\n",
      "\n",
      "[1865 rows x 2 columns], array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]))\n"
     ]
    }
   ],
   "source": [
    "clusters_text = TFIDF_cluster(data, save_clusters=False)\n",
    "clusters_industries = TFIDF_industries_and_functions_cluster(data, save_clusters=False)\n",
    "print(clusters_text, clusters_industries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b22880",
   "metadata": {},
   "source": [
    "## WORD2VEC CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "031faa4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/heni/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "def words_to_sentence(word_list):\n",
    "    return \" \".join(word_list)\n",
    "\n",
    "def remove_words_with_numbers(word_list_str):\n",
    "    \"\"\"\n",
    "    Takes a string representation of a list of words as input,\n",
    "    removes any special characters from the words, and then removes any words that contain numbers.\n",
    "\n",
    "    Args:\n",
    "      word_list_str: A string representation of a list of words.\n",
    "\n",
    "    Returns:\n",
    "      The function `remove_words_with_numbers` returns a list of words without any special characters or\n",
    "    numbers.\n",
    "    \"\"\"\n",
    "    word_list = ast.literal_eval(word_list_str)\n",
    "    word_list_without_special = [\n",
    "        re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", word) for word in word_list\n",
    "    ]\n",
    "    word_list_without_numbers = [\n",
    "        word for word in word_list_without_special if not re.search(r\"\\d\", word)\n",
    "    ]\n",
    "    return word_list_without_numbers\n",
    "\n",
    "df = load_data(kind=\"processed\")\n",
    "\n",
    "# Apply the function to the 'words' column of the DataFrame\n",
    "# df[\"description\"] = df[\"description\"].apply(\n",
    "#     lambda x: remove_words_with_numbers(x)\n",
    "# )\n",
    "\n",
    "##########################################\n",
    "\n",
    "##### PRETRAINED MODEL WORD2VEC ######\n",
    "# model = KeyedVectors.load_word2vec_format('models/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "# def description_to_vector(description, model):\n",
    "#     valid_words = [word for word in description if word in model.key_to_index]\n",
    "#     if valid_words:\n",
    "#         return np.mean([model[word] for word in valid_words], axis=0)\n",
    "#     else:\n",
    "#         return np.zeros(model.vector_size)\n",
    "# df['vector'] = df['description'].apply(lambda desc: description_to_vector(desc, model))\n",
    "###################################\n",
    "\n",
    "######### Word2Vec #########\n",
    "model = Word2Vec(sentences=df['description'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "def description_to_vector(description):\n",
    "    # Filter out words not in the model's vocabulary\n",
    "    valid_words = [word for word in description if word in model.wv.key_to_index]\n",
    "    if valid_words:\n",
    "        # Average the vectors of the words in the description\n",
    "        return np.mean(model.wv[valid_words], axis=0)\n",
    "    else:\n",
    "        # If no valid words, return a zero vector\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "df['vector'] = df['description'].apply(description_to_vector)\n",
    "##################################\n",
    "# Convert list of vectors to a 2D array for clustering\n",
    "vectors = np.array(df['vector'].tolist())\n",
    "\n",
    "# Apply KMeans clustering\n",
    "kmeans = KMeans(n_clusters=20) \n",
    "df['cluster'] = kmeans.fit_predict(vectors)\n",
    "\n",
    "df_id_and_cluster = df[[\"id\", \"cluster\"]].sort_values(\n",
    "    by=\"cluster\", ascending=True\n",
    ")\n",
    "df_id_and_cluster.to_csv(\"clusters/word2wev_clustering_id.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef0e832",
   "metadata": {},
   "source": [
    "## FEATURE CLUSTERING (ONE HOT ENCODED FUNCTIONS AND INDUSTRIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c63142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4a79ddd",
   "metadata": {},
   "source": [
    "## DOC2VEC CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4aaa6cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training doc2vec: 100%|\u001b[38;2;0;119;181m#######################\u001b[0m| 100/100 [01:52<00:00,  1.12s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.2849133e-03 -2.6715684e-03  2.1414864e-03  6.0340287e-03\n",
      "  9.8793395e-03 -6.2247431e-03  7.2544194e-03  2.9419660e-05\n",
      "  6.4916764e-03  5.3687263e-03 -1.8554962e-03 -4.9450602e-03\n",
      " -5.1674973e-03 -4.8928233e-03 -3.4063600e-03  9.3516108e-05\n",
      "  2.1961343e-03 -2.6351481e-03 -3.8856696e-03 -3.8499737e-03\n",
      " -4.4055749e-03 -7.7464636e-03 -5.6949425e-03  3.2602048e-03\n",
      " -7.1620359e-03  7.1301744e-03  4.6692286e-03 -5.0445641e-03\n",
      "  1.0659301e-03 -8.3084460e-03  6.8165065e-04  2.3031044e-03\n",
      " -2.6062273e-03 -9.0058362e-03  4.2783534e-03 -3.9463867e-03\n",
      " -7.0883157e-03 -7.4489675e-03  8.5577607e-04  8.2220305e-03\n",
      " -4.1868580e-03  9.0158870e-03 -4.6184212e-03 -2.8841526e-03\n",
      "  3.0309998e-03 -9.0387370e-03 -4.4420152e-03  9.2533948e-03\n",
      " -9.1791330e-03  5.9312833e-03]\n",
      "['crediwire', 'startup', 'group', 'young', 'passionate', 'individual', 'ambition', 'eager', 'shape', 'future', 'youre', 'passionate', 'design', 'development', 'dream', 'joining', 'next', 'fintech', 'generation', 'read', 'onthe', 'role', 'new', 'ux', 'designer', 'youll', 'make', 'user', 'journey', 'mockups', 'prototype', 'youll', 'work', 'hand', 'hand', 'many', 'department', 'stakeholder', 'important', 'great', 'collaboration', 'listening', 'skill', 'additionally', 'flair', 'background', 'graphic', 'design', 'would', 'advantageous', 'youll', 'play', 'role', 'shaping', 'product', 'key', 'responsibility', 'craft', 'refine', 'user', 'journey', 'wireframes', 'detailed', 'ui', 'design', 'develop', 'user', 'flow', 'focus', 'accounting', 'bank', 'system', 'simplify', 'intricate', 'idea', 'intuitive', 'mockups', 'engage', 'closely', 'user', 'capture', 'specific', 'need', 'collaborate', 'across', 'department', 'enhance', 'user', 'experience', 'fluent', 'danish', 'skill', 'background', 'ux', 'medium', 'interaction', 'design', 'proficient', 'design', 'tool', 'figma', 'sketch', 'adobe', 'xd', 'vision', 'deep', 'understanding', 'usercentric', 'design', 'testing', 'methodology', 'strong', 'collaboration', 'skill', 'bridge', 'customer', 'aspiration', 'vision', 'passionate', 'merging', 'creativity', 'commercial', 'outcome', 'ability', 'execute', 'idea', 'handson', 'foundation', 'design', 'theory', 'crediwire', 'company', 'crediwire', 'create', 'better', 'financial', 'experience', 'smes', 'accountant', 'bank', '', 'get', 'morning', 'collect', 'structure', 'activate', 'financial', 'data', 'create', 'knowledge', 'better', 'decision', 'ultimately', 'stronger', 'company', 'crediwire', 'platform', 'financial', 'optimization', 'successful', 'candidate', 'interested', 'working', 'startup', 'possibility', 'grow', 'promising', 'nextgeneration', 'fintech', 'company', 'quickly', 'become', 'valued', 'member', 'core', 'team', 'type', 'want', 'go', 'aboveandbeyond', 'offer', 'great', 'chance', 'work', 'highly', 'professional', 'team', 'professional', 'informal', 'work', 'environment', 'steep', 'learning', 'curve', 'interesting', 'opportunity', 'possibility', 'learn', 'introduce', 'new', 'datadriven', 'technology', 'great', 'office', 'centre', 'copenhagen', 'pension', 'plan', 'bank', 'advantage', 'monthly', 'crediwire', 'dinner', 'crediwire', 'people', 'behind', 'around', 'company', 'crediwire', 'run', 'advised', 'denmark', 'prominent', 'businesspeople', 'board', 'crediwire', 'jrgen', 'horwitz', 'former', 'ceo', 'banker', 'association', 'chair', 'credi', 'wire', 'board', 'also', 'contains', 'profile', 'jesper', 'theill', 'eriksen', 'ceo', 'templafy', 'former', 'ceo', 'tdc', 'consumer', 'carsten', 'dyrup', 'cfo', 'telia', 'denmark', 'jens', 'srensen', 'former', 'cto', 'e', 'conomics', 'research', 'board', 'carsten', 'rohde', 'professor', 'head', 'department', 'accounting', 'auditing', 'copenhagen', 'business', 'school', 'fritz', 'henglein', 'professor', 'computer', 'science', 'copenhagen', 'university', 'submit', 'application', 'portfolio', 'here']\n",
      "(\"['rwe', 'offshore', 'wind', 'gmb', 'hto', 'start', 'soon', 'possible', 'full', 'time', 'permanent', 'joining', 'u', 'mean', 'opportunity', 'work', 'exciting', 'sustainable', 'growing', 'industry', 'seeing', 'develop', 'whilst', 'contributing', 'large', 'complex', 'project', 'global', 'number', 'two', 'leader', 'offshore', 'wind', 'three', 'four', 'rotation', 'month', 'programme', 'free', 'explore', 'offshore', 'core', 'business', 'area', 'origination', 'development', 'engineering', 'construction', 'operation', 'commercialisation', 'programme', 'opportunity', 'continue', 'career', 'beyond', 'new', 'spirit', 'programme', 'area', 'get', 'ready', 'identify', 'challenge', 'really', 'inspire', 'extend', 'network', 'become', 'equipped', 'future', 'leadership', 'career', 'offshore', 'business', 'future', 'plan', 'join', 'rotational', 'programme', 'experience', 'different', 'core', 'offshore', 'business', 'area', 'month', 'work', 'different', 'project', 'initiative', 'gain', 'offshore', 'big', 'picture', 'deciding', 'permanent', 'position', 'programme', 'continuously', 'build', 'technical', 'business', 'leadership', 'skill', 'training', 'networking', 'seek', 'international', 'assignment', 'global', 'perspective', 'including', 'european', 'u', 'apac', 'market', 'prepare', 'potential', 'leadership', 'role', 'demonstrate', 'strategic', 'decisionmaking', 'ability', 'stay', 'updated', 'industry', 'trend', 'innovation', 'powerful', 'skill', 'year', 'work', 'experience', 'demanding', 'role', 'example', 'engineering', 'consulting', 'strategysuccessfully', 'completed', 'university', 'study', 'engineering', 'natural', 'science', 'business', 'commercial', 'mindset', 'affinity', 'global', 'power', 'market', 'esp', 'renewable', 'energy', 'curiosity', 'openness', 'towards', 'change', 'courage', 'question', 'status', 'quo', 'motivation', 'take', 'initiative', 'develop', 'idea', 'concept', 'personality', 'ability', 'act', 'comfortably', 'crosscultural', 'team', 'desire', 'take', 'potential', 'leadership', 'role', 'future', 'openness', 'relocate', 'global', 'rotation', 'programme', 'fluent', 'written', 'spoken', 'english', 'benefit', 'rely', 'broad', 'insight', 'offshore', 'wind', 'technology', 'rotating', 'core', 'business', 'area', 'along', 'offshore', 'value', 'chain', '', 'individualized', 'journey', 'selforganizing', 'rotation', 'mentor', 'provide', 'guide', 'along', 'way', 'great', 'opportunity', 'also', 'develop', 'offthejob', 'offshore', 'academy', 'including', 'onboarding', 'week', 'essen', 'campus', 'networking', 'opportunity', 'throughout', 'programme', 'embrace', 'unique', 'job', 'opportunity', 'join', 'u', 'enabling', 'greener', 'future', 'rwe', 'offshore', 'windrwecomcareer', 'benefit', 'many', 'advantage', '', 'curious', 'click', 'apply', 'click', 'ad', 'code', 'application', 'deadline', 'question', 'felix', 'gonzlez', 'fernndez', 'look', 'forward', 'receiving', 'application', 'value', 'diversity', 'therefore', 'welcome', 'application', 'irrespective', 'gender', 'disability', 'nationality', 'ethnic', 'social', 'background', 'religion', 'belief', 'age', 'sexual', 'orientation', 'identity', 'inclusionmatters', 'course', 'find', 'u', 'linked', 'twitter', 'xing', 'renewables', 'future', 'business', 'offshore', 'wind', 'pivotal', 'element', 'carbon', 'neutral', 'ambition', 'fixed', 'bottom', 'floating', 'turbine', '', 'plus', 'others', 'totalling', 'mw', 'operate', 'partner', '', 'already', 'generator', 'globally', 'plan', 'triple', 'gw', 'output', 'led', 'landmark', 'project', 'sofia', 'mw', 'kaskasi', 'mw', 'offshore', 'wind', 'constantly', 'evolving', 'technical', 'practical', 'social', 'challenge', 'financial', 'stability', 'mean', 'freedom', 'act', 'conviction', 'courage', 'pivot', 'harness', 'new', 'technology', '', 'continuously', 'improve', 'together']\", 0.7727063298225403)\n",
      "--------------------------------------------------\n",
      "(\"['hey', 'david', 'kennedy', 'recruitment', 'lookout', 'amazing', 'freelance', 'recruiter', 'join', 'team', 'growing', 'network', 'across', 'europe', 'youre', 'passionate', 'connecting', 'talented', 'individual', 'dream', 'job', 'knack', 'hiring', 'within', 'poland', 'want', 'hear', 'freelance', 'role', 'offer', 'commissionbased', 'structure', 'meaning', 'earnings', 'directly', 'tied', 'successful', 'candidate', 'placement', 'position', 'freelance', 'recruiter', 'polishlocation', 'poland', 'employment', 'type', 'self', 'employed', 'freelance', 'able', 'invoice', 'remuneration', 'commission', 'per', 'placed', 'candidate', 'responsibility', 'dive', 'various', 'channel', 'choice', 'like', 'job', 'board', 'social', 'medium', 'networking', 'find', 'attract', 'topnotch', 'candidate', 'conduct', 'interview', 'ass', 'qualification', 'determine', 'perfect', 'fit', 'specific', 'job', 'role', 'work', 'closely', 'service', 'delivery', 'manager', 'managing', 'director', 'make', 'recruitment', 'process', 'run', 'smoothly', 'candidate', 'keep', 'steady', 'pipeline', 'talented', 'candidate', 'future', 'opportunity', 'build', 'lasting', 'relationship', 'coordinate', 'interview', 'candidate', 'hiring', 'manager', 'like', 'pro', 'keep', 'candidate', 'profile', 'at', 'welldescribed', 'updated', 'future', 'reference', 'requirement', 'proficient', 'english', 'spoken', 'written', 'must', 'previous', 'experience', 'recruiter', 'within', 'marketingfmcge', 'commerce', 'within', 'poland', 'want', 'recruitershave', 'knack', 'talent', 'spotting', 'eye', 'detail', 'ability', 'turnaround', 'candidate', 'quickly', 'ability', 'juggle', 'multiple', 'recruitment', 'project', 'work', 'independently', 'great', 'time', 'management', 'responsiveness', 'organizational', 'ability', 'sourcing', 'strategy', 'utilising', 'network', 'benefit', 'enjoy', 'competitive', 'alwaysontime', 'zero', 'base', 'commissionbased', 'compensation', 'structureembrace', 'freedom', 'flexibility', 'fully', 'remote', 'work', 'anywhere', 'world', 'managing', 'schedule', 'dive', 'various', 'industry', 'work', 'diverse', 'whitecollar', 'client', 'join', 'collaborative', 'supportive', 'team', 'environment', 'enjoy', 'smoothness', 'fairness', 'clarity', 'collaboration', 'david', 'kennedy', 'recruitment']\", 0.7532331943511963)\n",
      "--------------------------------------------------\n",
      "(\"['ready', 'takeoff', 'join', 'skyhigh', 'squad', 'tired', 'mundane', 'job', 'passion', 'adventure', 'flair', 'customer', 'service', 'smile', 'light', 'sky', 'looking', 'job', 'title', 'cabin', 'crew', 'alias', 'superhero', 'skies', 'location', 'vienna', 'zagreb', 'zadar', 'palma', 'de', 'mallorca', 'mostly', 'clouds', 'contract', 'type', 'fulltime', 'uniform', 'red', 'role', 'nutshell', 'ensure', 'passenger', 'comfort', 'safety', 'inflight', 'happiness', 'transform', 'ordinary', 'flight', 'extraordinary', 'journey', 'requirements', 'age', 'still', 'counting', 'looking', 'candidate', 'whove', 'crossed', 'year', 'milestone', 'also', 'mastered', 'art', 'adulting', 'swimming', 'skill', 'ability', 'swim', 'secret', 'superhero', 'power', 'could', 'turn', 'real', 'lifeguard', 'height', 'height', 'range', 'cm', 'cm', 'give', 'boarding', 'pas', 'adventure', 'english', 'proficiency', 'speak', 'language', 'sky', 'english', 'smooth', 'inflight', 'sale', 'service', 'valid', 'passport', 'ticket', 'adventure', 'valid', 'passport', 'youre', 'global', 'jetsetter', 'making', 'team', 'player', 'working', 'team', 'isnt', 'suggestion', 'secret', 'recipe', 'success', 'benefits', 'free', 'cabin', 'crew', 'training', 'course', 'well', 'equip', 'skill', 'knowledge', 'need', 'soar', 'high', 'without', 'costing', 'dime', 'free', 'uniform', 'set', 'first', 'year', 'youll', 'rocking', 'chic', 'uniform', 'thatll', 'make', 'fashion', 'police', 'doubletake', 'career', 'development', 'sky', 'even', 'limit', 'onoff', 'roster', 'schedule', 'give', 'time', 'explore', 'amazing', 'places', 'staff', 'travel', 'perk', 'ready', 'explore', 'enjoy', 'exclusive', 'staff', 'travel', 'benefit', 'jetset', 'around', 'wide', 'range', 'destination', 'price', 'thatll', 'make', 'friend', 'jealous', 'job', 'security', 'stable', 'airline', 'join', 'u', 'peace', 'mind', 'come', 'working', 'financially', 'stable', 'airline', 'u', 'offer', 'onceinalifetime', 'opportunity', 'explore', 'wide', 'range', 'destination', 'making', 'better', 'place', 'one', 'flight', 'time', 'youre', 'ready', 'trade', 'usual', 'workplace', 'window', 'seat', 'hit', 'apply', 'button', 'faster', 'plane', 'takeoff', 'fly', 'u', 'let', 'career', 'soar', 'new', 'height', 'apply', 'embark', 'adventure', 'lifetime']\", 0.7530099153518677)\n",
      "--------------------------------------------------\n",
      "(\"['atelier', 'lumikha', 'serf', 'smart', 'streamlined', 'backoffice', 'work', 'leverage', 'offshore', 'talent', 'proven', 'technology', 'work', 'simple', 'complex', 'covered', 'last', 'decade', 'refined', 'approach', 'selective', 'hiring', 'incremental', 'improvement', 'build', 'culture', 'quality', 'performance', 'employee', 'wellbeing', 'atelier', 'lumikha', 'attracts', 'great', 'people', 'stay', 'get', 'better', 'efficient', 'lumikhans', 'build', 'use', 'technology', 'enhances', 'productivity', 'virtual', 'staff', 'support', 'agent', 'web', 'marketer', 'program', 'manager', 'role', 'passion', 'design', 'looking', 'job', 'marketing', 'might', 'good', 'fit', 'junior', 'graphic', 'designer', 'position', 'responsibility', 'include', 'responsible', 'working', 'australian', 'client', 'create', 'logo', 'business', 'card', 'flyer', 'excellent', 'opportunity', 'develop', 'professional', 'visual', 'design', 'skill', 'working', 'fastpaced', 'environment', 'showcase', 'creativity', 'design', 'chop', 'focus', 'fundamental', 'design', 'color', 'typography', 'clean', 'execution', 'capture', 'client', 'idea', 'business', 'identity', 'ideal', 'profile', 'experience', 'necessary', 'live', '', 'day', 'travel', 'dumaguete', 'creative', 'mind', 'passion', 'great', 'design', 'comfortable', 'working', 'deadline', 'attention', 'detail', 'good', 'communication', 'skill', 'time', 'management', 'skill', 'basic', 'understanding', 'graphic', 'software', 'photo', 'editing', 'vector', 'illustration', 'page', 'layout', 'whats', 'offer', 'competitive', 'salary', 'based', 'experience', 'skill', 'opportunity', 'work', 'exciting', 'project', 'topnotch', 'client', 'fun', 'dynamic', 'work', 'environment', 'team', 'passionate', 'talented', 'individual', 'continuous', 'learning', 'growth', 'opportunity', 'expand', 'skillset', 'flexible', 'working', 'hour', 'option', 'work', 'remotely']\", 0.7448887825012207)\n",
      "--------------------------------------------------\n",
      "(\"['vision', 'simple', 'yet', 'powerful', 'want', 'create', 'place', 'engineer', 'thrive', 'taking', 'exciting', 'project', 'using', 'latest', 'technology', 'free', 'shackle', 'legacy', 'solution', 'thanks', 'diverse', 'range', 'industry', 'work', 'day', 'never', 'boring', 'worldclass', 'apps', 'ar', 'solution', 'even', 'programming', 'robot', 'got', 'covered', 'write', 'code', 'screen', 'rather', 'develop', 'impactful', 'product', 'used', 'million', 'every', 'day', 'let', 'largescale', 'public', 'transport', 'application', 'leading', 'fintech', 'platform', 'storage', 'solution', 'contributing', 'greener', 'future', 'team', 'developer', 'java', 'java', 'script', 'android', 'o', 'platform', 'qa', 'engineer', 'coding', 'finger', 'ready', 'working', 'next', 'big', 'thing', 'value', 'openness', 'modesty', 'hard', 'work', 'simple', 'yet', 'great', 'solution', 'looking', 'next', 'generation', 'extraordinary', 'node', 'j', 'developer', 'part', 'fastmoving', 'talented', 'team', 'working', 'international', 'project', 'support', 'bring', 'stateoftheart', 'product', 'life', 'make', 'sure', 'constantly', 'develop', 'skill', 'meantime', 'node', 'j', 'developer', 'writing', 'code', 'high', 'standard', 'would', 'make', 'uncle', 'bob', 'martin', 'happy', 'build', 'backend', 'microservices', 'buzzword', 'create', 'maintain', 'rest', 'apis', 'take', 'part', 'codereview', 'process', 'supercharge', 'everybody', 'participates', 'supporting', 'project', 'scratch', 'architectural', 'mindset', 'part', 'scrum', 'practice', 'ceremony', 'exploring', 'programming', 'trend', 'adopting', 'new', 'best', 'practice', 'staying', 'uptodate', 'sharing', 'knowledge', 'experience', 'others', 'holding', 'coding', 'dojos', 'workshop', 'current', 'setup', 'currently', 'looking', 'someone', 'year', 'experience', 'developing', 'application', 'microservices', 'rest', 'apis', 'using', 'nest', 'framework', 'daily', 'work', 'xp', 'advantage', 'testing', 'covered', 'jest', 'testing', 'framework', 'ci', 'done', 'gitlab', 'version', 'control', 'git', 'use', 'type', 'orm', 'sequalize', 'daily', 'basis', 'pretty', 'cool', 'interested', 'clientside', 'coding', 'angular', 'superexcited', 'offering', 'team', 'mission', 'create', 'awesome', 'digital', 'product', 'supercharger', 'skill', 'training', 'inhouse', 'training', 'portfolio', 'conference', 'training', 'sponsorship', 'opportunity', 'try', 'latest', 'tech', 'stack', 'constant', 'innovation', 'highimpact', 'international', 'project', 'around', 'world', 'company', 'mac', 'book', 'library', 'personal', 'equipment', 'budget', 'healthcare', 'personal', 'day', 'playstation', 'unlimited', 'beer', 'coffee', 'whichever', 'suit', 'fruit', 'every', 'day', 'tr', 'rudi', 'snack', 'great', 'community', 'therefore', 'awesome', 'company', 'culture', 'cool', 'office', 'heart', 'budapest', 'dear', 'ukrainian', 'friend', 'know', 'life', 'put', 'awful', 'situation', 'make', 'decision', 'leave', 'home', 'come', 'hungary', 'refuge', 'deeply', 'share', 'pain', 'current', 'situation', 'hope', 'peace', 'soon', 'return', 'country', 'meantime', 'try', 'much', 'besides', 'raising', 'fundamental', 'good', 'thing', 'provide', 'job', 'working', 'field', 'software', 'development', 'similar', 'position', 'like', 'product', 'design', 'project', 'management', 'software', 'development', 'testing', 'love', 'team', 'provide', 'visa', 'support', 'long', 'term', 'place', 'supportive', 'welcoming', 'team', 'szeged', 'office', 'focusing', 'international', 'expansion', 'year', 'felt', 'time', 'think', 'locally', 'open', 'second', 'office', 'hungary', 'vibrant', 'city', 'szeged', 'bustling', 'tech', 'social', 'scene', 'stunning', 'architecture', 'laidback', 'atmosphere', 'city', 'true', 'supercharger', 'need', 'create', 'place', 'professional', 'thrive', 'find', 'international', 'challenge', 'opportunity', 'grow', 'amazing', 'company', 'culture']\", 0.7212293744087219)\n",
      "--------------------------------------------------\n",
      "(\"['eye', 'detail', 'driven', 'high', 'quality', 'thrive', 'dynamic', 'environment', 'stakeholder', 'worldwide', 'exciting', 'position', 'blue', 'power', 'partner', 'global', 'consultancy', 'project', 'development', 'company', 'dedicated', 'developing', 'renewable', 'energy', 'aim', 'accelerate', 'green', 'transition', 'shaping', 'renewable', 'technology', 'tomorrow', 'job', 'finance', 'controlling', 'team', 'provides', 'blue', 'power', 'partner', 'decisionmakers', 'reliable', 'timely', 'information', 'drive', 'needed', 'insight', 'control', 'support', 'business', 'make', 'scalable', 'solution', 'support', 'growth', 'company', 'group', 'consists', 'right', 'eleven', 'entity', 'spread', 'across', 'world', 'team', 'consists', 'four', 'fulltime', 'employee', 'one', 'student', 'denmark', 'addition', 'interact', 'external', 'colleague', 'abroad', 'assist', 'u', 'accounting', 'international', 'entity', 'informal', 'tone', 'focus', 'making', 'work', 'enjoyable', 'included', 'following', 'task', 'controlling', 'entity', 'including', 'cost', 'controlling', 'across', 'group', 'monthly', 'reporting', 'coordinate', 'auditor', 'perform', 'yearend', 'deliverable', 'budgeting', 'followup', 'project', 'earnings', 'utilization', 'analysis', 'updating', 'control', 'environment', 'making', 'sure', 'compliant', 'local', 'gaapad', 'hoc', 'analysis', 'knowledge', 'sharing', 'rest', 'team', 'daily', 'work', 'headquarters', 'aalborg', 'john', 'f', 'kennedy', 'plads', 'flexible', 'work', 'environment', 'like', 'rest', 'u', 'passionate', 'renewable', 'energy', 'thrive', 'entrepreneurial', 'international', 'environment', 'skill', 'value', 'right', 'match', 'personal', 'professional', 'skill', 'would', 'prefer', 'enrich', 'business', 'following', 'experience', 'relevant', 'master', 'degree', 'business', 'administration', 'finance', 'management', 'accounting', 'similar', 'experience', 'similar', 'role', 'strong', 'skill', 'excel', 'confident', 'business', 'central', 'power', 'biyou', 'strong', 'interest', 'data', 'automatization', 'financial', 'process', 'able', 'communicate', 'clearly', 'english', 'written', 'oral', 'person', 'structured', 'analytical', 'nature', 'operational', 'pragmatic', 'approach', 'creating', 'result', 'proactive', 'solutionsoriented', 'problem', 'arise', 'agile', 'flexible', 'responding', 'changing', 'need', 'organization', 'growth', 'good', 'collaboration', 'communication', 'skill', 'facing', 'colleague', 'worldwide', 'join', 'blue', 'power', 'partner', 'joining', 'blue', 'power', 'partner', 'become', 'part', 'international', 'culturally', 'diverse', 'company', 'join', 'flat', 'organization', 'work', 'team', 'dynamic', 'environment', 'day', 'people', 'greatest', 'asset', 'driver', 'success', 'people', 'grow', 'grow', 'love', 'social', 'event', 'celebration', 'prioritize', 'knowledge', 'sharing', 'offer', 'possibility', 'grow', 'professionally', 'personally', 'within', 'company', 'blue', 'power', 'partner', 'people', 'contact', 'additional', 'information', 'position', 'please', 'get', 'touch', 'mai', 'ejstrup', 'jensen', 'financial', 'controller', '', 'finance', 'controlling', 'start', 'date', 'flexible']\", 0.7197960019111633)\n",
      "--------------------------------------------------\n",
      "(\"['gm', 'opportunity', 'company', 'going', 'public', 'year', 'client', 'client', 'founded', 'taiwan', 'year', 'experience', 'employee', 'primary', 'focus', 'providing', 'software', 'solution', 'consulting', 'service', 'client', 'manufacturing', 'finance', 'insurance', 'telecom', 'industry', 'across', 'taiwan', 'japan', 'sg', 'thailand', 'vietnam', 'indonesia', 'job', 'description', 'copilot', 'group', 'ceo', 'expand', 'business', 'vietnam', 'successful', 'applicant', 'year', 'experience', 'leading', 'team', 'within', 'si', 'software', 'mnc', 'good', 'understanding', 'ai', 'digital', 'fintech', 'technology', 'experience', 'managing', 'oversea', 'client', 'must', 'head', 'sale', 'productengineering', 'background', 'highly', 'preferred', 'fluency', 'english', 'mandarin', 'must', 'whats', 'offer', 'gm', 'opportunity', 'contact', 'lisa', 'wang', 'quote', 'job', 'ref', 'jn']\", 0.7120808959007263)\n",
      "--------------------------------------------------\n",
      "(\"['hi', 'professional', 'greeting', 'ampstek', 'job', 'designation', 'accountant', 'job', 'location', 'copenhagen', 'denmark', 'job', 'type', 'permanent', 'onsite', 'experience', 'year', 'danish', 'english', 'mandatory', 'job', 'description', 'general', 'administration', 'task', 'experience', 'general', 'ledger', 'book', 'keeping', 'managing', 'day', 'day', 'accounting', 'job', 'transfer', 'monitor', 'incoming', 'outgoing', 'fund', 'different', 'bank', 'account', 'post', 'bank', 'incoming', 'outgoing', 'bank', 'transaction', 'accounting', 'software', 'reconcile', 'record', 'daily', 'monthly', 'basis', 'analyze', 'bank', 'transaction', 'perform', 'multiple', 'bank', 'account', 'intercompanyreconciliations', 'reconcile', 'intercompany', 'bank', 'transfer', 'account', 'norway', 'make', 'payment', 'bank', 'supplier', 'rd', 'party', 'prepare', 'post', 'journal', 'entry', 'accounting', 'software', 'prepare', 'reconciliation', 'finalize', 'account', 'reporting', 'management', 'perform', 'quarter', 'year', 'end', 'task', 'participate', 'various', 'accounting', 'project', 'inflight', 'intiatives', 'streamline', 'process', 'support', 'half', 'year', 'end', 'external', 'audit', 'excellent', 'good', 'hand', 'experience', 'working', 'excel', 'spereadsheets', 'communicate', 'supplier', 'bank', 'warehouse', 'internal', 'department', 'telephone', 'email', 'letter', 'experience', 'general', 'ledger', 'book', 'keeping', 'managing', 'day', 'day', 'accounting', 'job', 'transfer', 'monitor', 'incoming', 'outgoing', 'fund', 'different', 'bank', 'account', 'post', 'bank', 'incoming', 'outgoing', 'bank', 'transaction', 'accounting', 'software', 'reconilethe', 'record', 'daily', 'monthly', 'basis', 'analyze', 'bank', 'transaction', 'perform', 'multiple', 'bank', 'account', 'intercompanyreconciliations', 'reconcile', 'intercompany', 'bank', 'transfer', 'account', 'norway', 'make', 'payment', 'bank', 'supplier', 'rd', 'party', 'prepare', 'post', 'journal', 'entry', 'accounting', 'software', 'prepare', 'reconciliation', 'finalize', 'account', 'reporting', 'management', 'perform', 'quarter', 'year', 'end', 'task', 'participate', 'various', 'accounting', 'project', 'inflight', 'intiatives', 'streamline', 'process', 'support', 'half', 'year', 'end', 'external', 'audit', 'excellent', 'good', 'hand', 'experience', 'working', 'excel', 'spereadsheets', 'communicate', 'supplier', 'bank', 'warehouse', 'internal', 'department', 'telephone', 'email', 'letter', 'shrutika', 'baing', 'recruiter', 'europe', 'ukemail', 'shrutikabampstekcom', 'tel', 'ampstek', 'service', 'limited', 'kemp', 'house', 'city', 'road', 'londonecv', 'nxwebsite', 'httpswwwampstekcom']\", 0.7118332982063293)\n",
      "--------------------------------------------------\n",
      "(\"['denmark', 'remote', 'job', 'opportunity', 'leading', 'reliable', 'networking', 'device', 'accessory', 'company', 'responsibility', 'product', 'get', 'familiar', 'consumer', 'product', 'service', 'understand', 'new', 'technology', 'market', 'trend', 'perform', 'competitor', 'analysis', 'channel', 'account', 'responsible', 'selected', 'distributor', 'retailecom', 'key', 'account', 'detect', 'new', 'business', 'opportunity', 'develop', 'new', 'account', 'clear', 'strategy', 'condition', 'skill', 'experience', 'least', 'year', 'experience', 'channel', 'management', 'within', 'consumer', 'electronics', 'industry', 'new', 'customer', 'bd', 'experience', 'retailecom', 'channel', 'strong', 'sense', 'responsibility', 'keep', 'learning', 'good', 'level', 'office', 'package', 'offer', 'competitive', 'salary', 'package', 'pleasant', 'young', 'international', 'working', 'environment', 'freedom', 'determining', 'calendar', 'scheduling', 'course', 'car', 'allowance', 'laptop', 'phone', 'support', 'danish', 'regulation', 'place', 'work', 'remote', 'fluent', 'danish', 'english', 'nordic', 'language', 'plus']\", 0.7026790380477905)\n",
      "--------------------------------------------------\n",
      "(\"['growing', 'company', 'strong', 'business', 'performanceattractive', 'benefit', 'client', 'client', 'manufacturing', 'company', 'strong', 'performance', 'job', 'description', 'design', 'maintain', 'data', 'infrastructure', 'pipeline', 'trouble', 'shooting', 'monitor', 'data', 'pipeline', 'build', 'maintain', 'data', 'flow', 'successful', 'applicant', 'familiar', 'power', 'bifamiliar', 'sql', 'database', 'experience', 'python', 'java', 'programming', 'experience', 'cloud', 'technology', 'plus', 'experience', 'data', 'pipeline', 'plus', 'fluent', 'chinese', 'english', 'whats', 'offer', 'growing', 'company', 'strong', 'business', 'performance', 'attractive', 'benefit', 'contact', 'nick', 'wei', 'quote', 'job', 'ref', 'jn']\", 0.6976462602615356)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class Doc2VecWrapper:\n",
    "    def __init__(self):\n",
    "        self.tagged_documents = None\n",
    "        self.model = None\n",
    "        self.epochs = None\n",
    "        self.original_data_mapping = None\n",
    "\n",
    "    def init(self, vector_size, alpha, min_alpha, min_count, epochs):\n",
    "        \"\"\"\n",
    "        Initializes the doc2vec model.\n",
    "\n",
    "        vector_size: Dimensionality of the feature vectors.\n",
    "        alpha: The initial learning rate.\n",
    "        min_alpha: Learning rate will linearly drop to min_alpha as training progresses.\n",
    "        min_count: Ignores all words with total frequency lower than this.\n",
    "        epochs: Number of iterations (epochs) over the corpus.\n",
    "        \"\"\"\n",
    "        self.model = Doc2Vec(vector_size=vector_size,\n",
    "                             alpha=alpha,\n",
    "                             min_alpha=min_alpha,\n",
    "                             min_count=min_count)\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def fit(self, tokenized_texts: list[list[str]]):\n",
    "        \"\"\"\n",
    "        Fits the doc2vec model on the data.\n",
    "\n",
    "        tokenized_texts: List of lists of tokens.\n",
    "        \"\"\"\n",
    "        self._tag_data(tokenized_texts)\n",
    "        self.model.build_vocab(self.tagged_documents)\n",
    "\n",
    "        self.original_data_mapping = {\n",
    "            f\"DOC_{str(i)}\": text for i, text in enumerate(tokenized_texts)}\n",
    "\n",
    "    def _tag_data(self, tokenized_texts: list[list[str]]):\n",
    "        \"\"\"\n",
    "        Tags the data for the doc2vec model.\n",
    "\n",
    "        tokenized_texts: List of lists of tokens.\n",
    "        \"\"\"\n",
    "        self.tagged_documents = [TaggedDocument(\n",
    "            words=_d, tags=[f\"DOC_{str(i)}\"]) for i, _d in enumerate(tokenized_texts)]\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains a doc2vec model on the data.\n",
    "        \"\"\"\n",
    "        for epoch in tqdm(range(self.epochs), desc='Training doc2vec', ascii=True, colour=\"#0077B5\"):\n",
    "            self.model.train(self.tagged_documents,\n",
    "                             total_examples=self.model.corpus_count, epochs=1)\n",
    "            # decrease the learning rate\n",
    "            self.model.alpha -= 0.002\n",
    "            # fix the learning rate, no decay\n",
    "            self.model.min_alpha = self.model.alpha\n",
    "\n",
    "    def infer(self, tokenized_text: list[str]):\n",
    "        \"\"\"\n",
    "        Infers a vector for a given tokenized text.\n",
    "\n",
    "        tokenized_text: List of tokens.\n",
    "\n",
    "        returns: Vector representation of the text.\n",
    "        \"\"\"\n",
    "        return self.model.infer_vector(tokenized_text)\n",
    "\n",
    "    def most_similar(self, doc_tag, topn=10):\n",
    "        \"\"\"\n",
    "        Finds the most similar documents to a given document.\n",
    "\n",
    "        doc_tag: Tag of the document.\n",
    "        topn: Number of similar documents to return.\n",
    "\n",
    "        returns: List of tuples (tag, similarity).\n",
    "        \"\"\"\n",
    "        return self.model.dv.most_similar(doc_tag, topn=topn)\n",
    "\n",
    "    def most_similar_original_format(self, doc_tag, topn=10):\n",
    "        \"\"\"\n",
    "        Finds the most similar documents to a given document.\n",
    "\n",
    "        doc_tag: Tag of the document.\n",
    "        topn: Number of similar documents to return.\n",
    "\n",
    "        returns: List of tuples (tag, similarity).\n",
    "        \"\"\"\n",
    "        return [(self.original_data_mapping[doc_tag], similarity) for doc_tag, similarity in self.most_similar(doc_tag, topn)]\n",
    "\n",
    "jobs = load_data(kind=\"processed\")\n",
    "jobs_descriptions = jobs['description'].tolist()\n",
    "\n",
    "doc2vec = Doc2VecWrapper()\n",
    "doc2vec.init(vector_size=50, alpha=0.025,\n",
    "             min_alpha=0.00025, min_count=1, epochs=100)\n",
    "doc2vec.fit(jobs_descriptions)\n",
    "doc2vec.train()\n",
    "\n",
    "print(doc2vec.infer([\"you\", \"are\", \"a\", \"very\", \"good\", \"programmer\"]))\n",
    "\n",
    "# Original format query\n",
    "print(doc2vec.original_data_mapping[\"DOC_50\"])\n",
    "\n",
    "similar_docs = doc2vec.most_similar_original_format(\"DOC_50\")\n",
    "\n",
    "for doc in similar_docs:\n",
    "    print(doc)\n",
    "    print(50 * \"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942f20b7",
   "metadata": {},
   "source": [
    "## SIMILARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9d1204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minhashes(shingles, seeds):\n",
    "  hashs = []\n",
    "  for seed in range(seeds):\n",
    "    mini = float('inf')\n",
    "    for shi in shingles:\n",
    "      # hashes a list of strings\n",
    "      hash = 0\n",
    "      for e in shi:\n",
    "        hash = hash ^ mmh3.hash(e, seed)\n",
    "      # find the minimum value\n",
    "      if mini > hash:\n",
    "        mini = hash\n",
    "    hashs.append(mini)\n",
    "  return list(hashs)\n",
    "\n",
    "# get every signature in data\n",
    "\n",
    "\n",
    "def signatures(df, k, seeds):\n",
    "  hash_dic = {}\n",
    "  df = df.apply(\n",
    "        lambda x: ast.literal_eval(x)\n",
    "        )\n",
    "  for i in range(len(df)):\n",
    "    # make a description into k-shingles\n",
    "    shi = []\n",
    "    for ch in range(len(df[i])-k+1):\n",
    "      shi.append(df[i][ch:ch+k])\n",
    "\n",
    "    hash_dic[i] = minhashes(list(shi), seeds)\n",
    "  return hash_dic\n",
    "\n",
    "\n",
    "def convert_matrix(N, scores):\n",
    "  similarity_matrix = np.zeros((N, N))\n",
    "  for i in range(N):\n",
    "    for j in range(N):\n",
    "      if i == j:\n",
    "        similarity_matrix[i][j] = 1.0\n",
    "      elif i > j:\n",
    "        similarity_matrix[i][j] = scores[(j, i)]\n",
    "      else:\n",
    "        similarity_matrix[i][j] = scores[(i, j)]\n",
    "  return similarity_matrix\n",
    "\n",
    "\n",
    "def find_sim(data, q, seed):\n",
    "  \"\"\"\n",
    "  Finds the similarity between any two job's description for a given dataset using the shingle, minihash \n",
    "  and jaccord similarity.\n",
    "\n",
    "  Args:\n",
    "    data: The \"data\" parameter is the dataset that you want to cluster. It should be a 2D array-like\n",
    "  object, such as a numpy array or a pandas DataFrame, where each row represents a data point and each\n",
    "  column represents a feature of that data point.\n",
    "    q: The q parameter represents the number of shingles ( k = 2 or 3 for small documents such as emails)\n",
    "    seed: The seed parameter represents how mand seeds to use for doing the minihashes\n",
    "\n",
    "  Returns:\n",
    "    A dictionary where the keys are pairs of indices, and the values are scores representing the similarity \n",
    "    between job descriptions at those indices\n",
    "  \"\"\"\n",
    "  sign = signatures(data, q, seed)\n",
    "\n",
    "  score_list = {}\n",
    "  keys = list(sign.keys())\n",
    "  for k in tqdm(range(len(keys)-1), desc='Calculating jaccard similarity'):\n",
    "    for j in range(k+1, len(keys)):\n",
    "      # calculate jaccard simiarity and store the score\n",
    "      score = len(np.intersect1d(\n",
    "          sign[keys[k]], sign[keys[j]]))/len(np.union1d(sign[keys[k]], sign[keys[j]]))\n",
    "      score_list[(keys[k], keys[j])] = score\n",
    "  return score_list\n",
    "\n",
    "\n",
    "def louvain_cluster(N, scores):\n",
    "  \"\"\"\n",
    "  Determines the best partition of a graph for a given similarity score value\n",
    "  using the Louvain Community Detection Algorithm, (Not using Girvan Newman\n",
    "  because it's too time comsuming) and find its Davies-Bouldin index value.\n",
    "\n",
    "  Args:\n",
    "    N: length of data\n",
    "    scores: A dictionary where the keys are pairs of indices, and the values are scores representing the similarity \n",
    "    between job descriptions at those indices\n",
    "\n",
    "  Returns:\n",
    "    the cluster label for each data points and the corresponding Davies-Bouldin index.\n",
    "  \"\"\"\n",
    "  # Create a graph\n",
    "  G = nx.Graph()\n",
    "\n",
    "  # Add nodes (text points)\n",
    "  G.add_nodes_from(range(N))\n",
    "\n",
    "  # Add edges based on similarity scores (you can adjust the threshold)\n",
    "  for idx, idy in scores:\n",
    "    G.add_edge(idx, idy, weight=scores[(idx, idy)]*100)\n",
    "\n",
    "  # Use Louvain community detection algorithm to detect communities\n",
    "  communities = community.louvain_communities(G)\n",
    "\n",
    "  # Retrieve the cluster assignments\n",
    "  clusters = {}\n",
    "  for label, nodes in enumerate(communities):\n",
    "    for idx in nodes:\n",
    "      clusters[idx] = label\n",
    "\n",
    "  # Sort the cluster based on id order and calculate the dbi\n",
    "  sorted_dict = dict(sorted(clusters.items()))\n",
    "  dbi = davies_bouldin_score(convert_matrix(\n",
    "    N, scores), list(sorted_dict.values()))\n",
    "  return sorted_dict, dbi \n",
    "\n",
    "\n",
    "def kmean_cluster(N, scores):\n",
    "  \"\"\"\n",
    "  Determines the clusters for a given similarity matrix.\n",
    "\n",
    "  Args:\n",
    "    N: length of data\n",
    "    scores: A dictionary where the keys are pairs of indices, and the values are scores representing the similarity \n",
    "    between job descriptions at those indices\n",
    "    k_max: The parameter `k_max` represents the maximum number of clusters to consider. In the given\n",
    "    code, it is set to 30, which means the function will iterate over values of `k` from 2 to 30\n",
    "    (inclusive) to find the best value of `k` based on the. Defaults to 30\n",
    "    ground_truth: An array of cluster label generated by feature clustering \n",
    "\n",
    "  Returns:\n",
    "    the cluster label for each data points and the corresponding Davies-Bouldin index .\n",
    "  \"\"\"\n",
    "\n",
    "  warnings.filterwarnings(\"ignore\")\n",
    "  similarity_matrix = convert_matrix(N, scores)\n",
    "  \n",
    "  # Kmeans clustering\n",
    "  kmeans = KMeans(n_clusters=19)\n",
    "  # Convert similarity to distance\n",
    "  labels = kmeans.fit_predict(similarity_matrix)\n",
    "\n",
    "  dbi = davies_bouldin_score(similarity_matrix, labels)\n",
    "\n",
    "  # Retrieve the cluster assignments\n",
    "  clusters = {}\n",
    "  for label, idx in zip(labels, range(N)):\n",
    "    clusters[idx] = label\n",
    "\n",
    "  return clusters, dbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf269023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":wrench: [bold green]WORKING ON[/bold green]: Finding similarity ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating jaccard similarity: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1864/1864 [02:45<00:00, 11.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "df = load_data(kind=\"processed\")\n",
    "\n",
    "# Number of jobs\n",
    "N = len(df)\n",
    "# Give q & seeds for hash to find similarity for each job's descriptions\n",
    "# q = number of singles ( k = 2 or 3 for small documents such as emails)\n",
    "q = 2\n",
    "seeds = 100\n",
    "working_on(\"Finding similarity ...\")\n",
    "scores = find_sim(df['description'], q, seeds)\n",
    "\n",
    "# Plot the network based on similarity and find community based on graph\n",
    "# To evaluate the functionality of the cluster, calculate the DBI (The minimum\n",
    "# score is zero, with lower values indicating better clustering)\n",
    "# and measure rand index between feature label ground truth and prediction\n",
    "# (similarity score between 0.0 and 1.0, inclusive, 1.0 stands for a perfect match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6191e61f",
   "metadata": {},
   "source": [
    "## SIMILARITY CLUSTERING COMMUNITY DISCOVERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5255efad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":wrench: [bold green]WORKING ON[/bold green]: Clustering based on community discovery...\n"
     ]
    }
   ],
   "source": [
    "working_on(\"Clustering based on community discovery...\")\n",
    "cluster_graph, dbi_graph = louvain_cluster(N, scores)\n",
    "df['cluster_graph'] = cluster_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfd874e",
   "metadata": {},
   "source": [
    "## SIMILARITY CLUSTERING KMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c083787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":wrench: [bold green]WORKING ON[/bold green]: Clustering based on kmean...\n"
     ]
    }
   ],
   "source": [
    "working_on(\"Clustering based on kmean...\")\n",
    "cluster_kmean, dbi_kmean = kmean_cluster(N, scores)\n",
    "df['cluster_kmean'] = cluster_kmean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4b047f",
   "metadata": {},
   "source": [
    "# Save clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cf23cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":wrench: [bold green]WORKING ON[/bold green]: Saving clusters ...\n"
     ]
    }
   ],
   "source": [
    "working_on(\"Saving clusters ...\")\n",
    "graph_clusters = df[[\"id\", \"cluster_graph\"]]\n",
    "graph_clusters = graph_clusters.rename(columns={\"cluster_graph\": \"cluster\"})\n",
    "\n",
    "kmean_clusters = df[[\"id\", \"cluster_kmean\"]]\n",
    "kmean_clusters = kmean_clusters.rename(columns={\"cluster_kmean\": \"cluster\"})\n",
    "\n",
    "graph_clusters.to_csv(\"clusters/sim_community_discovery_clusters.csv\", index=False)\n",
    "kmean_clusters.to_csv(\"clusters/sim_kmeans_clusters.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011e05cb",
   "metadata": {},
   "source": [
    "## EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99d33923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clustering_methods(paths):\n",
    "    \"\"\"Loads multiple clusterings from specified file paths.\"\"\"\n",
    "    clustering_method = {}\n",
    "    for name, path in paths.items():\n",
    "        clustering_method[name] = pd.read_csv(path)\n",
    "    return clustering_method\n",
    "\n",
    "\n",
    "def compare_clusters_nmi(clusters):\n",
    "    \"\"\"Compares multiple sets of clusters using Normalized Mutual Information (NMI).\"\"\"\n",
    "    nmi_matrix = pd.DataFrame(index=clusters.keys(), columns=clusters.keys())\n",
    "    for name1, data1 in clusters.items():\n",
    "        for name2, data2 in clusters.items():\n",
    "            if name1 != name2:\n",
    "                merged_data = pd.merge(data1, data2, on='id', suffixes=('_1', '_2'))\n",
    "                nmi_score = normalized_mutual_info_score(\n",
    "                    merged_data['cluster_1'], merged_data['cluster_2'])\n",
    "                nmi_matrix.loc[name1, name2] = nmi_score\n",
    "            else:\n",
    "                nmi_matrix.loc[name1, name2] = 1.0  # Same clustering method comparison\n",
    "    return nmi_matrix\n",
    "\n",
    "\n",
    "def compare_clusters_rand_index(clusters):\n",
    "    \"\"\"Compares multiple sets of clusters using Rand Index.\"\"\"\n",
    "    rand_matrix = pd.DataFrame(index=clusters.keys(), columns=clusters.keys())\n",
    "    for name1, data1 in clusters.items():\n",
    "        for name2, data2 in clusters.items():\n",
    "            if name1 != name2:\n",
    "                merged_data = pd.merge(data1, data2, on='id', suffixes=('_1', '_2'))\n",
    "                rand = rand_score(\n",
    "                    merged_data[\"cluster_1\"], merged_data[\"cluster_2\"])\n",
    "                rand_matrix.loc[name1, name2] = rand\n",
    "            else:\n",
    "                # Same clustering method comparison\n",
    "                rand_matrix.loc[name1, name2] = 1.0\n",
    "    return rand_matrix\n",
    "\n",
    "\n",
    "def evaluation():\n",
    "    paths = {\n",
    "        # 'ground_truth': 'clusters/ground_truth.csv',\n",
    "        'ground_truth_gpt': 'clusters/ground_truth_gpt.csv',\n",
    "        'word2vec': 'clusters/word2vec_clusters.csv',\n",
    "        'tfidf_text': 'clusters/tf_idf_clusters.csv',\n",
    "        'tfidf_industries': \"clusters/tfidf_industries_and_functions_clusters.csv\",\n",
    "        'industries_functions': 'clusters/ind_fun_onehot_clusters.csv',\n",
    "        'similarity_community_disc': 'clusters/sim_community_discovery_clusters.csv',\n",
    "        'similarity_kmeans': 'clusters/sim_kmeans_clusters.csv',\n",
    "        'doc2vec_gmm': 'clusters/doc2vec_gmm_clusters.csv',\n",
    "        'doc2vec_kmeans': 'clusters/doc2vec_kmeans_clusters.csv',\n",
    "    }\n",
    "\n",
    "    # Load the datasets\n",
    "    working_on(\"Comparing clusters ...\")\n",
    "    cluster_methods = load_clustering_methods(paths)\n",
    "\n",
    "    # Compare the clusters and get NMI matrix\n",
    "    nmi_matrix = compare_clusters_nmi(cluster_methods)\n",
    "    rand_index_matrix = compare_clusters_rand_index(cluster_methods)\n",
    "\n",
    "    success(\"Normalized Mutual Information matrix:\")\n",
    "    # Dataframe to string\n",
    "    print(nmi_matrix.to_string(index=False))\n",
    "\n",
    "    success(\"Rand Index matrix:\")\n",
    "    print(rand_index_matrix.to_string(index=False))\n",
    "\n",
    "    ground_truth_nmi = nmi_matrix['ground_truth_gpt'].drop('ground_truth_gpt')\n",
    "    ground_truth_rand = rand_index_matrix['ground_truth_gpt'].drop(\n",
    "        'ground_truth_gpt')\n",
    "\n",
    "    # Select the best clustering method based on NMI and Rand Index\n",
    "    #best_nmi = ground_truth_nmi.idxmax()\n",
    "    #best_rand = ground_truth_rand.idxmax()\n",
    "    best_nmi_index = ground_truth_nmi.astype(float).idxmax()\n",
    "    best_rand_index = ground_truth_rand.astype(float).idxmax()\n",
    "    \n",
    "    # Get the corresponding values\n",
    "    best_nmi = ground_truth_nmi[best_nmi_index]\n",
    "    best_rand = ground_truth_rand[best_rand_index]\n",
    "    \n",
    "    winner(f\"Best clustering method based on NMI: {best_nmi}\")\n",
    "    #print(f\"NMI SCORE: {round(ground_truth_nmi[best_nmi],3)}\")\n",
    "    print(f\"NMI SCORE: {round(best_nmi, 3)}\")\n",
    "\n",
    "    winner(f\"Best clustering method based on Rand Index: {best_rand}\")\n",
    "    #print(f\"RAND SCORE: {round(ground_truth_rand[best_rand],3)}\")\n",
    "    print(f\"RAND SCORE: {round(best_rand, 3)}\")\n",
    "\n",
    "    # Plot the NMI and Rand Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8aca92db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":wrench: [bold green]WORKING ON[/bold green]: Comparing clusters ...\n",
      ":white_check_mark: [bold green]SUCCESS[/bold green]: Normalized Mutual Information matrix:\n",
      "ground_truth_gpt  word2vec tfidf_text tfidf_industries industries_functions similarity_community_disc similarity_kmeans doc2vec_gmm doc2vec_kmeans\n",
      "             1.0  0.082135   0.255119         0.262505             0.258349                  0.111291          0.134077    0.040393        0.03664\n",
      "        0.082135       1.0   0.161284         0.074703             0.059478                  0.113221          0.127688    0.050201         0.0563\n",
      "        0.255119  0.161284        1.0         0.231081              0.21041                  0.276787          0.334771    0.059916       0.059618\n",
      "        0.262505  0.074703   0.231081              1.0             0.440846                  0.131934          0.179344    0.041914       0.040662\n",
      "        0.258349  0.059478    0.21041         0.440846                  1.0                  0.113928          0.147667    0.040456       0.033989\n",
      "        0.111291  0.113221   0.276787         0.131934             0.113928                       1.0          0.362117    0.047496       0.045618\n",
      "        0.134077  0.127688   0.334771         0.179344             0.147667                  0.362117               1.0    0.046012       0.043814\n",
      "        0.040393  0.050201   0.059916         0.041914             0.040456                  0.047496          0.046012         1.0       0.716852\n",
      "         0.03664    0.0563   0.059618         0.040662             0.033989                  0.045618          0.043814    0.716852            1.0\n",
      ":white_check_mark: [bold green]SUCCESS[/bold green]: Rand Index matrix:\n",
      "ground_truth_gpt  word2vec tfidf_text tfidf_industries industries_functions similarity_community_disc similarity_kmeans doc2vec_gmm doc2vec_kmeans\n",
      "             1.0  0.815978    0.79268         0.829098             0.841678                  0.573793           0.61489    0.792766        0.79089\n",
      "        0.815978       1.0   0.831325         0.866484             0.880825                  0.583167          0.637753    0.842372       0.840564\n",
      "         0.79268  0.831325        1.0         0.834525             0.844728                  0.608963          0.654379    0.806224       0.804123\n",
      "        0.829098  0.866484   0.834525              1.0             0.898017                  0.588585           0.64453     0.84004       0.838089\n",
      "        0.841678  0.880825   0.844728         0.898017                  1.0                  0.585959          0.644175    0.853821        0.85153\n",
      "        0.573793  0.583167   0.608963         0.588585             0.585959                       1.0          0.597368    0.583092       0.581191\n",
      "         0.61489  0.637753   0.654379          0.64453             0.644175                  0.597368               1.0    0.629533        0.62836\n",
      "        0.792766  0.842372   0.806224          0.84004             0.853821                  0.583092          0.629533         1.0       0.922136\n",
      "         0.79089  0.840564   0.804123         0.838089              0.85153                  0.581191           0.62836    0.922136            1.0\n",
      ":trophy: [bold yellow]WINNER[/bold yellow]: Best clustering method based on NMI: 0.2625047415347343\n",
      "NMI SCORE: 0.263\n",
      ":trophy: [bold yellow]WINNER[/bold yellow]: Best clustering method based on Rand Index: 0.8416780770691182\n",
      "RAND SCORE: 0.842\n"
     ]
    }
   ],
   "source": [
    "evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740fc2a4",
   "metadata": {},
   "source": [
    "## GROUND TRUTH INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c5c902f",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 34\u001b[0m\n\u001b[1;32m     30\u001b[0m job_descriptions \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     32\u001b[0m industries \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSoftware & IT, Healthcare & Medicine, Education & Training, Engineering & Manufacturing, Finance & Accounting, Sales & Marketing, Creative Arts & Design, Hospitality & Tourism, Construction & Real Estate, Legal & Compliance, Science & Research, Human Resources & Recruitment, Transportation & Logistics, Agriculture & Environmental, Retail & Consumer Goods, Media & Communications, Government & Public Sector, Non-Profit & Social Services, Energy & Utilities, Arts & Entertainment\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 34\u001b[0m client \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mClient()\n\u001b[1;32m     36\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     37\u001b[0m yaml_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mground_truth.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_client.py:93\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m     91\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m     )\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "def transform_string(s):\n",
    "    return s[1:-1].replace(\"'\", \"\").replace(\", \", \" \")\n",
    "\n",
    "def api_call_thread(offer, client, result_container):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You are a professional job recruiter. Your task is to categorize a job description with keywords into one and only one of the specified 20 categories: {industries}. You are not allowed to use any other categories.\"},\n",
    "                {\"role\": \"user\", \"content\": \"Classify into one of the given indsutries. Job description: '''would like part ryanair group amazing cabin crew family k crew customer oriented love delivering great service want fast track career opportunity would delighted hear experience required bag enthusiasm team spirit europe largest airline carrying k guest daily flight looking next generation cabin crew join u brand new copenhagen base flying board ryanair group aircraft amazing perk including discounted staff travel destination across ryanair network fixed roster pattern free training industry leading pay journey becoming qualified cabin crew member start week training course learn fundamental skill require part day day role delivering top class safety customer service experience guest course required study exam taking place regular interval training culminates supernumerary flight followed cabin crew wing member ryanair group cabin crew family immersed culture day one career opportunity endless including becoming number base supervisor european base manager regional manager aspire becoming director inflight life cabin crew fun rewarding however demanding position safety number priority required operate early late shift report duty early morning early roster return home midnight afternoon roster morning person think twice applying requirement bag enthusiasm customer serviceoriented background ie previous experience working bar restaurant shop etc applicant must demonstrate legal entitlement work unrestricted basis across euyou must cm cm height must able swim meter unaided help hardworking flexible outgoing friendly personality adaptable happy work shift roster enjoy dealing public ability provide excellent customer service attitude comfortable speaking writing english ease passion travelling meeting new people benefit free cabin crew training course adventure experience lifetime within cabin crew network explore new culture city colleague day flexible day day staff roster unlimited highly discounted staff travel rate sale bonus free uniform year security working financially stable airline daily per diem provided whilst training direct employment contract highly competitive salary package click link start new exciting career sky'''. Keywords: '''management,manufacturing, technology, information,internet'''\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"Hospitality & Tourism\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Classify into one of the given indsutries. Job description: '''{offer['description']}'''. Keywords: '''{offer['keywords']}'''\"},\n",
    "            ]\n",
    "        )\n",
    "        result_container[\"response\"] = response\n",
    "    except Exception as e:\n",
    "        result_container[\"error\"] = str(e)\n",
    "\n",
    "def restart_script():\n",
    "    print(\"Restarting script...\")\n",
    "    os.execv(sys.executable, ['python'] + sys.argv)\n",
    "\n",
    "openai.organization = \"ORG_KEY\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "df = pd.read_csv(\"data/processed/cleaned_jobs.csv\", delimiter=';')\n",
    "\n",
    "df['description'] = df['description'].apply(transform_string)\n",
    "df['keywords'] = df['function'] + ', ' + df['industries']\n",
    "job_descriptions = df[['id', 'keywords', 'description']]\n",
    "\n",
    "industries = \"Software & IT, Healthcare & Medicine, Education & Training, Engineering & Manufacturing, Finance & Accounting, Sales & Marketing, Creative Arts & Design, Hospitality & Tourism, Construction & Real Estate, Legal & Compliance, Science & Research, Human Resources & Recruitment, Transportation & Logistics, Agriculture & Environmental, Retail & Consumer Goods, Media & Communications, Government & Public Sector, Non-Profit & Social Services, Energy & Utilities, Arts & Entertainment\"\n",
    "\n",
    "client = openai.Client()\n",
    "\n",
    "ground_truth = {}\n",
    "yaml_file = 'ground_truth.yaml'\n",
    "if os.path.exists(yaml_file):\n",
    "    with open(yaml_file, 'r') as file:\n",
    "        ground_truth = yaml.safe_load(file) or {}\n",
    "\n",
    "for index, offer in job_descriptions.iterrows():\n",
    "    if offer['id'] in ground_truth:\n",
    "        continue\n",
    "\n",
    "    result_container = {}\n",
    "    thread = threading.Thread(target=api_call_thread, args=(offer, client, result_container))\n",
    "    thread.start()\n",
    "    thread.join(timeout=10)\n",
    "\n",
    "    if thread.is_alive() or \"error\" in result_container:\n",
    "        restart_script()\n",
    "    \n",
    "    response = result_container.get(\"response\")\n",
    "    if response:\n",
    "        skills = response.choices[0].message.content\n",
    "        ground_truth[offer['id']] = skills\n",
    "        with open(yaml_file, 'w') as file:\n",
    "            yaml.dump(ground_truth, file, default_flow_style=False)\n",
    "        print(f\"Saved ground truth for offer ID: {offer['id']}\")\n",
    "\n",
    "ground_truth_df = pd.DataFrame.from_dict(ground_truth, orient='index', columns=['category'])\n",
    "ground_truth_df.index.name = 'id'\n",
    "ground_truth_df.reset_index(inplace=True)\n",
    "\n",
    "mapping_rules = {\n",
    "    'Software & IT': 'Software & IT',\n",
    "    'Creative Arts & Design': 'Creative Arts & Design',\n",
    "    'Engineering & Manufacturing': 'Engineering & Manufacturing',\n",
    "    'Manufacturing': 'Engineering & Manufacturing',\n",
    "    'Human Resources & Recruitment': 'Human Resources & Recruitment',\n",
    "    'Energy & Utilities': 'Energy & Utilities',\n",
    "    'Sales & Marketing': 'Sales & Marketing',\n",
    "    'Consumer Goods': 'Retail & Consumer Goods',\n",
    "    'Transportation & Logistics': 'Transportation & Logistics',\n",
    "    'Finance & Accounting': 'Finance & Accounting',\n",
    "    'Information Technology & Services': 'Software & IT',\n",
    "    'IT & Software': 'Software & IT',\n",
    "    'Non-Profit & Social Services': 'Non-Profit & Social Services',\n",
    "    'Media & Communications': 'Media & Communications',\n",
    "    'Technology': 'Software & IT',\n",
    "    'Hospitality & Tourism': 'Hospitality & Tourism',\n",
    "    'Retail & Consumer Goods': 'Retail & Consumer Goods',\n",
    "    'Technology & Information': 'Software & IT',\n",
    "    'Legal & Compliance': 'Legal & Compliance',\n",
    "    'Healthcare & Medicine': 'Healthcare & Medicine',\n",
    "    'Science & Research': 'Science & Research',\n",
    "    'Information Technology': 'Software & IT',\n",
    "    'Education & Training': 'Education & Training',\n",
    "    'Business & Entrepreneurship': 'Finance & Accounting',\n",
    "    'Logistics & Supply Chain': 'Transportation & Logistics',\n",
    "    'Construction & Real Estate': 'Construction & Real Estate',\n",
    "    'Arts & Entertainment': 'Arts & Entertainment',\n",
    "    'Agriculture & Environmental': 'Agriculture & Environmental',\n",
    "    'Staffing & Recruiting': 'Human Resources & Recruitment',\n",
    "    'Maritime & Transportation': 'Transportation & Logistics',\n",
    "    'Technology & IT': 'Software & IT',\n",
    "    'Public Relations & Communications': 'Media & Communications',\n",
    "    'Customer Service': 'Human Resources & Recruitment',\n",
    "    'Information Technology (IT)': 'Software & IT',\n",
    "    'Manufacturing & Engineering': 'Engineering & Manufacturing',\n",
    "    'Renewable energy': 'Energy & Utilities',\n",
    "    'Government & Public Sector': 'Government & Public Sector',\n",
    "    'Customer Success': 'Sales & Marketing',\n",
    "    'Insurance & Risk Management': 'Finance & Accounting',\n",
    "    'Human Resources': 'Human Resources & Recruitment',\n",
    "    'Marketing & Advertising': 'Sales & Marketing',\n",
    "    'Pharmaceutical & Healthcare': 'Healthcare & Medicine',\n",
    "    'Retail': 'Retail & Consumer Goods',\n",
    "    'Environmental & Sustainability': 'Agriculture & Environmental',\n",
    "    'Real Estate & Construction': 'Construction & Real Estate',\n",
    "    'Aerospace & Defense': 'Engineering & Manufacturing',\n",
    "    'Public Relations': 'Media & Communications',\n",
    "    'Event Planning & Management': 'Hospitality & Tourism',\n",
    "    'Sports & Recreation': 'Arts & Entertainment',\n",
    "    'Medical equipment manufacturing': 'Healthcare & Medicine',\n",
    "    'Renewable Energy': 'Energy & Utilities',\n",
    "    'Technology & Internet': 'Software & IT',\n",
    "    'Technology & Information Technology': 'Software & IT',\n",
    "    'Administration & Office Support': 'Human Resources & Recruitment',\n",
    "    'Information & Technology': 'Software & IT',\n",
    "    'Administration': 'Human Resources & Recruitment',\n",
    "    'Technology & Telecommunications': 'Software & IT',\n",
    "    'Insurance': 'Finance & Accounting',\n",
    "    'Insurance & Financial Services': 'Finance & Accounting',\n",
    "    'Logistics & Supply Chain Management': 'Transportation & Logistics',\n",
    "    'Market Research': 'Sales & Marketing'\n",
    "}\n",
    "\n",
    "ground_truth_df['category'] = ground_truth_df['category'].map(mapping_rules)\n",
    "\n",
    "ground_truth_df['category'] = pd.Categorical(ground_truth_df['category'])\n",
    "ground_truth_df['cluster'] = ground_truth_df['category'].cat.codes\n",
    "df_id_and_cluster = ground_truth_df[[\"id\", \"category\", \"cluster\"]].sort_values(\n",
    "    by=\"cluster\", ascending=True\n",
    ")\n",
    "\n",
    "df_id_and_cluster.to_csv(\"./csv_files/ground_truth_gpt.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129bd279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "288814bb",
   "metadata": {},
   "source": [
    "## SKILL EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc23e789",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Set your OpenAI API key from the environment\u001b[39;00m\n\u001b[1;32m      2\u001b[0m API_KEY \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(api_key\u001b[38;5;241m=\u001b[39mAPI_KEY)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_skills_gpt3\u001b[39m(job_description):\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# system = f\"You are an expert in job analysis. Your task is to identify skills required for a job based on its description, selecting only from the following predefined skills list: {', '.join(skill_list)}. You are not allowed to use any other skills than those mentioned in the skills list. Do not infer or add skills not mentioned in the description and provide the skills in a simple, comma-separated format.\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# system = f\"You are an expert in job analysis. Your task is to identify skills required for a job based on its description. You are only to identify soft skills. Do not infer or add skills not mentioned in the description and provide the skills in a simple, comma-separated format.\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     system \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are an expert in job analysis. Your task is to extract at most 10 skills required for a job based on its description. Do not infer or add skills not mentioned in the description. You are required to present me the skills in a raw list format: [skill1, skill2, ... skill10].\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_client.py:93\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m     91\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m     )\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "# Set your OpenAI API key from the environment\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "\n",
    "def extract_skills_gpt3(job_description):\n",
    "\n",
    "    # system = f\"You are an expert in job analysis. Your task is to identify skills required for a job based on its description, selecting only from the following predefined skills list: {', '.join(skill_list)}. You are not allowed to use any other skills than those mentioned in the skills list. Do not infer or add skills not mentioned in the description and provide the skills in a simple, comma-separated format.\"\n",
    "    # system = f\"You are an expert in job analysis. Your task is to identify skills required for a job based on its description. You are only to identify soft skills. Do not infer or add skills not mentioned in the description and provide the skills in a simple, comma-separated format.\"\n",
    "    system = f\"You are an expert in job analysis. Your task is to extract at most 10 skills required for a job based on its description. Do not infer or add skills not mentioned in the description. You are required to present me the skills in a raw list format: [skill1, skill2, ... skill10].\"\n",
    "\n",
    "    # prompt = f\"Extract and list the skills required for Present the skills in a simple, comma-separated list. No explanations or additional text. Job Description: '{job_description_str}' Skills:\"\n",
    "    prompt = f\"Identify at most 10 skills required for this job based on the description. Present them to me in a raw list format [skill1, skill2, ..., skill10]. Description: '{job_description}'\"\n",
    "\n",
    "    response = client.chat.completions.create(model=\"gpt-3.5-turbo\",\n",
    "                                              messages=[\n",
    "                                                  {\"role\": \"system\",\n",
    "                                                   \"content\": system},\n",
    "                                                  {\"role\": \"user\", \"content\": prompt},\n",
    "                                              ])\n",
    "\n",
    "    skills_response = response.choices[0].message.content\n",
    "    return skills_response\n",
    "\n",
    "\n",
    "def skill_extraction(save_skills=False):\n",
    "\n",
    "    df_clean = load_data(\"processed\")\n",
    "    df_raw = load_data(\"raw\")\n",
    "\n",
    "    df_clean = df_clean[['id', 'description']]\n",
    "    df_raw = df_raw[['id', 'description']]\n",
    "\n",
    "    # Obtain the original unprocessed job descriptions from the jobs that appear in the clean dataset\n",
    "    merged = pd.merge(df_clean, df_raw, on='id', how=\"left\",\n",
    "                      suffixes=('_clean', '_raw'))\n",
    "\n",
    "    # Drop duplicates based on id\n",
    "    merged = merged.drop_duplicates(subset=['id'])\n",
    "\n",
    "    extracted_skills = {\"id\": [], \"skills\": [], \"description_raw\": []}\n",
    "\n",
    "    N = len(merged)\n",
    "    count = 0\n",
    "\n",
    "    for _, row in merged.iterrows():\n",
    "        job_description = row['description_raw']\n",
    "        job_description = job_description.replace(\"\\n\", \" \")\n",
    "        pattern = r'(?<=[a-z])(?=[A-Z])'\n",
    "        job_description = re.sub(pattern, ' ', job_description)\n",
    "        # Remove the last 56 trash characters\n",
    "        job_description = job_description[:-56]\n",
    "\n",
    "        skills = extract_skills_gpt3(job_description)\n",
    "        _id = row['id']\n",
    "\n",
    "        extracted_skills[\"id\"].append(_id)\n",
    "        extracted_skills[\"skills\"].append(skills)\n",
    "        extracted_skills[\"description_raw\"].append(job_description)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        # Print progress in place\n",
    "        print(f\"\\rüí¨ Skills for {_id} extracted! Progress: {count}/{N}\", end=\"\")\n",
    "\n",
    "    extracted_skills_df = pd.DataFrame(extracted_skills)\n",
    "    success(\"Skills extracted\")\n",
    "    if save_skills:\n",
    "        name = \"skills_extracted_gpt3_v2.csv\"\n",
    "        extracted_skills_df.to_csv(\n",
    "            f\"extracted_skills/{name}\", index=False)\n",
    "        success(f\"Skills saved to extracted_skills/{name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b495418",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'skill_extraction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m skill_extraction(save_skills\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'skill_extraction' is not defined"
     ]
    }
   ],
   "source": [
    "skill_extraction(save_skills=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
