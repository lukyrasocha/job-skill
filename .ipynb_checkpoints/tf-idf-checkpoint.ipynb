{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31ef55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import load_data\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab5193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = load_data(kind=\"processed\")\n",
    "\n",
    "# Define the pos_tagger function\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "data = jobs[['id', 'description']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee7c8ff",
   "metadata": {},
   "source": [
    "## Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7442619e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize words in the 'description' column using a lambda function\n",
    "# The lambda function applies part-of-speech tagging and uses a custom 'pos_tagger' function\n",
    "# The result is a list of tuples, each containing a word and its corresponding part-of-speech category\n",
    "# Words are filtered to exclude those with an undefined or None part-of-speech tag\n",
    "data['description_categorized'] = data['description'].apply(\n",
    "    lambda x: [(word, pos_tagger(tag[1][0].upper())) for word, tag in zip(eval(x), nltk.pos_tag(eval(x))) if pos_tagger(tag[1][0].upper()) is not None]\n",
    ")\n",
    "\n",
    "# Create a new column 'description_adjective' by extracting adjectives from the 'description' column\n",
    "# The lambda function applies part-of-speech tagging and uses a custom 'pos_tagger' function\n",
    "# The result is a list of adjectives filtered from the original words\n",
    "# Words are filtered to exclude those with an undefined or None part-of-speech tag\n",
    "data['description_adjective'] = data['description'].apply(\n",
    "    lambda x: [word for word, pos_tag in [(word, pos_tagger(tag[1][0].upper())) for word, tag in zip(eval(x), nltk.pos_tag(eval(x))) if pos_tagger(tag[1][0].upper()) == wordnet.ADJ] if pos_tag is not None]\n",
    ")\n",
    "\n",
    "# create a new column 'description_noun' using the 'apply' function\n",
    "# for each text description in the 'description' column:\n",
    "data['description_noun'] = data['description'].apply(\n",
    "    # use a lambda function to extract nouns using POS tagging\n",
    "    lambda x: [\n",
    "        word  # extract the word\n",
    "        for word, pos_tag in [\n",
    "            (word, pos_tagger(tag[1][0].upper()))  # POS tag each word\n",
    "            for word, tag in zip(eval(x), nltk.pos_tag(eval(x)))  # pair each word with its POS tag\n",
    "            if pos_tagger(tag[1][0].upper()) == wordnet.NOUN  # filter for nouns\n",
    "        ]\n",
    "        if pos_tag is not None  # exclude words with undefined POS tags\n",
    "    ]\n",
    ")\n",
    "\n",
    "data['description_verb'] = data['description'].apply(lambda x: [word for word, pos_tag in [(word, pos_tagger(tag[1][0].upper())) for word, tag in zip(eval(x), nltk.pos_tag(eval(x))) if pos_tagger(tag[1][0].upper()) == wordnet.VERB] if pos_tag is not None])\n",
    "\n",
    "data['location_scraped'] = jobs['location_scraped']\n",
    "locations = data['location_scraped']\n",
    "description_adj = data['description_adjective']\n",
    "description_noun = data['description_noun']\n",
    "description_verb = data['description_verb']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce09c5eb",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e3b5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#description_adj is a list of text descriptions\n",
    "#convert it to a list of strings\n",
    "description_adj_strings = [' '.join(description) for description in description_adj]\n",
    "description_noun_strings = [' '.join(description) for description in description_noun]\n",
    "description_verb_strings = [' '.join(description) for description in description_verb]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "def get_tfidf_vectors(description_type):\n",
    "    \"\"\"\n",
    "    Get TF-IDF vectors and keywords for each description.\n",
    "\n",
    "    Parameters:\n",
    "    - description_type: List of text descriptions.\n",
    "\n",
    "    Returns:\n",
    "    - vectors: TF-IDF vectors.\n",
    "    - all_keywords: List of keywords for each description.\n",
    "    \"\"\"\n",
    "    # create a TF-IDF vectorizer\n",
    "#    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # fit and transform the text descriptions using the TF-IDF vectorizer\n",
    "    vectors = vectorizer.fit_transform(description_type)\n",
    "\n",
    "    # get the feature names (words) corresponding to the TF-IDF vectors\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # convert the sparse TF-IDF matrix to a dense representation\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "\n",
    "    # initialize a list to store keywords for each description\n",
    "    all_keywords = []\n",
    "\n",
    "    # iterate through each description in the dense representation\n",
    "    for description in denselist:\n",
    "        # extract keywords (feature names) where the TF-IDF value is greater than 0\n",
    "        keywords = [feature_names[i] for i, word in enumerate(description) if word > 0]\n",
    "\n",
    "        # append the keywords for the current description to the list\n",
    "        all_keywords.append(keywords)\n",
    "\n",
    "    return vectors, all_keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c7a5f3",
   "metadata": {},
   "source": [
    "## KMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc9abf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get job locations and define colors for each cluster\n",
    "locations = jobs['location_scraped']\n",
    "colors = [\n",
    "    \"#FF0000\",  # Red\n",
    "    \"#00FF00\",  # Green\n",
    "    \"#0000FF\",  # Blue\n",
    "    \"#FFFF00\",  # Yellow\n",
    "    \"#FF00FF\",  # Magenta\n",
    "    \"#00FFFF\",  # Cyan\n",
    "    \"#800080\",  # Purple\n",
    "    \"#008080\",  # Teal\n",
    "    \"#FFA500\",  # Orange\n",
    "    \"#008000\",  # Dark Green\n",
    "    \"#800000\",  # Maroon\n",
    "    \"#000080\",  # Navy\n",
    "    \"#808080\",  # Gray\n",
    "    \"#C0C0C0\",  # Silver\n",
    "    \"#A52A2A\",  # Brown\n",
    "    \"#00CED1\",  # Dark Turquoise\n",
    "    \"#FFD700\",  # Gold\n",
    "    \"#8B4513\",  # Saddle Brown\n",
    "    \"#2F4F4F\",  # Dark Slate Gray\n",
    "    \"#F08080\"   # Light Coral\n",
    "]\n",
    "\n",
    "# Set the number of clusters for K-Means\n",
    "true_k = 20\n",
    "\n",
    "# Initialize K-Means model with specified parameters\n",
    "model = KMeans(n_clusters=true_k, init=\"k-means++\", max_iter=100, n_init=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f517e790",
   "metadata": {},
   "source": [
    "## KMEANS - ADJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee4a07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors, all_keywords = get_tfidf_vectors(description_adj_strings)\n",
    "\n",
    "# Fit the model to the TF-IDF vectors\n",
    "model.fit(vectors)\n",
    "\n",
    "# Get the indices that sort the cluster centers in descending order\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "# Get the feature names (words) from the TF-IDF vectorizer\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Write the top terms for each cluster to a text file\n",
    "with open (\"clusters/kmeans_adj_results.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(true_k):\n",
    "        f.write(f\"Cluster {i+1}\")\n",
    "        f.write(\"\\n\")\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            f.write (' %s' % terms[ind],)\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# Predict cluster assignments for each document in the TF-IDF vectors\n",
    "kmean_indicates = model.fit_predict(vectors)\n",
    "\n",
    "# Add a 'cluster' column to the 'data' DataFrame\n",
    "data['cluster_adj'] = kmean_indicates\n",
    "\n",
    "\n",
    "# Perform PCA (Principal Component Analysis) for dimensionality reduction to 2 components\n",
    "pca = PCA(n_components=2)\n",
    "scatter_plot_points = pca.fit_transform(vectors.toarray())\n",
    "\n",
    "# Get job locations and define colors for each cluster\n",
    "locations = jobs['location_scraped']\n",
    "\n",
    "\n",
    "# Extract x and y coordinates for scatter plot\n",
    "x_axis = [o[0] for o in scatter_plot_points]\n",
    "y_axis = [o[1] for o in scatter_plot_points]\n",
    "\n",
    "# Create a scatter plot with colored points for each cluster and annotations for job locations\n",
    "fig, ax = plt.subplots(figsize=(50, 50))\n",
    "ax.scatter(x_axis, y_axis, c=[colors[d] for d in kmean_indicates], s=400)\n",
    "for i, txt in enumerate(locations):\n",
    "    ax.annotate(txt[0:10], (x_axis[i], y_axis[i]))\n",
    "\n",
    "# Save the scatter plot as an image file\n",
    "plt.savefig(\"figures/kmeans_adj.png\")\n",
    "\n",
    "# Save the results to a CSV file with 'id' and 'cluster' columns\n",
    "result_df = data[['id', 'cluster_adj']]\n",
    "result_df.to_csv('clusters/tf_idf_adj__clusters.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249ddcfd",
   "metadata": {},
   "source": [
    "## KMEANS - NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1835c0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors, all_keywords = get_tfidf_vectors(description_noun_strings)\n",
    "\n",
    "# Fit the model to the TF-IDF vectors\n",
    "model.fit(vectors)\n",
    "\n",
    "# Get the indices that sort the cluster centers in descending order\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "# Get the feature names (words) from the TF-IDF vectorizer\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Write the top terms for each cluster to a text file\n",
    "with open (\"clusters/kmeans_noun_results.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(true_k):\n",
    "        f.write(f\"Cluster {i+1}\")\n",
    "        f.write(\"\\n\")\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            f.write (' %s' % terms[ind],)\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# Predict cluster assignments for each document in the TF-IDF vectors\n",
    "kmean_indicates = model.fit_predict(vectors)\n",
    "\n",
    "# Add a 'cluster' column to the 'data' DataFrame\n",
    "data['cluster_noun'] = kmean_indicates\n",
    "\n",
    "\n",
    "# Perform PCA (Principal Component Analysis) for dimensionality reduction to 2 components\n",
    "pca = PCA(n_components=2)\n",
    "scatter_plot_points = pca.fit_transform(vectors.toarray())\n",
    "\n",
    "# Extract x and y coordinates for scatter plot\n",
    "x_axis = [o[0] for o in scatter_plot_points]\n",
    "y_axis = [o[1] for o in scatter_plot_points]\n",
    "\n",
    "# Create a scatter plot with colored points for each cluster and annotations for job locations\n",
    "fig, ax = plt.subplots(figsize=(50, 50))\n",
    "\n",
    "ax.scatter(x_axis, y_axis, c=[colors[d] for d in kmean_indicates], s=400)\n",
    "for i, txt in enumerate(locations):\n",
    "    ax.annotate(txt[0:10], (x_axis[i], y_axis[i]))\n",
    "\n",
    "# Save the scatter plot as an image file\n",
    "plt.savefig(\"figures/kmeans_noun.png\")\n",
    "# Save the results to a CSV file with 'id' and 'cluster' columns\n",
    "result_df = data[['id', 'cluster_noun']]\n",
    "result_df.to_csv('clusters/tf_idf_noun__clusters.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd94360",
   "metadata": {},
   "source": [
    "## KMEANS - VERB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeb3f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors, all_keywords = get_tfidf_vectors(description_verb_strings)\n",
    "\n",
    "# Fit the model to the TF-IDF vectors\n",
    "model.fit(vectors)\n",
    "\n",
    "# Get the indices that sort the cluster centers in descending order\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "# Get the feature names (words) from the TF-IDF vectorizer\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Write the top terms for each cluster to a text file\n",
    "with open (\"clusters/kmeans_verb_results.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(true_k):\n",
    "        f.write(f\"Cluster {i+1}\")\n",
    "        f.write(\"\\n\")\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            f.write (' %s' % terms[ind],)\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# Predict cluster assignments for each document in the TF-IDF vectors\n",
    "kmean_indicates = model.fit_predict(vectors)\n",
    "\n",
    "# Add a 'cluster' column to the 'data' DataFrame\n",
    "data['cluster_verb'] = kmean_indicates\n",
    "\n",
    "\n",
    "# Perform PCA (Principal Component Analysis) for dimensionality reduction to 2 components\n",
    "pca = PCA(n_components=2)\n",
    "scatter_plot_points = pca.fit_transform(vectors.toarray())\n",
    "\n",
    "# Extract x and y coordinates for scatter plot\n",
    "x_axis = [o[0] for o in scatter_plot_points]\n",
    "y_axis = [o[1] for o in scatter_plot_points]\n",
    "\n",
    "# Create a scatter plot with colored points for each cluster and annotations for job locations\n",
    "fig, ax = plt.subplots(figsize=(50, 50))\n",
    "ax.scatter(x_axis, y_axis, c=[colors[d] for d in kmean_indicates], s=400)\n",
    "for i, txt in enumerate(locations):\n",
    "    ax.annotate(txt[0:10], (x_axis[i], y_axis[i]))\n",
    "\n",
    "# Save the scatter plot as an image file\n",
    "plt.savefig(\"figures/kmeans_verb.png\")\n",
    "\n",
    "# Save the results to a CSV file with 'id' and 'cluster' columns\n",
    "result_df = data[['id', 'cluster_verb']]\n",
    "result_df.to_csv('clusters/tf_idf_verb__clusters.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4feb05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
